{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a proprioception-tuned Vision Transformer (ViT)\n",
    "\n",
    "We create a sensor processing model using Vision Transformer (ViT) based visual encoding finetuned with proprioception.\n",
    "\n",
    "We start with a pretrained ViT model, then train it to:\n",
    "1. Create a meaningful 128-dimensional latent representation\n",
    "2. Learn to map this representation to robot positions (proprioception)\n",
    "\n",
    "The sensor processing object associated with the trained model is in sensorprocessing/sp_vit.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from settings import Config\n",
    "\n",
    "import pathlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from behavior_cloning.demo_to_trainingdata import BCDemonstration\n",
    "from sensorprocessing.sp_vit import VitSensorProcessing\n",
    "from robot.al5d_position_controller import RobotPosition\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pointer config file: /home/ssheikholeslami/.config/BerryPicker/mainsettings.yaml\n",
      "Loading machine-specific config file: /home/ssheikholeslami/SaharaBerryPickerData/settings-sahara.yaml\n",
      "No system dependent experiment file\n",
      " /home/ssheikholeslami/SaharaBerryPickerData/experiments-Config/sensorprocessing_propriotuned_Vit/vit_huge_sysdep.yaml,\n",
      " that is ok, proceeding.\n",
      "Configuration for experiment: sensorprocessing_propriotuned_Vit/vit_huge successfully loaded\n"
     ]
    }
   ],
   "source": [
    "# The experiment/run we are going to run: the specified model will be created\n",
    "\n",
    "\n",
    "experiment = \"sensorprocessing_propriotuned_Vit\"\n",
    "# Other possible configurations:\n",
    "# run = \"vit_base\"  # ViT Base\n",
    "# run = \"vit_large\" # ViT Large\n",
    "run = \"vit_huge\" # ViT Huge\n",
    "\n",
    "exp = Config().get_experiment(experiment, run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create regression training data (image to proprioception)\n",
    "The training data (X, Y) is all the pictures from a demonstration with the corresponding proprioception data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_as_proprioception_training(task, proprioception_input_file, proprioception_target_file):\n",
    "    \"\"\"Loads all the images of a task, and processes it as two tensors as input and target data for proprioception training.\n",
    "    Caches the processed results into the input and target file pointed in the config. Remove those files to recalculate.\n",
    "    \"\"\"\n",
    "    retval = {}\n",
    "    if proprioception_input_file.exists():\n",
    "        retval[\"inputs\"] = torch.load(proprioception_input_file, weights_only=True)\n",
    "        retval[\"targets\"] = torch.load(proprioception_target_file, weights_only=True)\n",
    "    else:\n",
    "        demos_dir = pathlib.Path(Config()[\"demos\"][\"directory\"])\n",
    "        task_dir = pathlib.Path(demos_dir, \"demos\", task)\n",
    "\n",
    "        inputlist = []\n",
    "        targetlist = []\n",
    "\n",
    "        print(f\"Loading demonstrations from {task_dir}\")\n",
    "        for demo_dir in task_dir.iterdir():\n",
    "            if not demo_dir.is_dir():\n",
    "                continue\n",
    "            print(f\"Processing demonstration: {demo_dir.name}\")\n",
    "            bcd = BCDemonstration(demo_dir, sensorprocessor=None)\n",
    "            for i in range(bcd.trim_from, bcd.trim_to):\n",
    "                sensor_readings, _ = bcd.get_image(i)\n",
    "                inputlist.append(sensor_readings[0])\n",
    "                a = bcd.get_a(i)\n",
    "                rp = RobotPosition.from_vector(a)\n",
    "                anorm = rp.to_normalized_vector()\n",
    "                targetlist.append(torch.from_numpy(anorm))\n",
    "\n",
    "        retval[\"inputs\"] = torch.stack(inputlist)\n",
    "        retval[\"targets\"] = torch.stack(targetlist)\n",
    "        torch.save(retval[\"inputs\"], proprioception_input_file)\n",
    "        torch.save(retval[\"targets\"], proprioception_target_file)\n",
    "        print(f\"Saved {len(inputlist)} training examples\")\n",
    "\n",
    "    # Separate the training and validation data.\n",
    "    # We will be shuffling the demonstrations\n",
    "    length = retval[\"inputs\"].size(0)\n",
    "    rows = torch.randperm(length)\n",
    "    shuffled_inputs = retval[\"inputs\"][rows]\n",
    "    shuffled_targets = retval[\"targets\"][rows]\n",
    "\n",
    "    training_size = int(length * 0.67)\n",
    "    retval[\"inputs_training\"] = shuffled_inputs[1:training_size]\n",
    "    retval[\"targets_training\"] = shuffled_targets[1:training_size]\n",
    "\n",
    "    retval[\"inputs_validation\"] = shuffled_inputs[training_size:]\n",
    "    retval[\"targets_validation\"] = shuffled_targets[training_size:]\n",
    "\n",
    "    print(f\"Created {retval['inputs_training'].size(0)} training examples and {retval['inputs_validation'].size(0)} validation examples\")\n",
    "    return retval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory: /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_huge\n",
      "Loading demonstrations from /home/ssheikholeslami/SaharaBerryPickerData/demonstrations/demos/proprio_regressor_training\n",
      "Processing demonstration: 2025_03_08__15_02_56\n",
      "Cameras found: ['dev3']\n",
      "There are 445 steps in this demonstration\n",
      "This demonstration was recorded by the following cameras: ['dev3']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing demonstration: 2025_03_08__15_06_47\n",
      "Cameras found: ['dev3']\n",
      "There are 468 steps in this demonstration\n",
      "This demonstration was recorded by the following cameras: ['dev3']\n",
      "Processing demonstration: 2025_03_08__15_05_47\n",
      "Cameras found: ['dev3']\n",
      "There are 410 steps in this demonstration\n",
      "This demonstration was recorded by the following cameras: ['dev3']\n",
      "Processing demonstration: 2025_03_08__15_01_56\n",
      "Cameras found: ['dev3']\n",
      "There are 384 steps in this demonstration\n",
      "This demonstration was recorded by the following cameras: ['dev3']\n",
      "Saved 1703 training examples\n",
      "Created 1140 training examples and 562 validation examples\n"
     ]
    }
   ],
   "source": [
    "# Create output directory if it doesn't exist\n",
    "data_dir = pathlib.Path(exp[\"data_dir\"])\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Data directory: {data_dir}\")\n",
    "\n",
    "task = exp[\"proprioception_training_task\"]\n",
    "proprioception_input_file = pathlib.Path(exp[\"data_dir\"], exp[\"proprioception_input_file\"])\n",
    "proprioception_target_file = pathlib.Path(exp[\"data_dir\"], exp[\"proprioception_target_file\"])\n",
    "\n",
    "tr = load_images_as_proprioception_training(task, proprioception_input_file, proprioception_target_file)\n",
    "inputs_training = tr[\"inputs_training\"]\n",
    "targets_training = tr[\"targets_training\"]\n",
    "inputs_validation = tr[\"inputs_validation\"]\n",
    "targets_validation = tr[\"targets_validation\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the ViT model with proprioception regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing ViT Sensor Processing:\n",
      "  Model: vit_h_14\n",
      "  Latent dimension: 128\n",
      "  Image size: 518x518\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using vit_h_14 with output dimension 1280\n",
      "Created projection network: 1280 → 1024 → 512 → 128\n",
      "Created latent representation: 1280 → 1024 → 128\n",
      "Created proprioceptor: 128 → 64 → 64 → 6\n",
      "Feature extractor frozen. Projection and proprioceptor layers are trainable.\n",
      "Warning: Model file /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_huge/proprioception_mlp.pth does not exist. Using untrained model.\n",
      "Model created successfully\n",
      "Parameters accessed successfully\n",
      "Total parameters: 634107526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/apps/anaconda/anaconda-2023.09/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create the ViT model with proprioception\n",
    "sp = VitSensorProcessing(exp, device)\n",
    "model = sp.enc  # Get the actual encoder model for training\n",
    "\n",
    "\n",
    "# Debug code\n",
    "\n",
    "print(\"Model created successfully\")\n",
    "\n",
    "try:\n",
    "    params = model.parameters()\n",
    "    print(\"Parameters accessed successfully\")\n",
    "    param_count = sum(p.numel() for p in params)\n",
    "    print(f\"Total parameters: {param_count}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error accessing parameters: {e}\")\n",
    "\n",
    "    # Check individual components\n",
    "    try:\n",
    "        backbone_params = model.backbone.parameters()\n",
    "        print(\"Backbone parameters accessed successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error accessing backbone parameters: {e}\")\n",
    "\n",
    "    try:\n",
    "        projection_params = model.projection.parameters()\n",
    "        print(\"Projection parameters accessed successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error accessing projection parameters: {e}\")\n",
    "\n",
    "    try:\n",
    "        proprioceptor_params = model.proprioceptor.parameters()\n",
    "        print(\"Proprioceptor parameters accessed successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error accessing proprioceptor parameters: {e}\")\n",
    "\n",
    "# Select loss function\n",
    "loss_type = exp.get('loss', 'MSELoss')\n",
    "if loss_type == 'MSELoss':\n",
    "    criterion = nn.MSELoss()\n",
    "elif loss_type == 'L1Loss':\n",
    "    criterion = nn.L1Loss()\n",
    "else:\n",
    "    criterion = nn.MSELoss()  # Default to MSE\n",
    "\n",
    "# Set up optimizer with appropriate learning rate and weight decay\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=exp.get('learning_rate', 0.001),\n",
    "    weight_decay=exp.get('weight_decay', 0.01)\n",
    ")\n",
    "\n",
    "# Optional learning rate scheduler\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=3, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders for batching\n",
    "batch_size = exp.get('batch_size', 32)\n",
    "train_dataset = TensorDataset(inputs_training, targets_training)\n",
    "test_dataset = TensorDataset(inputs_validation, targets_validation)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_proprioception_model(model, criterion, optimizer, modelfile,\n",
    "                                        device=\"cpu\", epochs=20, scheduler=None,\n",
    "                                        log_interval=1):\n",
    "    \"\"\"Trains and saves the ViT proprioception model\n",
    "\n",
    "    Args:\n",
    "        model: ViT model with proprioception\n",
    "        criterion: Loss function\n",
    "        optimizer: Optimizer\n",
    "        modelfile: Path to save the model\n",
    "        device: Training device (cpu/cuda)\n",
    "        epochs: Number of training epochs\n",
    "        scheduler: Optional learning rate scheduler\n",
    "        log_interval: How often to print logs\n",
    "    \"\"\"\n",
    "    # Ensure model is on the right device\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "\n",
    "    # Keep track of the best validation loss\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "\n",
    "            # Forward pass through the full model (including proprioceptor)\n",
    "            predictions = model.forward(batch_X)\n",
    "            loss = criterion(predictions, batch_y)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in test_loader:\n",
    "                batch_X = batch_X.to(device)\n",
    "                batch_y = batch_y.to(device)\n",
    "                predictions = model(batch_X)\n",
    "                loss = criterion(predictions, batch_y)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(test_loader)\n",
    "\n",
    "        # Update learning rate if scheduler is provided\n",
    "        if scheduler is not None:\n",
    "            scheduler.step(avg_val_loss)\n",
    "\n",
    "        # Save the best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), modelfile)\n",
    "            print(f\"  New best model saved with validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "        # Log progress\n",
    "        if (epoch + 1) % log_interval == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "    # Final evaluation\n",
    "    print(f\"Training complete. Best validation loss: {best_val_loss:.4f}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training new model for 300 epochs\n",
      "  New best model saved with validation loss: 0.0300\n",
      "Epoch [1/300], Train Loss: 0.0821, Val Loss: 0.0300\n",
      "  New best model saved with validation loss: 0.0254\n",
      "Epoch [2/300], Train Loss: 0.0256, Val Loss: 0.0254\n",
      "  New best model saved with validation loss: 0.0228\n",
      "Epoch [3/300], Train Loss: 0.0244, Val Loss: 0.0228\n",
      "  New best model saved with validation loss: 0.0221\n",
      "Epoch [4/300], Train Loss: 0.0242, Val Loss: 0.0221\n",
      "Epoch [5/300], Train Loss: 0.0241, Val Loss: 0.0238\n",
      "  New best model saved with validation loss: 0.0220\n",
      "Epoch [6/300], Train Loss: 0.0249, Val Loss: 0.0220\n",
      "  New best model saved with validation loss: 0.0195\n",
      "Epoch [7/300], Train Loss: 0.0254, Val Loss: 0.0195\n",
      "Epoch [8/300], Train Loss: 0.0252, Val Loss: 0.0220\n",
      "Epoch [9/300], Train Loss: 0.0253, Val Loss: 0.0266\n",
      "Epoch [10/300], Train Loss: 0.0250, Val Loss: 0.0234\n",
      "Epoch [11/300], Train Loss: 0.0252, Val Loss: 0.0213\n",
      "  New best model saved with validation loss: 0.0172\n",
      "Epoch [12/300], Train Loss: 0.0219, Val Loss: 0.0172\n",
      "Epoch [13/300], Train Loss: 0.0218, Val Loss: 0.0185\n",
      "Epoch [14/300], Train Loss: 0.0208, Val Loss: 0.0179\n",
      "Epoch [15/300], Train Loss: 0.0224, Val Loss: 0.0185\n",
      "Epoch [16/300], Train Loss: 0.0216, Val Loss: 0.0234\n",
      "  New best model saved with validation loss: 0.0162\n",
      "Epoch [17/300], Train Loss: 0.0205, Val Loss: 0.0162\n",
      "  New best model saved with validation loss: 0.0150\n",
      "Epoch [18/300], Train Loss: 0.0190, Val Loss: 0.0150\n",
      "Epoch [19/300], Train Loss: 0.0190, Val Loss: 0.0166\n",
      "Epoch [20/300], Train Loss: 0.0188, Val Loss: 0.0156\n",
      "Epoch [21/300], Train Loss: 0.0188, Val Loss: 0.0161\n",
      "Epoch [22/300], Train Loss: 0.0185, Val Loss: 0.0165\n",
      "  New best model saved with validation loss: 0.0140\n",
      "Epoch [23/300], Train Loss: 0.0184, Val Loss: 0.0140\n",
      "  New best model saved with validation loss: 0.0136\n",
      "Epoch [24/300], Train Loss: 0.0170, Val Loss: 0.0136\n",
      "Epoch [25/300], Train Loss: 0.0168, Val Loss: 0.0142\n",
      "Epoch [26/300], Train Loss: 0.0168, Val Loss: 0.0138\n",
      "Epoch [27/300], Train Loss: 0.0166, Val Loss: 0.0141\n",
      "Epoch [28/300], Train Loss: 0.0172, Val Loss: 0.0150\n",
      "  New best model saved with validation loss: 0.0133\n",
      "Epoch [29/300], Train Loss: 0.0170, Val Loss: 0.0133\n",
      "  New best model saved with validation loss: 0.0132\n",
      "Epoch [30/300], Train Loss: 0.0158, Val Loss: 0.0132\n",
      "Epoch [31/300], Train Loss: 0.0154, Val Loss: 0.0134\n"
     ]
    }
   ],
   "source": [
    "modelfile = pathlib.Path(exp[\"data_dir\"], exp[\"proprioception_mlp_model_file\"])\n",
    "epochs = exp.get(\"epochs\", 20)\n",
    "\n",
    "# Check if model already exists\n",
    "if modelfile.exists() and exp.get(\"reload_existing_model\", True):\n",
    "    print(f\"Loading existing model from {modelfile}\")\n",
    "    model.load_state_dict(torch.load(modelfile, map_location=device))\n",
    "\n",
    "    # Optional: evaluate the loaded model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for batch_X, batch_y in test_loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            predictions = model(batch_X)\n",
    "            loss = criterion(predictions, batch_y)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(test_loader)\n",
    "        print(f\"Loaded model validation loss: {avg_val_loss:.4f}\")\n",
    "else:\n",
    "    print(f\"Training new model for {epochs} epochs\")\n",
    "    model = train_and_save_proprioception_model(\n",
    "        model, criterion, optimizer, modelfile,\n",
    "        device=device, epochs=epochs, scheduler=lr_scheduler\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing ViT Sensor Processing:\n",
      "  Model: vit_l_16\n",
      "  Latent dimension: 128\n",
      "  Image size: 224x224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using vit_l_16 with output dimension 1024\n",
      "Created projection network: 1024 → 512 → 256 → 128\n",
      "Created latent representation: 1024 → 512 → 128\n",
      "Created proprioceptor: 128 → 64 → 64 → 6\n",
      "Feature extractor frozen. Projection and proprioceptor layers are trainable.\n",
      "Loading ViT encoder weights from /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_large/proprioception_mlp.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lustre/fs1/home/ssheikholeslami/BerryPicker/src/sensorprocessing/../sensorprocessing/sp_vit.py:212: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.enc.load_state_dict(torch.load(modelfile, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing sensor processing on random examples:\n",
      "--------------------------------------------------\n",
      "Example 1:\n",
      "  Image shape: torch.Size([1, 3, 256, 256])\n",
      "  Latent shape: (128,)\n",
      "  Target position: [0.06060451 0.7903572  0.491293   0.704839   0.4744848  0.30784598]\n",
      "\n",
      "Example 2:\n",
      "  Image shape: torch.Size([1, 3, 256, 256])\n",
      "  Latent shape: (128,)\n",
      "  Target position: [0.5747702  0.78994364 0.6641628  0.33462417 0.29938844 0.71034545]\n",
      "\n",
      "Example 3:\n",
      "  Image shape: torch.Size([1, 3, 256, 256])\n",
      "  Latent shape: (128,)\n",
      "  Target position: [0.8682212 0.8242293 0.4242512 0.5075824 0.7660551 0.457051 ]\n",
      "\n",
      "Example 4:\n",
      "  Image shape: torch.Size([1, 3, 256, 256])\n",
      "  Latent shape: (128,)\n",
      "  Target position: [0.767953   0.702578   0.16808294 0.7865119  0.38399073 0.20142773]\n",
      "\n",
      "Example 5:\n",
      "  Image shape: torch.Size([1, 3, 256, 256])\n",
      "  Latent shape: (128,)\n",
      "  Target position: [0.3918777 0.5043456 0.8729367 0.8217159 0.833581  0.8309385]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the sensor processing module using the trained model\n",
    "sp = VitSensorProcessing(exp, device)\n",
    "\n",
    "# Test it on a few validation examples\n",
    "def test_sensor_processing(sp, test_images, test_targets, n_samples=5):\n",
    "    \"\"\"Test the sensor processing module on a few examples.\"\"\"\n",
    "    if n_samples > len(test_images):\n",
    "        n_samples = len(test_images)\n",
    "\n",
    "    # Get random indices\n",
    "    indices = torch.randperm(len(test_images))[:n_samples]\n",
    "\n",
    "    print(\"\\nTesting sensor processing on random examples:\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    for i, idx in enumerate(indices):\n",
    "        # Get image and target\n",
    "        image = test_images[idx].unsqueeze(0).to(device)  # Add batch dimension\n",
    "        target = test_targets[idx].cpu().numpy()\n",
    "\n",
    "        # Process the image to get the latent representation\n",
    "        latent = sp.process(image)\n",
    "\n",
    "        # Print the results\n",
    "        print(f\"Example {i+1}:\")\n",
    "        print(f\"  Image shape: {image.shape}\")\n",
    "        print(f\"  Latent shape: {latent.shape}\")\n",
    "        print(f\"  Target position: {target}\")\n",
    "        print()\n",
    "\n",
    "# Test the sensor processing\n",
    "test_sensor_processing(sp, inputs_validation, targets_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify the model's encoding and forward methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latent representation shape: torch.Size([1, 128])\n",
      "Robot position prediction shape: torch.Size([1, 6])\n",
      "Verification successful!\n"
     ]
    }
   ],
   "source": [
    "# Verify that the encoding method works correctly\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Get a sample image\n",
    "    sample_image = inputs_validation[0].unsqueeze(0).to(device)\n",
    "\n",
    "    # Get the latent representation using encode\n",
    "    latent = model.encode(sample_image)\n",
    "    print(f\"Latent representation shape: {latent.shape}\")\n",
    "\n",
    "    # Get the robot position prediction using forward\n",
    "    position = model.forward(sample_image)\n",
    "    print(f\"Robot position prediction shape: {position.shape}\")\n",
    "\n",
    "    # Check that the latent representation has the expected size\n",
    "    expected_latent_size = exp[\"latent_size\"]\n",
    "    assert latent.shape[1] == expected_latent_size, f\"Expected latent size {expected_latent_size}, got {latent.shape[1]}\"\n",
    "\n",
    "    # Check that the position prediction has the expected size\n",
    "    expected_output_size = exp[\"output_size\"]\n",
    "    assert position.shape[1] == expected_output_size, f\"Expected output size {expected_output_size}, got {position.shape[1]}\"\n",
    "\n",
    "    print(\"Verification successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save final model and summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to /home/ssheikholeslami/SaharaBerryPickerData/experiment_data/sensorprocessing_propriotuned_Vit/vit_large/proprioception_mlp.pth\n",
      "\n",
      "Training complete!\n",
      "Vision Transformer type: vit_l_16\n",
      "Latent space dimension: 128\n",
      "Output dimension (robot DOF): 6\n",
      "Use the VitSensorProcessing class to load and use this model for inference.\n"
     ]
    }
   ],
   "source": [
    "# Save the model and print summary\n",
    "final_modelfile = pathlib.Path(exp[\"data_dir\"], exp[\"proprioception_mlp_model_file\"])\n",
    "torch.save(model.state_dict(), final_modelfile)\n",
    "print(f\"Model saved to {final_modelfile}\")\n",
    "\n",
    "print(\"\\nTraining complete!\")\n",
    "print(f\"Vision Transformer type: {exp['vit_model']}\")\n",
    "print(f\"Latent space dimension: {exp['latent_size']}\")\n",
    "print(f\"Output dimension (robot DOF): {exp['output_size']}\")\n",
    "print(f\"Use the VitSensorProcessing class to load and use this model for inference.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
