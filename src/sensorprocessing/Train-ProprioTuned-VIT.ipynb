{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a proprioception-tuned Vision Transformer (ViT)\n",
    "\n",
    "We create a sensor processing model using Vision Transformer (ViT) based visual encoding finetuned with proprioception.\n",
    "\n",
    "We start with a pretrained ViT model, then train it to:\n",
    "1. Create a meaningful 128-dimensional latent representation\n",
    "2. Learn to map this representation to robot positions (proprioception)\n",
    "\n",
    "The sensor processing object associated with the trained model is in sensorprocessing/sp_vit.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from settings import Config\n",
    "\n",
    "import pathlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from behavior_cloning.demo_to_trainingdata import BCDemonstration\n",
    "from sensorprocessing.sp_vit import VitSensorProcessing\n",
    "from robot.al5d_position_controller import RobotPosition\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The experiment/run we are going to run: the specified model will be created\n",
    "\n",
    "\n",
    "experiment = \"sensorprocessing_propriotuned_Vit\"\n",
    "# Other possible configurations:\n",
    "# run = \"vit_base\"  # ViT Base\n",
    "run = \"vit_large\" # ViT Large\n",
    "\n",
    "exp = Config().get_experiment(experiment, run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create regression training data (image to proprioception)\n",
    "The training data (X, Y) is all the pictures from a demonstration with the corresponding proprioception data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_as_proprioception_training(task, proprioception_input_file, proprioception_target_file):\n",
    "    \"\"\"Loads all the images of a task, and processes it as two tensors as input and target data for proprioception training.\n",
    "    Caches the processed results into the input and target file pointed in the config. Remove those files to recalculate.\n",
    "    \"\"\"\n",
    "    retval = {}\n",
    "    if proprioception_input_file.exists():\n",
    "        retval[\"inputs\"] = torch.load(proprioception_input_file, weights_only=True)\n",
    "        retval[\"targets\"] = torch.load(proprioception_target_file, weights_only=True)\n",
    "    else:\n",
    "        demos_dir = pathlib.Path(Config()[\"demos\"][\"directory\"])\n",
    "        task_dir = pathlib.Path(demos_dir, \"demos\", task)\n",
    "\n",
    "        inputlist = []\n",
    "        targetlist = []\n",
    "\n",
    "        print(f\"Loading demonstrations from {task_dir}\")\n",
    "        for demo_dir in task_dir.iterdir():\n",
    "            if not demo_dir.is_dir():\n",
    "                continue\n",
    "            print(f\"Processing demonstration: {demo_dir.name}\")\n",
    "            bcd = BCDemonstration(demo_dir, sensorprocessor=None)\n",
    "            for i in range(bcd.trim_from, bcd.trim_to):\n",
    "                sensor_readings, _ = bcd.get_image(i)\n",
    "                inputlist.append(sensor_readings[0])\n",
    "                a = bcd.get_a(i)\n",
    "                rp = RobotPosition.from_vector(a)\n",
    "                anorm = rp.to_normalized_vector()\n",
    "                targetlist.append(torch.from_numpy(anorm))\n",
    "\n",
    "        retval[\"inputs\"] = torch.stack(inputlist)\n",
    "        retval[\"targets\"] = torch.stack(targetlist)\n",
    "        torch.save(retval[\"inputs\"], proprioception_input_file)\n",
    "        torch.save(retval[\"targets\"], proprioception_target_file)\n",
    "        print(f\"Saved {len(inputlist)} training examples\")\n",
    "\n",
    "    # Separate the training and validation data.\n",
    "    # We will be shuffling the demonstrations\n",
    "    length = retval[\"inputs\"].size(0)\n",
    "    rows = torch.randperm(length)\n",
    "    shuffled_inputs = retval[\"inputs\"][rows]\n",
    "    shuffled_targets = retval[\"targets\"][rows]\n",
    "\n",
    "    training_size = int(length * 0.67)\n",
    "    retval[\"inputs_training\"] = shuffled_inputs[1:training_size]\n",
    "    retval[\"targets_training\"] = shuffled_targets[1:training_size]\n",
    "\n",
    "    retval[\"inputs_validation\"] = shuffled_inputs[training_size:]\n",
    "    retval[\"targets_validation\"] = shuffled_targets[training_size:]\n",
    "\n",
    "    print(f\"Created {retval['inputs_training'].size(0)} training examples and {retval['inputs_validation'].size(0)} validation examples\")\n",
    "    return retval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory if it doesn't exist\n",
    "data_dir = pathlib.Path(exp[\"data_dir\"])\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Data directory: {data_dir}\")\n",
    "\n",
    "task = exp[\"proprioception_training_task\"]\n",
    "proprioception_input_file = pathlib.Path(exp[\"data_dir\"], exp[\"proprioception_input_file\"])\n",
    "proprioception_target_file = pathlib.Path(exp[\"data_dir\"], exp[\"proprioception_target_file\"])\n",
    "\n",
    "tr = load_images_as_proprioception_training(task, proprioception_input_file, proprioception_target_file)\n",
    "inputs_training = tr[\"inputs_training\"]\n",
    "targets_training = tr[\"targets_training\"]\n",
    "inputs_validation = tr[\"inputs_validation\"]\n",
    "targets_validation = tr[\"targets_validation\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the ViT model with proprioception regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the ViT model with proprioception\n",
    "sp = VitSensorProcessing(exp, device)\n",
    "model = sp.enc  # Get the actual encoder model for training\n",
    "\n",
    "print(\"Model created successfully\")\n",
    "\n",
    "try:\n",
    "    params = model.parameters()\n",
    "    print(\"Parameters accessed successfully\")\n",
    "    param_count = sum(p.numel() for p in params)\n",
    "    print(f\"Total parameters: {param_count}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error accessing parameters: {e}\")\n",
    "\n",
    "    # Check individual components\n",
    "    try:\n",
    "        backbone_params = model.backbone.parameters()\n",
    "        print(\"Backbone parameters accessed successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error accessing backbone parameters: {e}\")\n",
    "\n",
    "    try:\n",
    "        projection_params = model.projection.parameters()\n",
    "        print(\"Projection parameters accessed successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error accessing projection parameters: {e}\")\n",
    "\n",
    "    try:\n",
    "        proprioceptor_params = model.proprioceptor.parameters()\n",
    "        print(\"Proprioceptor parameters accessed successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error accessing proprioceptor parameters: {e}\")\n",
    "\n",
    "# Select loss function\n",
    "loss_type = exp.get('loss', 'MSELoss')\n",
    "if loss_type == 'MSELoss':\n",
    "    criterion = nn.MSELoss()\n",
    "elif loss_type == 'L1Loss':\n",
    "    criterion = nn.L1Loss()\n",
    "else:\n",
    "    criterion = nn.MSELoss()  # Default to MSE\n",
    "\n",
    "# Set up optimizer with appropriate learning rate and weight decay\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=exp.get('learning_rate', 0.001),\n",
    "    weight_decay=exp.get('weight_decay', 0.01)\n",
    ")\n",
    "\n",
    "# Optional learning rate scheduler\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=3, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders for batching\n",
    "batch_size = exp.get('batch_size', 32)\n",
    "train_dataset = TensorDataset(inputs_training, targets_training)\n",
    "test_dataset = TensorDataset(inputs_validation, targets_validation)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_proprioception_model(model, criterion, optimizer, modelfile,\n",
    "                                        device=\"cpu\", epochs=20, scheduler=None,\n",
    "                                        log_interval=1):\n",
    "    \"\"\"Trains and saves the ViT proprioception model\n",
    "\n",
    "    Args:\n",
    "        model: ViT model with proprioception\n",
    "        criterion: Loss function\n",
    "        optimizer: Optimizer\n",
    "        modelfile: Path to save the model\n",
    "        device: Training device (cpu/cuda)\n",
    "        epochs: Number of training epochs\n",
    "        scheduler: Optional learning rate scheduler\n",
    "        log_interval: How often to print logs\n",
    "    \"\"\"\n",
    "    # Ensure model is on the right device\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "\n",
    "    # Keep track of the best validation loss\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "\n",
    "            # Forward pass through the full model (including proprioceptor)\n",
    "            predictions = model.forward(batch_X)\n",
    "            loss = criterion(predictions, batch_y)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in test_loader:\n",
    "                batch_X = batch_X.to(device)\n",
    "                batch_y = batch_y.to(device)\n",
    "                predictions = model(batch_X)\n",
    "                loss = criterion(predictions, batch_y)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(test_loader)\n",
    "\n",
    "        # Update learning rate if scheduler is provided\n",
    "        if scheduler is not None:\n",
    "            scheduler.step(avg_val_loss)\n",
    "\n",
    "        # Save the best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), modelfile)\n",
    "            print(f\"  New best model saved with validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "        # Log progress\n",
    "        if (epoch + 1) % log_interval == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "    # Final evaluation\n",
    "    print(f\"Training complete. Best validation loss: {best_val_loss:.4f}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelfile = pathlib.Path(exp[\"data_dir\"], exp[\"proprioception_mlp_model_file\"])\n",
    "epochs = exp.get(\"epochs\", 20)\n",
    "\n",
    "# Check if model already exists\n",
    "if modelfile.exists() and exp.get(\"reload_existing_model\", True):\n",
    "    print(f\"Loading existing model from {modelfile}\")\n",
    "    model.load_state_dict(torch.load(modelfile, map_location=device))\n",
    "\n",
    "    # Optional: evaluate the loaded model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for batch_X, batch_y in test_loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            predictions = model(batch_X)\n",
    "            loss = criterion(predictions, batch_y)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(test_loader)\n",
    "        print(f\"Loaded model validation loss: {avg_val_loss:.4f}\")\n",
    "else:\n",
    "    print(f\"Training new model for {epochs} epochs\")\n",
    "    model = train_and_save_proprioception_model(\n",
    "        model, criterion, optimizer, modelfile,\n",
    "        device=device, epochs=epochs, scheduler=lr_scheduler\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the sensor processing module using the trained model\n",
    "sp = VitSensorProcessing(exp, device)\n",
    "\n",
    "# Test it on a few validation examples\n",
    "def test_sensor_processing(sp, test_images, test_targets, n_samples=5):\n",
    "    \"\"\"Test the sensor processing module on a few examples.\"\"\"\n",
    "    if n_samples > len(test_images):\n",
    "        n_samples = len(test_images)\n",
    "\n",
    "    # Get random indices\n",
    "    indices = torch.randperm(len(test_images))[:n_samples]\n",
    "\n",
    "    print(\"\\nTesting sensor processing on random examples:\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    for i, idx in enumerate(indices):\n",
    "        # Get image and target\n",
    "        image = test_images[idx].unsqueeze(0).to(device)  # Add batch dimension\n",
    "        target = test_targets[idx].cpu().numpy()\n",
    "\n",
    "        # Process the image to get the latent representation\n",
    "        latent = sp.process(image)\n",
    "\n",
    "        # Print the results\n",
    "        print(f\"Example {i+1}:\")\n",
    "        print(f\"  Image shape: {image.shape}\")\n",
    "        print(f\"  Latent shape: {latent.shape}\")\n",
    "        print(f\"  Target position: {target}\")\n",
    "        print()\n",
    "\n",
    "# Test the sensor processing\n",
    "test_sensor_processing(sp, inputs_validation, targets_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify the model's encoding and forward methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that the encoding method works correctly\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Get a sample image\n",
    "    sample_image = inputs_validation[0].unsqueeze(0).to(device)\n",
    "\n",
    "    # Get the latent representation using encode\n",
    "    latent = model.encode(sample_image)\n",
    "    print(f\"Latent representation shape: {latent.shape}\")\n",
    "\n",
    "    # Get the robot position prediction using forward\n",
    "    position = model.forward(sample_image)\n",
    "    print(f\"Robot position prediction shape: {position.shape}\")\n",
    "\n",
    "    # Check that the latent representation has the expected size\n",
    "    expected_latent_size = exp[\"latent_size\"]\n",
    "    assert latent.shape[1] == expected_latent_size, f\"Expected latent size {expected_latent_size}, got {latent.shape[1]}\"\n",
    "\n",
    "    # Check that the position prediction has the expected size\n",
    "    expected_output_size = exp[\"output_size\"]\n",
    "    assert position.shape[1] == expected_output_size, f\"Expected output size {expected_output_size}, got {position.shape[1]}\"\n",
    "\n",
    "    print(\"Verification successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save final model and summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model and print summary\n",
    "final_modelfile = pathlib.Path(exp[\"data_dir\"], exp[\"proprioception_mlp_model_file\"])\n",
    "torch.save(model.state_dict(), final_modelfile)\n",
    "print(f\"Model saved to {final_modelfile}\")\n",
    "\n",
    "print(\"\\nTraining complete!\")\n",
    "print(f\"Vision Transformer type: {exp['vit_model']}\")\n",
    "print(f\"Latent space dimension: {exp['latent_size']}\")\n",
    "print(f\"Output dimension (robot DOF): {exp['output_size']}\")\n",
    "print(f\"Use the VitSensorProcessing class to load and use this model for inference.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
