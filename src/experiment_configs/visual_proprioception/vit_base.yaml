
name: "vit-base-128"
model_type: "ViTProprioTunedRegression"

# Vision Transformer parameters

vit_model: "vit_b_16"  # Could be vit_b_16, vit_l_16, vit_h_14, etc.
vit_weights: "DEFAULT"  # Use DEFAULT for the standard pretrained weights
vit_output_dim: 768    # Output dimension of the base ViT model
# vit_output_dim: 1024   # for vit_l_16
# vit_output_dim: 1280   #for vit_h_14
projection_hidden_dim: 512  # For vit_b_16, recommended values: 384-512
                           # For vit_l_16, recommended values: 512-768
                           # For vit_h_14, recommended values: 768-1024


latent_size: 128       # Size of the latent embedding to generate
image_size: 224        # Required input size for the ViT model
loss: "MSE"
# Training parameters
batch_size: 32
learning_rate: 0.0001
freeze_feature_extractor: true  # Whether to freeze the ViT backbone during training
epochs: 1000
batch_size: 8
proprio_step_1: 64         # For final MLP
proprio_step_2: 64
regressor_hidden_size_1: 64
regressor_hidden_size_2: 64
output_size: 6  # Number of DoF for the robot position
resume: null
use_6d_token: false
# Training hyperparameters
encoding_size: 128

batch_size: 8

# Freezing backbone?
freeze_backbone: false

# Possibly referencing the sensorprocessing
sensor_processing: 'Vit'
sp_experiment: 'sensorprocessing_propriotuned_Vit'
sp_run: 'vit_base'

# If you want to resume from a checkpoint
resume: null

# If you have a 6D token approach, etc.
use_6d_token: false
proprioception_training_task: 'proprio_regressor_training'
proprioception_testing_task: 'proprio_regressor_validation'