
name: "sp_vit_transformer_wb"
model_type: "VisionTransformer6D"


# Model architecture parameters
vit_model: "vit_b_16"  # Could be vit_b_16, vit_l_16, vit_h_14, etc.
vit_weights: "DEFAULT"  # Use DEFAULT for the standard pretrained weights
vit_output_dim: 768    # Output dimension of the base ViT model
latent_size: 128       # Size of the latent embedding to generate
image_size: 224        # Required input size for the ViT model
loss: "MSE"
# Training parameters
batch_size: 32
learning_rate: 0.0001
freeze_feature_extractor: true  # Whether to freeze the ViT backbone during training
epochs: 1000
batch_size: 8
proprio_step_1: 64         # For final MLP
proprio_step_2: 64
regressor_hidden_size_1: 64
regressor_hidden_size_2: 64
# File paths
encoding_size: 128
in_channels: 3      # Number of input channels (RGB)

# Model architecture parameters
patch_size: 16      # Size of image patches (16Ã—16)
embed_dim: 384      # Dimension of patch embeddings
depth: 8            # Number of transformer blocks
n_heads: 6          # Number of attention heads
mlp_ratio: 4.0      # Ratio for MLP hidden dimension
qkv_bias: true      # Whether to use bias in QKV projection
latent_size: 128    # Size of the latent embedding to generate

# Regularization parameters
drop_rate: 0.1      # Dropout rate for attention output and MLP
attn_drop_rate: 0.0 # Dropout rate for attention weights

# Training parameters
batch_size: 32
learning_rate: 0.0003
weight_decay: 0.01
proprioception_input_file: train_inputs.pt
proprioception_target_file: train_targets.pt
proprioception_test_input_file: test_inputs.pt
proprioception_test_target_file: test_targets.pt

# Task names
proprioception_training_task: 'proprio_regressor_training'
proprioception_testing_task: 'proprio_regressor_validation'

# Output parameters for regression
output_size: 6  # Number of DoF for the robot position
resume: null
use_6d_token: false
# Note: This configuration is required for the sp_vit.py module.
# All values (including 128 for latent_size and 224 for image_size)
# are specified here and not hardcoded in the implementation.