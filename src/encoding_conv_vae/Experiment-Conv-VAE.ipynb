{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactively validate a Convolutional Variational Autoencoder\n",
    "\n",
    "The code in this nodebook is using a pre-trained Conv-VAE package to load a pre-trained model and experiment with the pre-trained encoded and decoder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceeding to load config file: G:\\My Drive\\LotziStudy\\Code\\PackageTracking\\VisionBasedRobotManipulator\\settings\\settings-LotziYoga.yaml\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from settings import Config\n",
    "sys.path.append(Config().values[\"conv_vae\"][\"code_dir\"])\n",
    "\n",
    "# print(Config().values)\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# these imports are from the Conv-VAE package\n",
    "import data_loader.data_loaders as module_data\n",
    "import model.loss as module_loss\n",
    "import model.metric as module_metric\n",
    "import model.model as module_arch\n",
    "from parse_config import ConfigParser\n",
    "from torch.nn import functional as F\n",
    "import torchvision.utils as vutils\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import socket\n",
    "\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "#Fixes PosixPath Error\n",
    "import pathlib\n",
    "\n",
    "# Oh, this hack was fixing something, but for me it is the other way around\n",
    "#temp = pathlib.PosixPath\n",
    "#pathlib.PosixPath = pathlib.WindowsPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As the code is highly dependent on the command line, emulating it here\n",
    "args = argparse.ArgumentParser(description='PyTorch Template')\n",
    "args.add_argument('-c', '--config', default=None, type=str,\n",
    "                     help='config file path (default: None)')\n",
    "args.add_argument('-r', '--resume', default=None, type=str,\n",
    "                      help='path to latest checkpoint (default: None)')\n",
    "args.add_argument('-d', '--device', default=None, type=str,\n",
    "                      help='indices of GPUs to enable (default: all)')\n",
    "\n",
    "model_path = pathlib.Path(Config().values[\"conv_vae\"][\"model_dir\"])\n",
    "model_path = pathlib.Path(model_path, \"models\", Config().values[\"conv_vae\"][\"model_type\"])\n",
    "latest = latest_training_run(model_path)\n",
    "print(latest)\n",
    "model_path = pathlib.Path(model_path, latest)\n",
    "model = latest_model(model_path)\n",
    "\n",
    "print(model_path)\n",
    "# how to pick the latest file\n",
    "print(model)  # Output: 87\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#model_path = pathlib.Path(VAE_MODEL_DIR, VAE_MODEL_FILE).resolve()\n",
    "\n",
    "value = [\"this-script\", f\"-c{VAE_CONFIG}\", f\"-r{model_path}\"]\n",
    "\n",
    "# we are changing the parameters from here, to avoid changing the github \n",
    "# downloaded package\n",
    "sys.argv = value\n",
    "config = ConfigParser.from_args(args)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = config.get_logger('test')\n",
    "\n",
    "# setup data_loader instances\n",
    "# LOTZI: this is a specific data loader, which with the current implementation, it is very specific to the celeba. It needs to be changed...\n",
    "\n",
    "# FIXME: Initially I changed this data loader to something else and added into the data_loader a new one called RobotDataLoader. \n",
    "# But I should not change the github downloaded package. \n",
    "# So for the time being, I changed it back to CelebDataLoader, with the \n",
    "# awareness that I can specify something in config and update it correspondingly here \n",
    "\n",
    "data_loader = getattr(module_data, config['data_loader']['type'])(\n",
    "    config['data_loader']['args']['data_dir'],\n",
    "    batch_size=36,\n",
    "    shuffle=False,\n",
    "    validation_split=0.0,\n",
    "    # training=False,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "# LOTZI: this is an uninitialized model architecture\n",
    "# build model architecture\n",
    "model = config.init_obj('arch', module_arch)\n",
    "logger.info(model)\n",
    "\n",
    "# get function handles of loss and metrics\n",
    "loss_fn = getattr(module_loss, config['loss'])\n",
    "# metric_fns = [getattr(module_metric, met) for met in config['metrics']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logger.info('Loading checkpoint: {} ...'.format(config.resume))\n",
    "# checkpoint = torch.load(config.resume)\n",
    "\n",
    "# loading on CPU-only machine\n",
    "print(\"Loading the checkpoint\")\n",
    "checkpoint = torch.load(config.resume, map_location=torch.device('cpu'))\n",
    "print(\"Checkpoint loaded\")\n",
    "\n",
    "state_dict = checkpoint['state_dict']\n",
    "if config['n_gpu'] > 1:\n",
    "    model = torch.nn.DataParallel(model)\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "# prepare model for testing\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)\n",
    "print(model.encoder)\n",
    "print(f\"latent_dim {model.latent_dim}\")\n",
    "# print(model.hidden_dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pass one picture through the complete autoencoder\n",
    "\n",
    "Pass one specific picture through the complete autoencoder. Compare the input and the output. This is basically trying out whether the VAE had captured the picture sufficiently.\n",
    "\n",
    "This code also is intended as a sample of how to use the pre-trained model, how to feed it new data without the training code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "# picture_name = '../../localdata/img/Rafael.jpg'\n",
    "# picture_name = '../../localdata/img/indian-man.jpeg'\n",
    "# picture_name = '../../localdata/img/00029_dev2.jpg'\n",
    "\n",
    "# This is just a randomly chosen image from the training data. \n",
    "\n",
    "directory = pathlib.Path(VAE_TRAININGDATA_DIR, \"Class-Folder\")\n",
    "\n",
    "files = list(directory.glob('*.jpg'))\n",
    "picture_name = pathlib.Path(directory, files[11])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#\"ID_0046_AGE_0072_CONTRAST_1_CT.jpg\")\n",
    "# Load an image using PIL\n",
    "image = Image.open(picture_name)\n",
    "\n",
    "plt.imshow(image)\n",
    "\n",
    "print(image.mode)\n",
    "# at least for the medical image, this is in 16 bit unsigned integer\n",
    "# \n",
    "image = image.convert(\"RGB\")\n",
    "\n",
    "plt.imshow(image)\n",
    "\n",
    "# Define a transform pipeline (e.g., converting to tensor)\n",
    "# transform = transforms.ToTensor()\n",
    "image_size = 64\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.CenterCrop(image_size),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "# Apply the transform\n",
    "image_tensor = transform(image)\n",
    "\n",
    "# Display some information about the image tensor\n",
    "print(image_tensor.shape)  # e.g., torch.Size([3, H, W])\n",
    "\n",
    "# Convert the tensor to a format suitable for matplotlib (from [C, H, W] to [H, W, C])\n",
    "image_tensor_for_pic = image_tensor.permute(1, 2, 0)\n",
    "#plt.imshow(image_tensor_for_pic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add a batch dimension: shape becomes [1, 3, 224, 224]\n",
    "image_tensor_2 = image_tensor.unsqueeze(0)\n",
    "\n",
    "# Move tensor to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "image_tensor_3 = image_tensor_2.to(device)\n",
    "\n",
    "# Running the input on the output\n",
    "output, mu, logvar = model(image_tensor_3)\n",
    "\n",
    "# Output: the visual reconstruction\n",
    "#print(output)\n",
    "output_for_pic = output[0].cpu().permute(1, 2, 0).detach().numpy()\n",
    "plt.imshow(output_for_pic)\n",
    "\n",
    "# mu: the encoding, I think\n",
    "print(f\"mu = {mu}\")\n",
    "\n",
    "# log_var: the log-var values for this input \n",
    "print(f\"logvar = {logvar}\")\n",
    "\n",
    "# initial and new\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "axs[0].imshow(image_tensor_for_pic)\n",
    "axs[1].imshow(output_for_pic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Try to do a decoding from the same mu\n",
    "z2 = model.reparameterize(mu, logvar)\n",
    "\n",
    "for i in range(512):\n",
    "    z2[0][i] = \n",
    "\n",
    "#z2[0][1] = 2.0\n",
    "#z2[0][3] = 2.0\n",
    "output2 = model.decode(z2)\n",
    "output_for_pic2 = output2[0].cpu().permute(1, 2, 0).detach().numpy()\n",
    "\n",
    "# initial and new\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "axs[0].imshow(output_for_pic)\n",
    "axs[1].imshow(output_for_pic2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating random samples from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# z2 = model.reparameterize(mu, logvar)\n",
    "# initial and new\n",
    "fig, axs = plt.subplots(5, 5, figsize=(10, 5))\n",
    "for x in range(0, 5):\n",
    "    for y in range(0, 5):\n",
    "        for i in range(512):\n",
    "            z2 = model.reparameterize(mu, logvar)\n",
    "            z2[0][i] = random.uniform(-0.5, 0.5)\n",
    "            output2 = model.decode(z2)\n",
    "            output_for_pic2 = output2[0].cpu().permute(1, 2, 0).detach().numpy()\n",
    "            axs[x][y].imshow(output_for_pic2)\n",
    "#axs[0].imshow(output_for_pic)\n",
    "#axs[1].imshow(output_for_pic2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
