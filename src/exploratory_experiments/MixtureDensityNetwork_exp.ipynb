{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MDN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_gaussians):\n",
    "        super(MDN, self).__init__()\n",
    "        \n",
    "        # Define layers\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        # Outputs: means, stds, and mixing coefficients for each Gaussian\n",
    "        self.pi_layer = nn.Linear(hidden_dim, num_gaussians)  # Mixing coefficients\n",
    "        self.mu_layer = nn.Linear(hidden_dim, num_gaussians)  # Means\n",
    "        self.sigma_layer = nn.Linear(hidden_dim, num_gaussians)  # Standard deviations\n",
    "        \n",
    "        self.num_gaussians = num_gaussians\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Forward pass through hidden layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        # Output layers for GMM parameters\n",
    "        pi = F.softmax(self.pi_layer(x), dim=1)  # Mixing coefficients (softmax for normalization)\n",
    "        mu = self.mu_layer(x)  # Means of Gaussians\n",
    "        sigma = torch.exp(self.sigma_layer(x))  # Std devs (exponential to ensure positivity)\n",
    "        \n",
    "        return pi, mu, sigma\n",
    "\n",
    "# Example usage:\n",
    "# Define input dimensions and model parameters\n",
    "input_dim = 1  # Example: single feature input\n",
    "hidden_dim = 20\n",
    "num_gaussians = 3  # Number of Gaussian components in the mixture\n",
    "\n",
    "# Instantiate the MDN model\n",
    "mdn = MDN(input_dim, hidden_dim, num_gaussians)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mdn_loss(pi, mu, sigma, target):\n",
    "    # Reshape for broadcasting\n",
    "    target = target.view(-1, 1)  # Reshape target to (batch_size, 1)\n",
    "\n",
    "    # Calculate Gaussian probabilities for each component\n",
    "    gaussian = torch.exp(-0.5 * ((target - mu) / sigma)**2) / (sigma * torch.sqrt(torch.tensor(2 * torch.pi)))\n",
    "    \n",
    "    # Weighted sum of Gaussians using pi as the weights\n",
    "    weighted_gaussian = pi * gaussian\n",
    "    probability_density = torch.sum(weighted_gaussian, dim=1)  # Sum over all components\n",
    "    \n",
    "    # Negative log-likelihood\n",
    "    nll = -torch.log(probability_density + 1e-10)  # Add epsilon for numerical stability\n",
    "    return torch.mean(nll)\n",
    "\n",
    "# Example usage:\n",
    "# Assume `inputs` is your input data tensor and `targets` are the ground-truth outputs\n",
    "inputs = torch.randn(5, input_dim)  # Example input batch\n",
    "targets = torch.randn(5)  # Example target batch\n",
    "\n",
    "# Forward pass through MDN to get pi, mu, sigma\n",
    "pi, mu, sigma = mdn(inputs)\n",
    "\n",
    "# Calculate MDN loss\n",
    "loss = mdn_loss(pi, mu, sigma, targets)\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(mdn.parameters(), lr=0.001)\n",
    "\n",
    "# Example training loop\n",
    "for epoch in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    pi, mu, sigma = mdn(inputs)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = mdn_loss(pi, mu, sigma, targets)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
