{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual proprioception experiments\n",
    "\n",
    "Experiments whether the robot can recover its own positional parameters from the visual input. This is useful in itself, but it is also a reasonable sanity test of whether the vision system is meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pointer config file: /home/lboloni/.config/BerryPicker/mainsettings.yaml\n",
      "Loading machine-specific config file: /home/lboloni/Insync/lotzi.boloni@gmail.com/Google Drive/LotziStudy/Code/PackageTracking/BerryPicker/settings/settings-tredy2.yaml\n",
      "Missing experiment system dependent config file /home/lboloni/Insync/lotzi.boloni@gmail.com/Google Drive/LotziStudy/Code/PackageTracking/BerryPicker/settings/experiment-config/Tredy2/visual_proprioception/vae_mlp_00_sysdep.yaml, that is ok, proceeding.\n",
      "Configuration for experiment: visual_proprioception/vae_mlp_00 successfully loaded\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'exp_config' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msensorprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sp_conv_vae\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrobot\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mal5d_position_controller\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RobotPosition\n\u001b[0;32m---> 24\u001b[0m exp \u001b[38;5;241m=\u001b[39m \u001b[43mConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvisual_proprioception\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvae_mlp_00\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(exp)\n",
      "File \u001b[0;32m~/Documents/Hackingwork/_Checkouts/BerryPicker/BerryPicker/src/exploratory_experiments/../settings.py:92\u001b[0m, in \u001b[0;36mConfig.get_experiment\u001b[0;34m(self, group_name, run_name)\u001b[0m\n\u001b[1;32m     88\u001b[0m     exp_config \u001b[38;5;241m=\u001b[39m indep_config \u001b[38;5;241m|\u001b[39m dep_config\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConfiguration for experiment: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgroup_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m successfully loaded\u001b[39m\u001b[38;5;124m\"\u001b[39m, flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 92\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mexp_config\u001b[49m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'exp_config' referenced before assignment"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from settings import Config\n",
    "\n",
    "import pathlib\n",
    "#from pprint import pformat\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "from behavior_cloning.demo_to_trainingdata import BCDemonstration\n",
    "from sensorprocessing import sp_conv_vae\n",
    "from robot.al5d_position_controller import RobotPosition\n",
    "\n",
    "exp = Config().get_experiment(\"visual_proprioception\", \"vae_mlp_00\")\n",
    "print(exp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the training and validation data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_demonstrations_as_proprioception_training(sp, task, proprioception_input_file, proprioception_target_file):\n",
    "    \"\"\"Loads all the images of a task, and processes it as two tensors as input and target data for proprioception training. \n",
    "    Caches the processed results into the input and target file pointed in the config. Remove those files to recalculate\n",
    "    \"\"\"\n",
    "    retval = {}\n",
    "    if proprioception_input_file.exists():\n",
    "        retval[\"inputs\"] = torch.load(proprioception_input_file, weights_only=True)\n",
    "        retval[\"targets\"] = torch.load(proprioception_target_file, weights_only=True)\n",
    "    else:\n",
    "        demos_dir = pathlib.Path(Config()[\"demos\"][\"directory\"])\n",
    "        task_dir = pathlib.Path(demos_dir, \"demos\", task)\n",
    "        \n",
    "        inputlist = []\n",
    "        targetlist = []\n",
    "\n",
    "        for demo_dir in task_dir.iterdir():\n",
    "            if not demo_dir.is_dir():\n",
    "                pass\n",
    "            bcd = BCDemonstration(demo_dir, sensorprocessor=sp)\n",
    "            print(bcd)\n",
    "            z, a = bcd.read_z_a()\n",
    "            # normalize the actions\n",
    "            print(z.shape)\n",
    "            print(a.shape)\n",
    "            anorm = np.zeros(a.shape, np.float32)\n",
    "            for i in range(a.shape[0]):\n",
    "                rp = RobotPosition.from_vector(a[i])\n",
    "                anorm[i,:] = rp.to_normalized_vector()        \n",
    "            # FIXME the repeated name for inputs and targets\n",
    "            print(z.shape)\n",
    "            print(anorm.shape)\n",
    "\n",
    "            for i in range(z.shape[0]):\n",
    "                inp = torch.from_numpy(z[i])\n",
    "                tgt = torch.from_numpy(anorm[i])\n",
    "                inputlist.append(inp)\n",
    "                targetlist.append(tgt)\n",
    "\n",
    "        retval[\"inputs\"] = torch.stack(inputlist)\n",
    "        retval[\"targets\"] = torch.stack(targetlist)\n",
    "        torch.save(retval[\"inputs\"], proprioception_input_file)\n",
    "        torch.save(retval[\"targets\"], proprioception_target_file)\n",
    "\n",
    "    # Separate the training and validation data. \n",
    "    # We will be shuffling the demonstrations \n",
    "    length = retval[\"inputs\"].size(0)\n",
    "    rows = torch.randperm(length) \n",
    "    shuffled_inputs = retval[\"inputs\"][rows]\n",
    "    shuffled_targets = retval[\"targets\"][rows]\n",
    "\n",
    "    training_size = int( length * 0.67 )\n",
    "    retval[\"inputs_training\"] = shuffled_inputs[1:training_size]\n",
    "    retval[\"targets_training\"] = shuffled_targets[1:training_size]\n",
    "\n",
    "    retval[\"inputs_validation\"] = shuffled_inputs[training_size:]\n",
    "    retval[\"targets_validation\"] = shuffled_targets[training_size:]\n",
    "\n",
    "    return retval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the MLP regression model\n",
    "class MLPRegression(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLPRegression, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it seems to work with this one very well...\n",
    "# task = \"proprioception-cluttered\"\n",
    "\n",
    "conv_vae_jsonfile = Config()[\"controller\"][\"vae_json\"]\n",
    "conv_vae_resume_model_pthfile = Config()[\"controller\"][\"vae_model\"]\n",
    "sp = sp_conv_vae.ConvVaeSensorProcessing(conv_vae_jsonfile, conv_vae_resume_model_pthfile)\n",
    "\n",
    "task = exp[\"proprioception_training_task\"]\n",
    "proprioception_input_file = pathlib.Path(exp[\"data_dir\"], \n",
    "                                            exp[\"proprioception_input_file\"])\n",
    "proprioception_target_file = pathlib.Path(exp[\"data_dir\"], \n",
    "                                            exp[\"proprioception_target_file\"])\n",
    "tr = load_demonstrations_as_proprioception_training(sp, task, \n",
    "                                                    proprioception_input_file, proprioception_target_file)\n",
    "inputs_training = tr[\"inputs_training\"]\n",
    "targets_training = tr[\"targets_training\"]\n",
    "inputs_validation = tr[\"inputs_validation\"]\n",
    "targets_validation = tr[\"targets_validation\"]\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "input_size = inputs_training.size(1)\n",
    "hidden_size = 64\n",
    "output_size = targets_training.size(1)\n",
    "\n",
    "print(input_size)\n",
    "print(output_size)\n",
    "\n",
    "model = MLPRegression(input_size, hidden_size, output_size)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders for batching\n",
    "batch_size = 32\n",
    "train_dataset = TensorDataset(inputs_training, targets_training)\n",
    "test_dataset = TensorDataset(inputs_validation, targets_validation)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_proprioception_model(modelfile, epochs=20):\n",
    "    \"\"\"Trains and saves the proprioception model\n",
    "    FIXME: must have parameters etc to investigate alternative models. \n",
    "    \"\"\"\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            # Forward pass\n",
    "            predictions = model(batch_X)\n",
    "            loss = criterion(predictions, batch_y)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss / len(train_loader):.4f}')\n",
    "\n",
    "    # Evaluate the model\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in test_loader:\n",
    "            predictions = model(batch_X)\n",
    "            loss = criterion(predictions, batch_y)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    print(f'Test Loss: {test_loss:.4f}')\n",
    "    torch.save(model.state_dict(), modelfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelfile = pathlib.Path(Config()[\"explorations\"][\"proprioception_mlp_model_file\"])\n",
    "modelfile = pathlib.Path(exp[\"data_dir\"], \n",
    "                         exp[\"proprioception_mlp_model_file\"])\n",
    "epochs = exp[\"epochs\"]\n",
    "if modelfile.exists():\n",
    "    model.load_state_dict(torch.load(modelfile))\n",
    "else:\n",
    "    train_and_save_proprioception_model(modelfile, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if the reloaded model works\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        predictions = model(batch_X)\n",
    "        loss = criterion(predictions, batch_y)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "test_loss /= len(test_loader)\n",
    "print(f'Test Loss: {test_loss:.4f}')\n",
    "torch.save(model.state_dict(), modelfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the proprioception\n",
    "Run the model with the original input.\n",
    "FIXME: here we need to make a different set of tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y = []\n",
    "\n",
    "task = exp[\"proprioception_testing_task\"]\n",
    "proprioception_input_file = pathlib.Path(\n",
    "    exp[\"datadir\"], exp[\"proprioception_test_input_file\"])\n",
    "proprioception_target_file = pathlib.Path(\n",
    "    exp[\"data_dir\"], exp[\"proprioception_test_target_file\"])\n",
    "tr2 = load_demonstrations_as_proprioception_training(\n",
    "    sp, task, proprioception_input_file, proprioception_target_file)\n",
    "\n",
    "inputs = tr2[\"inputs\"] # these are actually tensors\n",
    "targets = tr2[\"targets\"]\n",
    "no_from = 0\n",
    "no_to = inputs.shape[0]\n",
    "ypred = []\n",
    "y = []\n",
    "t = []\n",
    "with torch.no_grad():\n",
    "    for i in range(no_from, no_to):\n",
    "        x = inputs[i]\n",
    "        predictions = model(torch.unsqueeze(x, dim=0))\n",
    "        # append the data \n",
    "        t.append(i)\n",
    "        y.append(targets[i].numpy())\n",
    "        ypred.append(predictions[0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = np.array(ypred)\n",
    "y = np.array(y)\n",
    "t = np.array(t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a graph with the six degrees of freedom, predicted and real value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2,3, constrained_layout=True)\n",
    "titles = [\"height\",\"distance\", \"heading\", \"wrist_angle\", \"wrist_rotation\", \"gripper\"]\n",
    "for i in range(Config()[\"robot\"][\"action_space_size\"]):\n",
    "    ax = axs[i//3, i%3] \n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.plot(t, y[:,i], label=\"y\")\n",
    "    ax.plot(t, ypred[:,i], label=\"yhat\")\n",
    "    ax.legend()\n",
    "    ax.set_title(titles[i])\n",
    "\n",
    "graphfilename = pathlib.Path(exp[\"data_dir\"], \"proprio_error.pdf\")\n",
    "plt.savefig(graphfilename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
