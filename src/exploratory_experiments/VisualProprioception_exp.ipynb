{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual proprioception experiments\n",
    "\n",
    "Experiments whether the robot can recover its own positional parameters from the visual input. This is useful in itself, but it is also a reasonable sanity test of whether the vision system is meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pointer config file: C:\\Users\\lboloni\\.config\\BerryPicker\\mainsettings.yaml\n",
      "Loading machine-specific config file: G:\\My Drive\\LotziStudy\\Code\\PackageTracking\\BerryPicker\\settings\\settings-LotziYoga.yaml\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from settings import Config\n",
    "\n",
    "import pathlib\n",
    "#from pprint import pformat\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "from behavior_cloning.demo_to_trainingdata import BCDemonstration\n",
    "from sensorprocessing import sp_conv_vae, sp_cnn\n",
    "from robot.al5d_position_controller import RobotPosition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the training and validation data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_demonstrations_as_proprioception_training(sp, task, proprioception_input_file, proprioception_target_file):\n",
    "    \"\"\"Loads all the images of a task, and processes it as two tensors as input and target data for proprioception training. \n",
    "    Caches the processed results into the input and target file pointed in the config. Remove those files to recalculate\n",
    "    \"\"\"\n",
    "    retval = {}\n",
    "    if proprioception_input_file.exists():\n",
    "        retval[\"inputs\"] = torch.load(proprioception_input_file, weights_only=True)\n",
    "        retval[\"targets\"] = torch.load(proprioception_target_file, weights_only=True)\n",
    "    else:\n",
    "        demos_dir = pathlib.Path(Config()[\"demos\"][\"directory\"])\n",
    "        task_dir = pathlib.Path(demos_dir, \"demos\", task)\n",
    "        \n",
    "        inputlist = []\n",
    "        targetlist = []\n",
    "\n",
    "        for demo_dir in task_dir.iterdir():\n",
    "            if not demo_dir.is_dir():\n",
    "                pass\n",
    "            bcd = BCDemonstration(demo_dir, sensorprocessor=sp)\n",
    "            print(bcd)\n",
    "            z, a = bcd.read_z_a()\n",
    "            # normalize the actions\n",
    "            print(z.shape)\n",
    "            print(a.shape)\n",
    "            anorm = np.zeros(a.shape, np.float32)\n",
    "            for i in range(a.shape[0]):\n",
    "                rp = RobotPosition.from_vector(a[i])\n",
    "                anorm[i,:] = rp.to_normalized_vector()        \n",
    "            # FIXME the repeated name for inputs and targets\n",
    "            print(z.shape)\n",
    "            print(anorm.shape)\n",
    "\n",
    "            for i in range(z.shape[0]):\n",
    "                inp = torch.from_numpy(z[i])\n",
    "                tgt = torch.from_numpy(anorm[i])\n",
    "                inputlist.append(inp)\n",
    "                targetlist.append(tgt)\n",
    "\n",
    "        retval[\"inputs\"] = torch.stack(inputlist)\n",
    "        retval[\"targets\"] = torch.stack(targetlist)\n",
    "        torch.save(retval[\"inputs\"], proprioception_input_file)\n",
    "        torch.save(retval[\"targets\"], proprioception_target_file)\n",
    "\n",
    "    # Separate the training and validation data. \n",
    "    # We will be shuffling the demonstrations \n",
    "    length = retval[\"inputs\"].size(0)\n",
    "    rows = torch.randperm(length) \n",
    "    shuffled_inputs = retval[\"inputs\"][rows]\n",
    "    shuffled_targets = retval[\"targets\"][rows]\n",
    "\n",
    "    training_size = int( length * 0.67 )\n",
    "    retval[\"inputs_training\"] = shuffled_inputs[1:training_size]\n",
    "    retval[\"targets_training\"] = shuffled_targets[1:training_size]\n",
    "\n",
    "    retval[\"inputs_validation\"] = shuffled_inputs[training_size:]\n",
    "    retval[\"targets_validation\"] = shuffled_targets[training_size:]\n",
    "\n",
    "    return retval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the MLP regression model\n",
    "class MLPRegression(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLPRegression, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing experiment system dependent config file G:\\My Drive\\LotziStudy\\Code\\PackageTracking\\BerryPicker\\settings\\experiment-config\\LotziYoga\\visual_proprioception\\vae_mlp_01_sysdep.yaml, that is ok, proceeding.\n",
      "Configuration for experiment: visual_proprioception/vae_mlp_01 successfully loaded\n",
      "{'proprioception_mlp_model_file': 'proprioception_mlp.pth', 'proprioception_input_file': 'train_inputs.pt', 'proprioception_target_file': 'train_targets.pt', 'proprioception_test_input_file': 'test_inputs.pt', 'proprioception_test_target_file': 'test_targets.pt', 'epochs': 3000, 'group_name': 'visual_proprioception', 'proprioception_training_task': 'random-uncluttered', 'proprioception_testing_task': 'random-uncluttered-test', 'run_name': 'vae_mlp_01', 'data_dir': WindowsPath('c:/Users/lboloni/Documents/Code/_TempData/BerryPicker-experiments/visual_proprioception/vae_mlp_01')}\n",
      "Missing experiment system dependent config file G:\\My Drive\\LotziStudy\\Code\\PackageTracking\\BerryPicker\\settings\\experiment-config\\LotziYoga\\sp_cnn\\vgg19_orig_sysdep.yaml, that is ok, proceeding.\n",
      "Configuration for experiment: sp_cnn/vgg19_orig successfully loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lboloni\\Documents\\Code\\_VirtualEnvironments\\Robot\\Robot-venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lboloni\\Documents\\Code\\_VirtualEnvironments\\Robot\\Robot-venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "c:\\Users\\lboloni\\Documents\\Code\\_Checkouts\\BerryPicker\\src\\exploratory_experiments\\..\\sensorprocessing\\sp_cnn.py:78: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.enc.load_state_dict(torch.load(modelfile))\n"
     ]
    }
   ],
   "source": [
    "# run = \"vae_mlp_00\"\n",
    "run = \"vae_mlp_01\"\n",
    "exp = Config().get_experiment(\"visual_proprioception\", run)\n",
    "print(exp)\n",
    "\n",
    "sptype = \"vgg19\"\n",
    "\n",
    "if sptype == \"vae\":\n",
    "    spexp = Config().get_experiment(\"conv_vae\", \"vae_01\")\n",
    "    sp = sp_conv_vae.get_sp_of_conv_vae_experiment(\"vae_01\")\n",
    "if sptype == \"vgg19\":\n",
    "    spexp = Config().get_experiment(\"sp_cnn\", \"vgg19_orig\")\n",
    "    sp = sp_cnn.VGG19SensorProcessing(spexp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cameras found: ['dev2']\n",
      "There are 596 steps in this demonstration\n",
      "This demonstration was recorded by the following cameras: ['dev2']\n",
      "{'actiontype': 'rc-position-target',\n",
      " 'camera': 'dev2',\n",
      " 'cameras': ['dev2'],\n",
      " 'maxsteps': 596,\n",
      " 'sensorprocessor': <sensorprocessing.sp_cnn.VGG19SensorProcessing object at 0x000001A5AC34F4D0>,\n",
      " 'source_dir': WindowsPath('C:/Users/lboloni/Documents/Code/_TempData/BerryPicker-demos/demos/random-uncluttered/2024_12_26__16_40_20'),\n",
      " 'trim_from': 1,\n",
      " 'trim_to': 596}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'VGG19SensorProcessing' object has no attribute 'transform'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m proprioception_input_file \u001b[38;5;241m=\u001b[39m pathlib\u001b[38;5;241m.\u001b[39mPath(exp[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m], \n\u001b[0;32m      3\u001b[0m                                             exp[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproprioception_input_file\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m      4\u001b[0m proprioception_target_file \u001b[38;5;241m=\u001b[39m pathlib\u001b[38;5;241m.\u001b[39mPath(exp[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m], \n\u001b[0;32m      5\u001b[0m                                             exp[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproprioception_target_file\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m----> 6\u001b[0m tr \u001b[38;5;241m=\u001b[39m \u001b[43mload_demonstrations_as_proprioception_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43msp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43mproprioception_input_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproprioception_target_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m inputs_training \u001b[38;5;241m=\u001b[39m tr[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs_training\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      9\u001b[0m targets_training \u001b[38;5;241m=\u001b[39m tr[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtargets_training\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "Cell \u001b[1;32mIn[2], line 21\u001b[0m, in \u001b[0;36mload_demonstrations_as_proprioception_training\u001b[1;34m(sp, task, proprioception_input_file, proprioception_target_file)\u001b[0m\n\u001b[0;32m     19\u001b[0m bcd \u001b[38;5;241m=\u001b[39m BCDemonstration(demo_dir, sensorprocessor\u001b[38;5;241m=\u001b[39msp)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(bcd)\n\u001b[1;32m---> 21\u001b[0m z, a \u001b[38;5;241m=\u001b[39m \u001b[43mbcd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_z_a\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# normalize the actions\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(z\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32mc:\\Users\\lboloni\\Documents\\Code\\_Checkouts\\BerryPicker\\src\\exploratory_experiments\\..\\behavior_cloning\\demo_to_trainingdata.py:85\u001b[0m, in \u001b[0;36mBCDemonstration.read_z_a\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m a \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrim_from, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrim_to):\n\u001b[1;32m---> 85\u001b[0m     zval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_z\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;66;03m# print(zval.cpu())\u001b[39;00m\n\u001b[0;32m     87\u001b[0m     z\u001b[38;5;241m.\u001b[39mappend(zval)\n",
      "File \u001b[1;32mc:\\Users\\lboloni\\Documents\\Code\\_Checkouts\\BerryPicker\\src\\exploratory_experiments\\..\\behavior_cloning\\demo_to_trainingdata.py:97\u001b[0m, in \u001b[0;36mBCDemonstration.get_z\u001b[1;34m(self, i)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_z\u001b[39m(\u001b[38;5;28mself\u001b[39m, i):\n\u001b[0;32m     96\u001b[0m     filepath \u001b[38;5;241m=\u001b[39m pathlib\u001b[38;5;241m.\u001b[39mPath(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m05d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcamera\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 97\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msensorprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m val\n",
      "File \u001b[1;32mc:\\Users\\lboloni\\Documents\\Code\\_Checkouts\\BerryPicker\\src\\exploratory_experiments\\..\\sensorprocessing\\sensor_processing.py:24\u001b[0m, in \u001b[0;36mAbstractSensorProcessing.process_file\u001b[1;34m(self, sensor_readings_file)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_file\u001b[39m(\u001b[38;5;28mself\u001b[39m, sensor_readings_file):\n\u001b[0;32m     23\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Processsed file\"\"\"\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m     sensor_readings, image \u001b[38;5;241m=\u001b[39m load_picturefile_to_tensor(sensor_readings_file, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m)\n\u001b[0;32m     25\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess(sensor_readings)\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'VGG19SensorProcessing' object has no attribute 'transform'"
     ]
    }
   ],
   "source": [
    "task = exp[\"proprioception_training_task\"]\n",
    "proprioception_input_file = pathlib.Path(exp[\"data_dir\"], \n",
    "                                            exp[\"proprioception_input_file\"])\n",
    "proprioception_target_file = pathlib.Path(exp[\"data_dir\"], \n",
    "                                            exp[\"proprioception_target_file\"])\n",
    "tr = load_demonstrations_as_proprioception_training(sp, task, \n",
    "                                                    proprioception_input_file, proprioception_target_file)\n",
    "inputs_training = tr[\"inputs_training\"]\n",
    "targets_training = tr[\"targets_training\"]\n",
    "inputs_validation = tr[\"inputs_validation\"]\n",
    "targets_validation = tr[\"targets_validation\"]\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "input_size = inputs_training.size(1)\n",
    "hidden_size = 64\n",
    "output_size = targets_training.size(1)\n",
    "\n",
    "print(input_size)\n",
    "print(output_size)\n",
    "\n",
    "model = MLPRegression(input_size, hidden_size, output_size)\n",
    "# criterion = nn.MSELoss()\n",
    "# Experiment: would this be better???\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders for batching\n",
    "batch_size = 32\n",
    "train_dataset = TensorDataset(inputs_training, targets_training)\n",
    "test_dataset = TensorDataset(inputs_validation, targets_validation)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_proprioception_model(modelfile, epochs=20):\n",
    "    \"\"\"Trains and saves the proprioception model\n",
    "    FIXME: must have parameters etc to investigate alternative models. \n",
    "    \"\"\"\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            # Forward pass\n",
    "            predictions = model(batch_X)\n",
    "            loss = criterion(predictions, batch_y)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss / len(train_loader):.4f}')\n",
    "\n",
    "    # Evaluate the model\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in test_loader:\n",
    "            predictions = model(batch_X)\n",
    "            loss = criterion(predictions, batch_y)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    print(f'Test Loss: {test_loss:.4f}')\n",
    "    torch.save(model.state_dict(), modelfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelfile = pathlib.Path(Config()[\"explorations\"][\"proprioception_mlp_model_file\"])\n",
    "modelfile = pathlib.Path(exp[\"data_dir\"], \n",
    "                         exp[\"proprioception_mlp_model_file\"])\n",
    "epochs = exp[\"epochs\"]\n",
    "if modelfile.exists():\n",
    "    model.load_state_dict(torch.load(modelfile))\n",
    "else:\n",
    "    train_and_save_proprioception_model(modelfile, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if the reloaded model works\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        predictions = model(batch_X)\n",
    "        loss = criterion(predictions, batch_y)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "test_loss /= len(test_loader)\n",
    "print(f'Test Loss: {test_loss:.4f}')\n",
    "torch.save(model.state_dict(), modelfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the proprioception\n",
    "Run the model with the original input.\n",
    "FIXME: here we need to make a different set of tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y = []\n",
    "\n",
    "task = exp[\"proprioception_testing_task\"]\n",
    "proprioception_input_file = pathlib.Path(\n",
    "    exp[\"data_dir\"], exp[\"proprioception_test_input_file\"])\n",
    "proprioception_target_file = pathlib.Path(\n",
    "    exp[\"data_dir\"], exp[\"proprioception_test_target_file\"])\n",
    "tr2 = load_demonstrations_as_proprioception_training(\n",
    "    sp, task, proprioception_input_file, proprioception_target_file)\n",
    "\n",
    "inputs = tr2[\"inputs\"] # these are actually tensors\n",
    "targets = tr2[\"targets\"]\n",
    "no_from = 0\n",
    "no_to = inputs.shape[0]\n",
    "ypred = []\n",
    "y = []\n",
    "t = []\n",
    "with torch.no_grad():\n",
    "    for i in range(no_from, no_to):\n",
    "        x = inputs[i]\n",
    "        predictions = model(torch.unsqueeze(x, dim=0))\n",
    "        # append the data \n",
    "        t.append(i)\n",
    "        y.append(targets[i].numpy())\n",
    "        ypred.append(predictions[0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = np.array(ypred)\n",
    "y = np.array(y)\n",
    "t = np.array(t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a graph with the six degrees of freedom, predicted and real value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2,3, constrained_layout=True)\n",
    "titles = [\"height\",\"distance\", \"heading\", \"wrist_angle\", \"wrist_rotation\", \"gripper\"]\n",
    "for i in range(Config()[\"robot\"][\"action_space_size\"]):\n",
    "    ax = axs[i//3, i%3] \n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.plot(t, y[:,i], label=\"y\")\n",
    "    ax.plot(t, ypred[:,i], label=\"yhat\")\n",
    "    ax.legend()\n",
    "    ax.set_title(titles[i])\n",
    "\n",
    "graphfilename = pathlib.Path(exp[\"data_dir\"], \"proprio_error.pdf\")\n",
    "plt.savefig(graphfilename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_MAX = {\"height\": 5.0, \"distance\": 10.0, \"heading\": 90.0, \n",
    "               \"wrist_angle\": 90.0, \"wrist_rotation\": 75.0 + 90.0, \n",
    "               \"gripper\": 100}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, fld in enumerate(POS_MAX):\n",
    "    print(i, fld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Robot-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
