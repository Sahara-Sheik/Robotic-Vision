{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a CNN-based visual encoding finetuned with proprioception\n",
    "\n",
    "We create an encoding for the robot starting from a pretrained CNN model. As the feature vector of this is still large (eg 512 * 7 * 7), we reduce this to the encoding with an MLP. \n",
    "\n",
    "We finetune the encoding with information from proprioception.  \n",
    "\n",
    "The sensor processing object associated with the network trained like this is in sensorprocessing/sp_cnn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from settings import Config\n",
    "\n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "#import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from behavior_cloning.demo_to_trainingdata import BCDemonstration\n",
    "from sensorprocessing.sp_cnn import VGG19Regression\n",
    "from robot.al5d_position_controller import RobotPosition\n",
    "\n",
    "# Move data to GPU (if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pointer config file: C:\\Users\\lboloni\\.config\\BerryPicker\\mainsettings.yaml\n",
      "Loading machine-specific config file: G:\\My Drive\\LotziStudy\\Code\\PackageTracking\\BerryPicker\\settings\\settings-LotziYoga.yaml\n",
      "Missing experiment system dependent config file G:\\My Drive\\LotziStudy\\Code\\PackageTracking\\BerryPicker\\settings\\experiment-config\\LotziYoga\\sp_cnn\\vgg19_orig_sysdep.yaml, that is ok, proceeding.\n",
      "Configuration for experiment: sp_cnn/vgg19_orig successfully loaded\n"
     ]
    }
   ],
   "source": [
    "run = \"vgg19_orig\"\n",
    "exp = Config().get_experiment(\"sp_cnn\", run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the training data\n",
    "The training data (X, Y) is all the pictures from a demonstration with the corresponding proprioception data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_as_proprioception_training(task, proprioception_input_file, proprioception_target_file):\n",
    "    \"\"\"Loads all the images of a task, and processes it as two tensors as input and target data for proprioception training. \n",
    "    Caches the processed results into the input and target file pointed in the config. Remove those files to recalculate\n",
    "    \"\"\"\n",
    "    retval = {}\n",
    "    if proprioception_input_file.exists():\n",
    "        retval[\"inputs\"] = torch.load(proprioception_input_file, weights_only=True)\n",
    "        retval[\"targets\"] = torch.load(proprioception_target_file, weights_only=True)\n",
    "    else:\n",
    "        demos_dir = pathlib.Path(Config()[\"demos\"][\"directory\"])\n",
    "        task_dir = pathlib.Path(demos_dir, \"demos\", task)\n",
    "        \n",
    "        inputlist = []\n",
    "        targetlist = []\n",
    "\n",
    "        for demo_dir in task_dir.iterdir():\n",
    "            if not demo_dir.is_dir():\n",
    "                pass\n",
    "            bcd = BCDemonstration(demo_dir, sensorprocessor=None)\n",
    "            for i in range(bcd.trim_from, bcd.trim_to):\n",
    "                sensor_readings, _ = bcd.get_image(i)\n",
    "                inputlist.append(sensor_readings[0])\n",
    "                a = bcd.get_a(i)\n",
    "                rp = RobotPosition.from_vector(a)\n",
    "                anorm = rp.to_normalized_vector()        \n",
    "                targetlist.append(torch.from_numpy(anorm))\n",
    "\n",
    "        retval[\"inputs\"] = torch.stack(inputlist)\n",
    "        retval[\"targets\"] = torch.stack(targetlist)\n",
    "        torch.save(retval[\"inputs\"], proprioception_input_file)\n",
    "        torch.save(retval[\"targets\"], proprioception_target_file)\n",
    "\n",
    "    # Separate the training and validation data. \n",
    "    # We will be shuffling the demonstrations \n",
    "    length = retval[\"inputs\"].size(0)\n",
    "    rows = torch.randperm(length) \n",
    "    shuffled_inputs = retval[\"inputs\"][rows]\n",
    "    shuffled_targets = retval[\"targets\"][rows]\n",
    "\n",
    "    training_size = int( length * 0.67 )\n",
    "    retval[\"inputs_training\"] = shuffled_inputs[1:training_size]\n",
    "    retval[\"targets_training\"] = shuffled_targets[1:training_size]\n",
    "\n",
    "    retval[\"inputs_validation\"] = shuffled_inputs[training_size:]\n",
    "    retval[\"targets_validation\"] = shuffled_targets[training_size:]\n",
    "\n",
    "    return retval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = exp[\"proprioception_training_task\"]\n",
    "proprioception_input_file = pathlib.Path(exp[\"data_dir\"], \n",
    "                                            exp[\"proprioception_input_file\"])\n",
    "proprioception_target_file = pathlib.Path(exp[\"data_dir\"], \n",
    "                                          exp[\"proprioception_target_file\"])\n",
    "\n",
    "tr = load_images_as_proprioception_training(task, \n",
    "                                            proprioception_input_file, proprioception_target_file)\n",
    "inputs_training = tr[\"inputs_training\"]\n",
    "targets_training = tr[\"targets_training\"]\n",
    "inputs_validation = tr[\"inputs_validation\"]\n",
    "targets_validation = tr[\"targets_validation\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a model that performs this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lboloni\\Documents\\Code\\_VirtualEnvironments\\Robot\\Robot-venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lboloni\\Documents\\Code\\_VirtualEnvironments\\Robot\\Robot-venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "hidden_size = exp[\"latent_dims\"]\n",
    "output_size = Config()[\"robot\"][\"action_space_size\"]\n",
    "model = VGG19Regression(hidden_size, output_size)\n",
    "criterion = nn.MSELoss()\n",
    "# Experiment: would this be better???\n",
    "# criterion = nn.L1Loss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders for batching\n",
    "batch_size = 32\n",
    "train_dataset = TensorDataset(inputs_training, targets_training)\n",
    "test_dataset = TensorDataset(inputs_validation, targets_validation)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_proprioception_model(model, criterion, optimizer, modelfile, device=\"cpu\", epochs=20):\n",
    "    \"\"\"Trains and saves the proprioception model\n",
    "    FIXME: must have parameters etc to investigate alternative models. \n",
    "    \"\"\"\n",
    "\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "    # Training loop\n",
    "    num_epochs = epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            # Forward pass\n",
    "            # print(\"batchX immediately after for loobatch_X.device\" )\n",
    "            predictions = model(batch_X)\n",
    "            #print(batch_y.device)\n",
    "            # I don't understand this  \n",
    "            # print(batch_X.device)\n",
    "            loss = criterion(predictions, batch_y)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        if (epoch + 1) % 1 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss / len(train_loader):.4f}')\n",
    "\n",
    "    # Evaluate the model\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in test_loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            predictions = model(batch_X)\n",
    "            loss = criterion(predictions, batch_y)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    print(f'Test Loss: {test_loss:.4f}')\n",
    "    torch.save(model.state_dict(), modelfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lboloni\\AppData\\Local\\Temp\\ipykernel_95368\\2774848867.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(modelfile))\n"
     ]
    }
   ],
   "source": [
    "# modelfile = pathlib.Path(Config()[\"explorations\"][\"proprioception_mlp_model_file\"])\n",
    "modelfile = pathlib.Path(exp[\"data_dir\"], \n",
    "                         exp[\"proprioception_mlp_model_file\"])\n",
    "epochs = exp[\"epochs\"]\n",
    "if modelfile.exists():\n",
    "    model.load_state_dict(torch.load(modelfile))\n",
    "else:\n",
    "    train_and_save_proprioception_model(model, criterion, optimizer, modelfile, device=device, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an encoding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing experiment system dependent config file G:\\My Drive\\LotziStudy\\Code\\PackageTracking\\BerryPicker\\settings\\experiment-config\\LotziYoga\\sp_cnn\\vgg19_orig_sysdep.yaml, that is ok, proceeding.\n",
      "Configuration for experiment: sp_cnn/vgg19_orig successfully loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lboloni\\AppData\\Local\\Temp\\ipykernel_95368\\1035967932.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  enc.load_state_dict(torch.load(modelfile))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run = \"vgg19_orig\"\n",
    "exp = Config().get_experiment(\"sp_cnn\", run)\n",
    "hidden_size = exp[\"latent_dims\"]\n",
    "output_size = Config()[\"robot\"][\"action_space_size\"]\n",
    "enc = VGG19Regression(hidden_size=hidden_size, output_size=output_size)\n",
    "\n",
    "modelfile = pathlib.Path(exp[\"data_dir\"], \n",
    "                         exp[\"proprioception_mlp_model_file\"])\n",
    "assert modelfile.exists()\n",
    "enc.load_state_dict(torch.load(modelfile))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cameras found: ['dev2']\n",
      "There are 596 steps in this demonstration\n",
      "This demonstration was recorded by the following cameras: ['dev2']\n",
      "Cameras found: ['dev2']\n",
      "There are 388 steps in this demonstration\n",
      "This demonstration was recorded by the following cameras: ['dev2']\n",
      "Cameras found: ['dev2']\n",
      "There are 547 steps in this demonstration\n",
      "This demonstration was recorded by the following cameras: ['dev2']\n",
      "Cameras found: ['dev2']\n",
      "There are 523 steps in this demonstration\n",
      "This demonstration was recorded by the following cameras: ['dev2']\n",
      "tensor([[ 0.2871, -0.7076, -0.6596,  0.6664, -1.6266, -0.7539, -0.2257, -0.1133,\n",
      "         -3.3364, -1.0993, -3.2065, -3.5036, -0.3321, -0.7227, -0.9788,  0.8726,\n",
      "          0.4608, -0.5545, -1.5234, -0.2796, -0.6565,  1.0066, -1.4650, -1.3725,\n",
      "         -0.5085, -2.8840, -2.8110, -2.8296, -0.8329, -2.8855,  0.1827, -0.9695,\n",
      "         -0.8734, -1.6654, -1.3980, -4.0554, -2.1957, -1.3554, -2.4790,  0.6896,\n",
      "         -0.0712, -1.4673,  0.6510, -2.1829, -0.5456, -0.0125, -1.3350, -2.1477,\n",
      "         -0.1646, -1.4787, -2.7583, -1.1788,  0.1239, -1.2494, -1.1390, -0.7997,\n",
      "         -1.4009, -0.7889, -1.1986, -0.4741, -1.3821, -1.4243, -3.8954, -1.0160,\n",
      "          0.1375, -1.8712, -1.7930, -2.3814, -0.4632, -1.0004, -1.4224, -1.1996,\n",
      "          2.0320, -1.4183, -0.4354, -3.1689, -0.5985, -0.5779,  0.1825,  0.1970,\n",
      "         -0.7350, -0.5204, -0.4024, -0.6238, -3.7881, -0.8977,  0.2288, -1.3785,\n",
      "         -1.0176, -0.2226,  0.8018, -0.5362, -0.0630, -1.4211, -2.7674, -2.3312,\n",
      "         -0.1267, -1.6859, -2.6526, -1.0319, -0.0646,  0.5194,  0.7383, -1.3655,\n",
      "         -0.0281, -1.6020, -1.7309, -0.4273, -0.4689, -1.1584, -1.0928, -1.4320,\n",
      "         -1.9634, -1.6380, -3.7641,  0.1571, -2.2810, -1.3146, -0.3266,  0.7463,\n",
      "          0.9734, -1.1538, -1.8068,  0.5024, -0.3632, -1.2896, -1.6889,  0.8855,\n",
      "         -1.0138, -2.1964, -1.2000,  0.4020, -1.3571, -1.7203, -0.6717, -0.2961,\n",
      "         -0.6794, -0.9024,  0.0921, -0.4125, -0.4291, -0.3212, -0.0952, -2.6091,\n",
      "         -2.5840, -0.1219,  0.8193, -2.2693,  0.5473, -2.2659, -1.7135,  0.0591,\n",
      "         -0.3291, -0.8396, -0.3577, -2.0157, -1.3252, -3.2424, -1.7717,  0.6629,\n",
      "         -0.6049, -2.2843, -1.3281, -1.7851,  0.2761,  0.1989, -0.5141, -0.8700,\n",
      "          0.1571,  0.1071, -0.6772, -1.8299,  0.0929, -1.3279,  0.2635,  0.3314,\n",
      "         -2.4780, -1.4054, -2.0872, -0.2080, -0.3417, -2.1092, -3.4802, -0.2338,\n",
      "         -0.8136, -0.1203,  0.2145, -0.4857, -1.4514,  0.9278, -2.0451, -1.6566,\n",
      "         -0.1519, -1.5788, -1.9548, -1.4947, -0.1917, -1.0353, -0.3978,  0.9366,\n",
      "          0.6069, -0.5476, -0.7906, -0.3849, -2.0170,  0.0527, -1.4278, -1.3637,\n",
      "         -0.0978,  0.5024, -0.3599,  0.3623,  0.4736,  0.1589, -0.1980, -1.2385,\n",
      "         -0.1400, -1.9428, -1.9755, -0.8985, -0.2098, -2.2293, -1.6869, -0.2868,\n",
      "          0.0360,  0.6999, -3.3895, -3.1394, -0.2791, -1.7443, -2.7753, -0.9409,\n",
      "         -0.6874, -0.4360, -2.0342, -1.6936, -1.2753,  0.2420, -0.9951, -1.5373,\n",
      "         -1.3095, -0.2943, -0.1538,  1.7674, -1.2057,  0.5791,  0.6478, -1.8918,\n",
      "         -1.1358, -1.5802, -2.0383,  0.1208, -2.7543, -0.9394, -3.0119, -1.0178,\n",
      "         -1.4410, -2.3950, -0.8127, -2.2946, -2.0296, -0.5186, -2.1486, -1.7690,\n",
      "          0.6496,  0.4077, -1.7663, -0.4910, -0.9839, -2.5823, -1.3872, -1.3997,\n",
      "         -1.4718, -2.1126, -1.1485,  1.6757, -0.5599, -2.5921, -1.7892, -1.7572,\n",
      "         -1.9788, -1.1314,  0.0660, -1.4770, -2.6395, -1.8376,  0.0927, -0.6664,\n",
      "          0.4845, -1.8583, -1.1601, -0.5014, -1.2845, -1.9845, -1.6404,  0.6638,\n",
      "         -1.0615, -0.4755, -1.3519,  0.5787, -0.4829, -0.4042, -0.3350, -2.2574,\n",
      "         -0.7824,  0.3963, -0.5657, -2.5942,  0.6393, -2.2930, -1.3117,  0.6270,\n",
      "         -2.5373, -0.7930, -0.6874, -2.8120, -1.9835,  0.3376, -1.3015,  0.0296,\n",
      "         -1.1894, -1.0178, -1.4731, -0.5041, -2.2168, -0.4365, -1.0658, -3.1181,\n",
      "         -1.5075, -1.8080, -1.6587, -0.4334, -1.9531, -2.2614, -1.0017, -1.6748,\n",
      "         -3.4687, -1.4048, -2.0482, -0.6057, -2.0806, -1.0001,  0.1804, -1.8978,\n",
      "         -1.2077,  0.2630, -0.8756, -1.4616,  1.0571,  0.4611, -0.5179, -1.6508,\n",
      "         -0.9038, -0.7107, -0.5867, -2.6719, -1.0233, -2.3231, -0.4807, -0.0571,\n",
      "         -2.1700, -0.2730, -2.4164, -2.1557, -2.4808,  0.7366, -0.2768, -3.6108,\n",
      "         -0.6008, -1.2448, -2.8581,  0.5336, -2.0633, -0.5763, -0.7450, -0.6304,\n",
      "         -2.0918, -0.9556,  0.2724, -0.6867, -1.0591, -1.3318, -1.2467,  0.3196,\n",
      "          1.0348, -0.5999, -2.1425, -2.4821,  0.9441, -1.7796, -1.9164, -0.6954,\n",
      "         -1.8200, -1.8785, -1.8056, -2.1630, -0.6154, -0.9190, -1.2606, -1.5504,\n",
      "         -1.9593, -1.7161,  1.0997, -1.6817, -1.1405, -2.6994, -0.6317,  0.2521,\n",
      "          0.0744, -1.9859, -0.3050, -0.8249, -1.2074,  0.2327, -1.3883, -2.7190,\n",
      "         -0.7123,  0.4453,  1.9494, -0.1940, -1.0587, -0.6765, -1.1286,  0.5695,\n",
      "         -2.1159,  0.8849, -1.1415, -0.2641, -1.1458,  0.3326, -0.2004, -2.3615,\n",
      "         -1.4453, -2.4888, -2.2247,  0.0170, -0.2998, -1.7485, -1.9192, -1.8066,\n",
      "         -1.0521, -1.9367, -1.4181,  1.0325, -1.2102, -1.0419, -2.3507, -0.9343,\n",
      "          0.4462, -0.1040, -0.7888, -1.3853,  1.2706, -1.7702, -1.6920, -3.3151,\n",
      "         -1.1937, -0.2674,  0.5156,  1.5209, -2.7156, -0.6485, -0.4992, -1.1728,\n",
      "         -2.7612, -1.5614, -0.5666, -0.8789,  0.3073,  0.7680, -0.2360, -1.8093,\n",
      "         -1.3042, -0.7518, -0.1866, -0.3095, -1.0465, -1.8571, -0.1102, -0.8310,\n",
      "         -2.1111, -1.6233, -0.8323, -0.3748, -1.9242, -0.5318, -1.6281, -0.9432,\n",
      "         -1.9717, -2.8094, -0.6769, -3.8093, -1.7480, -3.1652, -0.6065, -3.1959,\n",
      "         -2.4941, -2.1720, -2.1267, -0.7294, -0.9606, -1.7421, -1.8914, -3.1015,\n",
      "         -1.0990,  0.1447, -0.2147, -0.1134, -1.1229, -1.9440, -0.3557, -1.4750]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "task = \"random-uncluttered\"\n",
    "demos_dir = pathlib.Path(Config()[\"demos\"][\"directory\"])\n",
    "task_dir = pathlib.Path(demos_dir, \"demos\", task)\n",
    "enc = enc.to(device)\n",
    "\n",
    "\n",
    "inputlist = []\n",
    "targetlist = []\n",
    "with torch.no_grad():\n",
    "    for demo_dir in task_dir.iterdir():\n",
    "        if not demo_dir.is_dir():\n",
    "            pass\n",
    "        bcd = BCDemonstration(demo_dir, sensorprocessor=None)\n",
    "        for i in range(bcd.trim_from, bcd.trim_to):\n",
    "            sensor_readings, _ = bcd.get_image(i)\n",
    "            z = enc.encode(sensor_readings)\n",
    "            break\n",
    "\n",
    "print(z)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Robot-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
