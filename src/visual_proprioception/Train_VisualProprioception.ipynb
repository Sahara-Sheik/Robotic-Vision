{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train models for visual proprioception\n",
    "\n",
    "Train a regression model for visual proprioception. The input is sensory data (eg. a camera image). This is encoded by a p;predefined sensorprocessing component into a latent representation. What we are training and saving here is a regressor that is mapping the latent representation to the position of the robot (eg. a vector of 6 degrees of freedom).\n",
    "\n",
    "The specification of this regressor is specified in an experiment of the type \"visual_proprioception\". Running this notebook will train and save this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pointer config file: /home/ssheikholeslami/.config/BerryPicker/mainsettings.yaml\n",
      "Loading machine-specific config file: /home/ssheikholeslami/SaharaBerryPickerData/settings-sahara.yaml\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from settings import Config\n",
    "\n",
    "import pathlib\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "from visual_proprioception.visproprio_helper import load_demonstrations_as_proprioception_training, get_visual_proprioception_sp\n",
    "from visual_proprioception.visproprio_models import VisProprio_SimpleMLPRegression\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No system dependent experiment file\n",
      " /home/ssheikholeslami/SaharaBerryPickerData/experiments-Config/visual_proprioception/vp_convvae_256_sysdep.yaml,\n",
      " that is ok, proceeding.\n",
      "Configuration for experiment: visual_proprioception/vp_convvae_256 successfully loaded\n",
      "{'data_dir': PosixPath('/home/ssheikholeslami/SaharaBerryPickerData/experiment_data/visual_proprioception/vp_convvae_256'),\n",
      " 'encoding_size': 256,\n",
      " 'epochs': 1000,\n",
      " 'exp_run_sys_indep_file': PosixPath('/lustre/fs1/home/ssheikholeslami/BerryPicker/src/experiment_configs/visual_proprioception/vp_convvae_256.yaml'),\n",
      " 'group_name': 'visual_proprioception',\n",
      " 'loss': 'MSE',\n",
      " 'name': 'conv-vae-256',\n",
      " 'output_size': 6,\n",
      " 'proprioception_input_file': 'train_inputs.pt',\n",
      " 'proprioception_mlp_model_file': 'proprioception_mlp.pth',\n",
      " 'proprioception_target_file': 'train_targets.pt',\n",
      " 'proprioception_test_input_file': 'test_inputs.pt',\n",
      " 'proprioception_test_target_file': 'test_targets.pt',\n",
      " 'proprioception_testing_task': 'proprio_regressor_validation',\n",
      " 'proprioception_training_task': 'proprio_regressor_training',\n",
      " 'regressor_hidden_size_1': 64,\n",
      " 'regressor_hidden_size_2': 64,\n",
      " 'run_name': 'vp_convvae_256',\n",
      " 'sensor_processing': 'ConvVaeSensorProcessing',\n",
      " 'sp_experiment': 'sensorprocessing_conv_vae',\n",
      " 'sp_run': 'proprio_256'}\n",
      "No system dependent experiment file\n",
      " /home/ssheikholeslami/SaharaBerryPickerData/experiments-Config/sensorprocessing_conv_vae/proprio_256_sysdep.yaml,\n",
      " that is ok, proceeding.\n",
      "Configuration for experiment: sensorprocessing_conv_vae/proprio_256 successfully loaded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: logging configuration file is not found in logger/logger_config.json.\n"
     ]
    }
   ],
   "source": [
    "experiment = \"visual_proprioception\"\n",
    "# the latent space 128 ones\n",
    "# run = \"vp_aruco_128\"\n",
    "# run = \"vp_convvae_128\"\n",
    "# run = \"vp_ptun_vgg19_128\"\n",
    "# run = \"vp_ptun_resnet50_128\"\n",
    "\n",
    "# the latent space 256 ones\n",
    "# run = \"vp_convvae_256\"\n",
    "run = \"vp_ptun_vgg19_256\"\n",
    "# run = \"vp_ptun_resnet50_256\"\n",
    "\n",
    "# the VIT\n",
    "# run = \"vit_base\"\n",
    "# run = \"vit_huge\"\n",
    "# run = \"vit_large\"\n",
    "# run = \"vitwb\"\n",
    "\n",
    "exp = Config().get_experiment(experiment, run)\n",
    "pprint(exp)\n",
    "\n",
    "sp = get_visual_proprioception_sp(exp, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'proprioception_mlp_model_file': 'proprioception_mlp.pth', 'proprioception_input_file': 'train_inputs.pt', 'proprioception_target_file': 'train_targets.pt', 'proprioception_test_input_file': 'test_inputs.pt', 'proprioception_test_target_file': 'test_targets.pt', 'encoding_size': 256, 'regressor_hidden_size_1': 64, 'regressor_hidden_size_2': 64, 'output_size': 6, 'epochs': 1000, 'group_name': 'visual_proprioception', 'name': 'conv-vae-256', 'proprioception_training_task': 'proprio_regressor_training', 'proprioception_testing_task': 'proprio_regressor_validation', 'loss': 'MSE', 'sensor_processing': 'ConvVaeSensorProcessing', 'sp_experiment': 'sensorprocessing_conv_vae', 'sp_run': 'proprio_256', 'run_name': 'vp_convvae_256', 'data_dir': PosixPath('/home/ssheikholeslami/SaharaBerryPickerData/experiment_data/visual_proprioception/vp_convvae_256'), 'exp_run_sys_indep_file': PosixPath('/lustre/fs1/home/ssheikholeslami/BerryPicker/src/experiment_configs/visual_proprioception/vp_convvae_256.yaml')}\n",
      "inputttttttttttttttttttttttttt 256\n"
     ]
    }
   ],
   "source": [
    "# Create the regression model\n",
    "\n",
    "model = VisProprio_SimpleMLPRegression(exp)\n",
    "if exp[\"loss\"] == \"MSE\":\n",
    "    criterion = nn.MSELoss()\n",
    "elif exp[\"loss\"] == \"L1\":\n",
    "    criterion = nn.L1Loss()\n",
    "else:\n",
    "    raise Exception(f'Unknown loss type {exp[\"loss\"]}')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and cache the training data. \n",
    "* Iterate through the images and process them into latent encodings. \n",
    "* Iterate through the json files describing the robot position\n",
    "* Save the input and target values into files in the experiment directory. These will act as caches for later runs\n",
    "* Create the training and validation splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cameras found: ['dev2']\n",
      "There are 562 steps in this demonstration\n",
      "This demonstration was recorded by the following cameras: ['dev2']\n",
      "this is bcdddddddd\n",
      "{'actiontype': 'rc-position-target',\n",
      " 'camera': 'dev2',\n",
      " 'cameras': ['dev2'],\n",
      " 'maxsteps': 562,\n",
      " 'sensorprocessor': <sensorprocessing.sp_conv_vae.ConvVaeSensorProcessing object at 0x7f2e4772b9d0>,\n",
      " 'source_dir': PosixPath('/home/ssheikholeslami/SaharaBerryPickerData/demonstrations/demos/proprio_regressor_training/2025_03_08__14_38_55'),\n",
      " 'trim_from': 1,\n",
      " 'trim_to': 562}\n",
      "this is z.shape\n",
      "(561, 256)\n",
      "this a.shape\n",
      "(561, 6)\n",
      "this is z.shape\n",
      "(561, 256)\n",
      "this is anorm.shape\n",
      "(561, 6)\n",
      "Cameras found: ['dev2']\n",
      "There are 487 steps in this demonstration\n",
      "This demonstration was recorded by the following cameras: ['dev2']\n",
      "this is bcdddddddd\n",
      "{'actiontype': 'rc-position-target',\n",
      " 'camera': 'dev2',\n",
      " 'cameras': ['dev2'],\n",
      " 'maxsteps': 487,\n",
      " 'sensorprocessor': <sensorprocessing.sp_conv_vae.ConvVaeSensorProcessing object at 0x7f2e4772b9d0>,\n",
      " 'source_dir': PosixPath('/home/ssheikholeslami/SaharaBerryPickerData/demonstrations/demos/proprio_regressor_training/2025_03_08__14_36_03'),\n",
      " 'trim_from': 1,\n",
      " 'trim_to': 487}\n",
      "this is z.shape\n",
      "(486, 256)\n",
      "this a.shape\n",
      "(486, 6)\n",
      "this is z.shape\n",
      "(486, 256)\n",
      "this is anorm.shape\n",
      "(486, 6)\n",
      "Cameras found: ['dev2']\n",
      "There are 578 steps in this demonstration\n",
      "This demonstration was recorded by the following cameras: ['dev2']\n",
      "this is bcdddddddd\n",
      "{'actiontype': 'rc-position-target',\n",
      " 'camera': 'dev2',\n",
      " 'cameras': ['dev2'],\n",
      " 'maxsteps': 578,\n",
      " 'sensorprocessor': <sensorprocessing.sp_conv_vae.ConvVaeSensorProcessing object at 0x7f2e4772b9d0>,\n",
      " 'source_dir': PosixPath('/home/ssheikholeslami/SaharaBerryPickerData/demonstrations/demos/proprio_regressor_training/2025_03_08__14_46_58'),\n",
      " 'trim_from': 1,\n",
      " 'trim_to': 578}\n",
      "this is z.shape\n",
      "(577, 256)\n",
      "this a.shape\n",
      "(577, 6)\n",
      "this is z.shape\n",
      "(577, 256)\n",
      "this is anorm.shape\n",
      "(577, 6)\n",
      "Cameras found: ['dev2']\n",
      "There are 526 steps in this demonstration\n",
      "This demonstration was recorded by the following cameras: ['dev2']\n",
      "this is bcdddddddd\n",
      "{'actiontype': 'rc-position-target',\n",
      " 'camera': 'dev2',\n",
      " 'cameras': ['dev2'],\n",
      " 'maxsteps': 526,\n",
      " 'sensorprocessor': <sensorprocessing.sp_conv_vae.ConvVaeSensorProcessing object at 0x7f2e4772b9d0>,\n",
      " 'source_dir': PosixPath('/home/ssheikholeslami/SaharaBerryPickerData/demonstrations/demos/proprio_regressor_training/2025_03_08__14_37_26'),\n",
      " 'trim_from': 1,\n",
      " 'trim_to': 526}\n",
      "this is z.shape\n",
      "(525, 256)\n",
      "this a.shape\n",
      "(525, 6)\n",
      "this is z.shape\n",
      "(525, 256)\n",
      "this is anorm.shape\n",
      "(525, 6)\n",
      "torch.Size([2149, 256])\n",
      "torch.Size([2149, 6])\n"
     ]
    }
   ],
   "source": [
    "task = exp[\"proprioception_training_task\"]\n",
    "proprioception_input_file = pathlib.Path(\n",
    "    exp[\"data_dir\"], exp[\"proprioception_input_file\"])\n",
    "proprioception_target_file = pathlib.Path(\n",
    "    exp[\"data_dir\"], exp[\"proprioception_target_file\"])\n",
    "tr = load_demonstrations_as_proprioception_training(\n",
    "    sp, task, proprioception_input_file, proprioception_target_file)\n",
    "inputs_training = tr[\"inputs_training\"]\n",
    "targets_training = tr[\"targets_training\"]\n",
    "inputs_validation = tr[\"inputs_validation\"]\n",
    "targets_validation = tr[\"targets_validation\"]\n",
    "\n",
    "# Create DataLoaders for batching\n",
    "batch_size = 32\n",
    "train_dataset = TensorDataset(inputs_training, targets_training)\n",
    "test_dataset = TensorDataset(inputs_validation, targets_validation)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_proprioception_model(exp):\n",
    "    \"\"\"Trains and saves the proprioception model\n",
    "    \"\"\"\n",
    "    modelfile = pathlib.Path(exp[\"data_dir\"],\n",
    "                         exp[\"proprioception_mlp_model_file\"])\n",
    "    if modelfile.exists():\n",
    "        raise Exception(f'Model already trained {modelfile}.')\n",
    "    num_epochs = exp[\"epochs\"]\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            # Forward pass\n",
    "            predictions = model(batch_X)\n",
    "            loss = criterion(predictions, batch_y)\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss / len(train_loader):.4f}')\n",
    "\n",
    "    # Evaluate the model\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in test_loader:\n",
    "            predictions = model(batch_X)\n",
    "            loss = criterion(predictions, batch_y)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    print(f'Test Loss: {test_loss:.4f}')\n",
    "    torch.save(model.state_dict(), modelfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/1000], Loss: 0.0226\n",
      "Epoch [20/1000], Loss: 0.0132\n",
      "Epoch [30/1000], Loss: 0.0099\n",
      "Epoch [40/1000], Loss: 0.0084\n",
      "Epoch [50/1000], Loss: 0.0070\n",
      "Epoch [60/1000], Loss: 0.0062\n",
      "Epoch [70/1000], Loss: 0.0053\n",
      "Epoch [80/1000], Loss: 0.0047\n",
      "Epoch [90/1000], Loss: 0.0045\n",
      "Epoch [100/1000], Loss: 0.0037\n",
      "Epoch [110/1000], Loss: 0.0036\n",
      "Epoch [120/1000], Loss: 0.0032\n",
      "Epoch [130/1000], Loss: 0.0034\n",
      "Epoch [140/1000], Loss: 0.0027\n",
      "Epoch [150/1000], Loss: 0.0027\n",
      "Epoch [160/1000], Loss: 0.0025\n",
      "Epoch [170/1000], Loss: 0.0024\n",
      "Epoch [180/1000], Loss: 0.0024\n",
      "Epoch [190/1000], Loss: 0.0021\n",
      "Epoch [200/1000], Loss: 0.0020\n",
      "Epoch [210/1000], Loss: 0.0022\n",
      "Epoch [220/1000], Loss: 0.0019\n",
      "Epoch [230/1000], Loss: 0.0017\n",
      "Epoch [240/1000], Loss: 0.0018\n",
      "Epoch [250/1000], Loss: 0.0017\n",
      "Epoch [260/1000], Loss: 0.0017\n",
      "Epoch [270/1000], Loss: 0.0019\n",
      "Epoch [280/1000], Loss: 0.0016\n",
      "Epoch [290/1000], Loss: 0.0015\n",
      "Epoch [300/1000], Loss: 0.0018\n",
      "Epoch [310/1000], Loss: 0.0016\n",
      "Epoch [320/1000], Loss: 0.0018\n",
      "Epoch [330/1000], Loss: 0.0015\n",
      "Epoch [340/1000], Loss: 0.0013\n",
      "Epoch [350/1000], Loss: 0.0014\n",
      "Epoch [360/1000], Loss: 0.0014\n",
      "Epoch [370/1000], Loss: 0.0015\n",
      "Epoch [380/1000], Loss: 0.0013\n",
      "Epoch [390/1000], Loss: 0.0012\n",
      "Epoch [400/1000], Loss: 0.0013\n",
      "Epoch [410/1000], Loss: 0.0012\n",
      "Epoch [420/1000], Loss: 0.0014\n",
      "Epoch [430/1000], Loss: 0.0012\n",
      "Epoch [440/1000], Loss: 0.0012\n",
      "Epoch [450/1000], Loss: 0.0013\n",
      "Epoch [460/1000], Loss: 0.0012\n",
      "Epoch [470/1000], Loss: 0.0011\n",
      "Epoch [480/1000], Loss: 0.0012\n",
      "Epoch [490/1000], Loss: 0.0010\n",
      "Epoch [500/1000], Loss: 0.0012\n",
      "Epoch [510/1000], Loss: 0.0010\n",
      "Epoch [520/1000], Loss: 0.0011\n",
      "Epoch [530/1000], Loss: 0.0010\n",
      "Epoch [540/1000], Loss: 0.0010\n",
      "Epoch [550/1000], Loss: 0.0011\n",
      "Epoch [560/1000], Loss: 0.0011\n",
      "Epoch [570/1000], Loss: 0.0011\n",
      "Epoch [580/1000], Loss: 0.0011\n",
      "Epoch [590/1000], Loss: 0.0010\n",
      "Epoch [600/1000], Loss: 0.0011\n",
      "Epoch [610/1000], Loss: 0.0010\n",
      "Epoch [620/1000], Loss: 0.0011\n",
      "Epoch [630/1000], Loss: 0.0010\n",
      "Epoch [640/1000], Loss: 0.0009\n",
      "Epoch [650/1000], Loss: 0.0011\n",
      "Epoch [660/1000], Loss: 0.0009\n",
      "Epoch [670/1000], Loss: 0.0011\n",
      "Epoch [680/1000], Loss: 0.0011\n",
      "Epoch [690/1000], Loss: 0.0009\n",
      "Epoch [700/1000], Loss: 0.0009\n",
      "Epoch [710/1000], Loss: 0.0010\n",
      "Epoch [720/1000], Loss: 0.0009\n",
      "Epoch [730/1000], Loss: 0.0009\n",
      "Epoch [740/1000], Loss: 0.0008\n",
      "Epoch [750/1000], Loss: 0.0010\n",
      "Epoch [760/1000], Loss: 0.0010\n",
      "Epoch [770/1000], Loss: 0.0008\n",
      "Epoch [780/1000], Loss: 0.0009\n",
      "Epoch [790/1000], Loss: 0.0009\n",
      "Epoch [800/1000], Loss: 0.0008\n",
      "Epoch [810/1000], Loss: 0.0009\n",
      "Epoch [820/1000], Loss: 0.0008\n",
      "Epoch [830/1000], Loss: 0.0008\n",
      "Epoch [840/1000], Loss: 0.0008\n",
      "Epoch [850/1000], Loss: 0.0008\n",
      "Epoch [860/1000], Loss: 0.0009\n",
      "Epoch [870/1000], Loss: 0.0009\n",
      "Epoch [880/1000], Loss: 0.0008\n",
      "Epoch [890/1000], Loss: 0.0007\n",
      "Epoch [900/1000], Loss: 0.0007\n",
      "Epoch [910/1000], Loss: 0.0008\n",
      "Epoch [920/1000], Loss: 0.0008\n",
      "Epoch [930/1000], Loss: 0.0008\n",
      "Epoch [940/1000], Loss: 0.0007\n",
      "Epoch [950/1000], Loss: 0.0008\n",
      "Epoch [960/1000], Loss: 0.0008\n",
      "Epoch [970/1000], Loss: 0.0009\n",
      "Epoch [980/1000], Loss: 0.0007\n",
      "Epoch [990/1000], Loss: 0.0008\n",
      "Epoch [1000/1000], Loss: 0.0008\n",
      "Test Loss: 0.0023\n"
     ]
    }
   ],
   "source": [
    "# modelfile = pathlib.Path(Config()[\"explorations\"][\"proprioception_mlp_model_file\"])\n",
    "\n",
    "#if modelfile.exists():\n",
    "#    model.load_state_dict(torch.load(modelfile))\n",
    "#else:\n",
    "train_and_save_proprioception_model(exp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
