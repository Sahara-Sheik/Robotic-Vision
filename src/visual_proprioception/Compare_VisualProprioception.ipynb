{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare models for visual proprioception\n",
    "\n",
    "Compares regression models for visual proprioception, by running them on specific test data, and creating comparison graphs that put all of them onto the graphs. \n",
    "\n",
    "Each configuration is specified by a run of type visual_proprioception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pointer config file: /home/lboloni/.config/BerryPicker/mainsettings.yaml\n",
      "Loading machine-specific config file: /home/lboloni/Insync/lotzi.boloni@gmail.com/Google Drive/LotziStudy/Code/PackageTracking/BerryPicker/settings/settings-tredy2.yaml\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from settings import Config\n",
    "\n",
    "import pathlib\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# from behavior_cloning.demo_to_trainingdata import BCDemonstration\n",
    "from sensorprocessing import sp_conv_vae, sp_propriotuned_cnn\n",
    "# from robot.al5d_position_controller import RobotPosition\n",
    "\n",
    "from visual_proprioception.visproprio_helper import load_demonstrations_as_proprioception_training\n",
    "from visual_proprioception.visproprio_models import VisProprio_SimpleMLPRegression\n",
    "from visproprio_helper import get_visual_proprioception_sp\n",
    "\n",
    "# Move data to GPU (if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No system dependent experiment file\n",
      " /home/lboloni/Insync/lotzi.boloni@gmail.com/Google Drive/LotziStudy/Code/PackageTracking/BerryPicker/settings/experiment-config/Tredy2/visual_proprioception/vp_mpl_conv_vae_1_sysdep.yaml,\n",
      " that is ok, proceeding.\n",
      "Configuration for experiment: visual_proprioception/vp_mpl_conv_vae_1 successfully loaded\n",
      "No system dependent experiment file\n",
      " /home/lboloni/Insync/lotzi.boloni@gmail.com/Google Drive/LotziStudy/Code/PackageTracking/BerryPicker/settings/experiment-config/Tredy2/visual_proprioception/vp_mpl_conv_vae_1_sysdep.yaml,\n",
      " that is ok, proceeding.\n",
      "Configuration for experiment: visual_proprioception/vp_mpl_conv_vae_1 successfully loaded\n",
      "No system dependent experiment file\n",
      " /home/lboloni/Insync/lotzi.boloni@gmail.com/Google Drive/LotziStudy/Code/PackageTracking/BerryPicker/settings/experiment-config/Tredy2/sensorprocessing_conv_vae/proprio_128_sysdep.yaml,\n",
      " that is ok, proceeding.\n",
      "Configuration for experiment: sensorprocessing_conv_vae/proprio_128 successfully loaded\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'sp_experiment'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 30\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#if exp[\"sensor_processing\"] == \"ConvVaeSensorProcessing\":\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#    spexp = Config().get_experiment(\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#        exp['sp_experiment'], exp['sp_run'])\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m#else:\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m#    raise Exception('Unknown sensor processing {exp[\"sensor_processing\"]}')\u001b[39;00m\n\u001b[1;32m     29\u001b[0m spexp \u001b[38;5;241m=\u001b[39m Config()\u001b[38;5;241m.\u001b[39mget_experiment(exp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msp_experiment\u001b[39m\u001b[38;5;124m'\u001b[39m], exp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msp_run\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 30\u001b[0m sp \u001b[38;5;241m=\u001b[39m \u001b[43mget_visual_proprioception_sp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspexp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m sps\u001b[38;5;241m.\u001b[39mappend(sp)\n\u001b[1;32m     33\u001b[0m model \u001b[38;5;241m=\u001b[39m VisProprio_SimpleMLPRegression(exp)\n",
      "File \u001b[0;32m~/Documents/Hackingwork/_Checkouts/BerryPicker/BerryPicker/src/visual_proprioception/visproprio_helper.py:86\u001b[0m, in \u001b[0;36mget_visual_proprioception_sp\u001b[0;34m(exp, device)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_visual_proprioception_sp\u001b[39m(exp, device):\n\u001b[1;32m     84\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Gets the sensor processing component specified by the \u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;124;03m    visual_proprioception experiment.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m     spexp \u001b[38;5;241m=\u001b[39m Config()\u001b[38;5;241m.\u001b[39mget_experiment(\u001b[43mexp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msp_experiment\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, exp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msp_run\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exp[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msensor_processing\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConvVaeSensorProcessing\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     88\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m sp_conv_vae\u001b[38;5;241m.\u001b[39mConvVaeSensorProcessing(spexp)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'sp_experiment'"
     ]
    }
   ],
   "source": [
    "experiment = \"visual_proprioception\"\n",
    "\n",
    "# the lead run is the one that will provide the test data.\n",
    "# also the directory of this run is where the graphs will be saved.\n",
    "\n",
    "leadrun = \"vp_mpl_conv_vae_1\"\n",
    "# the runs considered in the experiment \n",
    "runs = [\"vp_mpl_conv_vae_1\", \"vp_mpl_propriotuned_cnn_1\"] #, \"vp_mpl_aruco_1\"]\n",
    "\n",
    "# now load them\n",
    "leadexp = Config().get_experiment(experiment, leadrun)\n",
    "\n",
    "exps = []\n",
    "sps = [ ]\n",
    "models = [ ]\n",
    "for run in runs:\n",
    "    exp = Config().get_experiment(experiment, run)\n",
    "    exps.append(exp)\n",
    "\n",
    "    #if exp[\"sensor_processing\"] == \"ConvVaeSensorProcessing\":\n",
    "    #    spexp = Config().get_experiment(\n",
    "    #        exp['sp_experiment'], exp['sp_run'])\n",
    "    #    sp = sp_conv_vae.ConvVaeSensorProcessing(spexp)\n",
    "    #elif exp['sensor_processing']==\"VGG19ProprioTunedSensorProcessing\":\n",
    "    #    spexp = Config().get_experiment(exp['sp_experiment'], exp['sp_run'])\n",
    "    #    sp = sp_propriotuned_cnn.VGG19ProprioTunedSensorProcessing(spexp, device)\n",
    "    #else:\n",
    "    #    raise Exception('Unknown sensor processing {exp[\"sensor_processing\"]}')\n",
    "    spexp = Config().get_experiment(exp['sp_experiment'], exp['sp_run'])\n",
    "    sp = get_visual_proprioception_sp(spexp, device)\n",
    "    sps.append(sp)\n",
    "    model = VisProprio_SimpleMLPRegression(exp)\n",
    "    modelfile = pathlib.Path(exp[\"data_dir\"], \n",
    "                            exp[\"proprioception_mlp_model_file\"])\n",
    "    model.load_state_dict(torch.load(modelfile))\n",
    "    models.append(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = leadexp[\"proprioception_testing_task\"]\n",
    "proprioception_input_file = pathlib.Path(\n",
    "    leadexp[\"data_dir\"], leadexp[\"proprioception_test_input_file\"])\n",
    "proprioception_target_file = pathlib.Path(\n",
    "    leadexp[\"data_dir\"], leadexp[\"proprioception_test_target_file\"])\n",
    "tr = load_demonstrations_as_proprioception_training(\n",
    "    sp, task, proprioception_input_file, proprioception_target_file)\n",
    "\n",
    "targets = tr[\"targets\"]\n",
    "print(f\"There are {targets.shape[0]} data points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate throuh all the components. The thing with it though is that this will get the test data from the components, not from the lead exp.\n",
    "\n",
    "FIXME: maybe I could just write a function that runs a particular proprioceptor on a whole task and returns the y, and then just call that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_from = 0\n",
    "no_to = targets.shape[0]\n",
    "\n",
    "ypreds = []\n",
    "\n",
    "for exp, sp, model in zip(exps, sps, models):\n",
    "    task = exp[\"proprioception_testing_task\"]\n",
    "    proprioception_input_file = pathlib.Path(\n",
    "        exp[\"data_dir\"], exp[\"proprioception_test_input_file\"])\n",
    "    proprioception_target_file = pathlib.Path(\n",
    "        exp[\"data_dir\"], exp[\"proprioception_test_target_file\"])\n",
    "    tr = load_demonstrations_as_proprioception_training(\n",
    "        sp, task, proprioception_input_file, proprioception_target_file)\n",
    "    inputs = tr[\"inputs\"] \n",
    "    ypred = []\n",
    "    y = []\n",
    "    t = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(no_from, no_to):\n",
    "            x = inputs[i]\n",
    "            predictions = model(torch.unsqueeze(x, dim=0))\n",
    "            # append the data \n",
    "            t.append(i)\n",
    "            y.append(targets[i].numpy())\n",
    "            ypred.append(predictions[0].numpy())\n",
    "    ypred = np.array(ypred)\n",
    "    ypreds.append(ypred)            \n",
    "    y = np.array(y)\n",
    "    t = np.array(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we should have the ypreds, the y and the t and we can plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2,3, figsize=(8, 6))\n",
    "titles = [\"height\",\"distance\", \"heading\", \"wrist_angle\", \"wrist_rotation\", \"gripper\"]\n",
    "for i in range(Config()[\"robot\"][\"action_space_size\"]):\n",
    "    ax = axs[i//3, i%3] \n",
    "    ax.set_ylim(0, 1.4)\n",
    "    ax.plot(t, y[:,i], label=\"ground truth\")\n",
    "    for ypred, exp in zip(ypreds,exps):\n",
    "        # fixme, fix the label to the name in the exp\n",
    "        ax.plot(t, ypred[:,i], label=exp[\"name\"])\n",
    "    if i==0:\n",
    "        fig.legend(bbox_to_anchor=(1.25, 1))\n",
    "    ax.set_title(titles[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "graphfilename = pathlib.Path(leadexp[\"data_dir\"], \"comparison.pdf\")\n",
    "plt.savefig(graphfilename, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the average accuracy\n",
    "Create a graph that is comparing the average accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2,3, figsize=(4, 3))\n",
    "\n",
    "for i in range(Config()[\"robot\"][\"action_space_size\"]):\n",
    "    ax = axs[i//3, i%3] \n",
    "    ax.set_ylim(0, 0.4)\n",
    "    # ax.plot(t, y[:,i], label=\"y\")\n",
    "    bars = []\n",
    "    names = []\n",
    "    for ypred, exp in zip(ypreds,exps):\n",
    "        ## FIXME: some kind of different plot\n",
    "        # error = avg(y[:,i], ypred[:,i])\n",
    "        error = math.sqrt(np.mean((y[:,i]- ypred[:,i]) ** 2))\n",
    "        br = ax.bar(exp[\"name\"], error)\n",
    "        bars.append(br)\n",
    "        names.append(exp[\"name\"])\n",
    "    # Remove x-axis labels if desired\n",
    "    ax.set_xticks([])        \n",
    "    if i==0:\n",
    "        fig.legend(bars, names, bbox_to_anchor=(1, 1.1), ncol=2) \n",
    "    fig.tight_layout()\n",
    "    ax.set_title(titles[i])\n",
    "\n",
    "fig.tight_layout()\n",
    "graphfilename = pathlib.Path(leadexp[\"data_dir\"], \"msecomparison.pdf\")\n",
    "plt.savefig(graphfilename, bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
