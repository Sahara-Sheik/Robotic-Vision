{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare models for visual proprioception\n",
    "\n",
    "Compares regression models for visual proprioception, by running them on specific test data, and creating comparison graphs that put all of them onto the graphs. \n",
    "\n",
    "Each configuration is specified by a run of type visual_proprioception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from settings import Config\n",
    "import pathlib\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "\n",
    "from sensorprocessing import sp_conv_vae, sp_propriotuned_cnn, sp_vit\n",
    "from visual_proprioception.visproprio_helper import load_demonstrations_as_proprioception_training\n",
    "from visual_proprioception.visproprio_models import VisProprio_SimpleMLPRegression\n",
    "from visproprio_helper import get_visual_proprioception_sp\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = \"visual_proprioception\"\n",
    "\n",
    "# the lead run is the one that will provide the test data.\n",
    "# also the directory of this run is where the graphs will be saved.\n",
    "\n",
    "# all of them\n",
    "# leadrun = \"comp_all\"\n",
    "leadrun = \"comp_vae\"\n",
    "# leadrun = \"comp_resnet\"\n",
    "# leadrun = \"comp_vgg\"\n",
    "# leadrun = \"comp_all_with_vit\"\n",
    "# leadrun = \"comp_vit\"\n",
    "# leadrun = \"comp_aruco\"\n",
    "\n",
    "# now load them\n",
    "\n",
    "leadexp = Config().get_experiment(experiment, leadrun)\n",
    "\n",
    "runs = leadexp[\"tocompare\"]\n",
    "\n",
    "\n",
    "\n",
    "exps = []\n",
    "sps = [ ]\n",
    "models = [ ]\n",
    "for run in runs:\n",
    "    exp = Config().get_experiment(experiment, run)\n",
    "    exps.append(exp)\n",
    "    sp = get_visual_proprioception_sp(exp, device)\n",
    "    sps.append(sp)\n",
    "    model = VisProprio_SimpleMLPRegression(exp)\n",
    "    modelfile = pathlib.Path(exp[\"data_dir\"],\n",
    "                            exp[\"proprioception_mlp_model_file\"])\n",
    "    model.load_state_dict(torch.load(modelfile))\n",
    "    models.append(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = leadexp[\"proprioception_testing_task\"]\n",
    "proprioception_input_file = pathlib.Path(\n",
    "    leadexp[\"data_dir\"], leadexp[\"proprioception_test_input_file\"])\n",
    "proprioception_target_file = pathlib.Path(\n",
    "    leadexp[\"data_dir\"], leadexp[\"proprioception_test_target_file\"])\n",
    "tr = load_demonstrations_as_proprioception_training(\n",
    "    sp, task, proprioception_input_file, proprioception_target_file)\n",
    "\n",
    "targets = tr[\"targets\"]\n",
    "print(f\"There are {targets.shape[0]} data points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate throuh all the components. The thing with it though is that this will get the test data from the components, not from the lead exp.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_from = 0\n",
    "no_to = targets.shape[0]\n",
    "\n",
    "ypreds = []\n",
    "\n",
    "for exp, sp, model in zip(exps, sps, models):\n",
    "    task = exp[\"proprioception_testing_task\"]\n",
    "    proprioception_input_file = pathlib.Path(\n",
    "        exp[\"data_dir\"], exp[\"proprioception_test_input_file\"])\n",
    "    proprioception_target_file = pathlib.Path(\n",
    "        exp[\"data_dir\"], exp[\"proprioception_test_target_file\"])\n",
    "    tr = load_demonstrations_as_proprioception_training(\n",
    "        sp, task, proprioception_input_file, proprioception_target_file)\n",
    "    inputs = tr[\"inputs\"]\n",
    "    ypred = []\n",
    "    y = []\n",
    "    t = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(no_from, no_to):\n",
    "            x = inputs[i]\n",
    "            predictions = model(torch.unsqueeze(x, dim=0))\n",
    "            # append the data\n",
    "            t.append(i)\n",
    "            y.append(targets[i].numpy())\n",
    "            ypred.append(predictions[0].numpy())\n",
    "    ypred = np.array(ypred)\n",
    "    ypreds.append(ypred)\n",
    "    y = np.array(y)\n",
    "    t = np.array(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time compare plot - double column\n",
    "At this point, we should have the ypreds, the y and the t and we can plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2,3, figsize=(8, 6))\n",
    "titles = [\"height\",\"distance\", \"heading\", \"wrist_angle\", \"wrist_rotation\", \"gripper\"]\n",
    "for i in range(Config()[\"robot\"][\"action_space_size\"]):\n",
    "    ax = axs[i//3, i%3]\n",
    "    ax.set_ylim(0, 1.4)\n",
    "    ax.plot(t, y[:,i], label=\"ground truth\")\n",
    "    for ypred, exp in zip(ypreds,exps):\n",
    "        ax.plot(t, ypred[:,i], label=exp[\"name\"])\n",
    "    if i==0:\n",
    "        fig.legend(bbox_to_anchor=(1.25, 1))\n",
    "    ax.set_title(titles[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "graphfilename = pathlib.Path(leadexp[\"data_dir\"], \"comparison.pdf\")\n",
    "plt.savefig(graphfilename, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time compare plot, single column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2,3, figsize=(7, 5.3))\n",
    "titles = [\"height\",\"distance\", \"heading\", \"wrist_angle\", \"wrist_rotation\", \"gripper\"]\n",
    "for i in range(Config()[\"robot\"][\"action_space_size\"]):\n",
    "    ax = axs[i//3, i%3]\n",
    "    ax.set_ylim(0, 2.0)\n",
    "    for ypred, exp in zip(ypreds,exps):\n",
    "        ax.plot(t, ypred[:,i], label=exp[\"name\"], linewidth=1)\n",
    "    ax.plot(t, y[:,i], label=\"ground truth\", linewidth=2, color=\"black\")\n",
    "    if i==2:\n",
    "        ax.legend()\n",
    "    ax.set_title(titles[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "graphfilename = pathlib.Path(leadexp[\"data_dir\"], \"comparison.pdf\")\n",
    "plt.savefig(graphfilename, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time compare plot, vertical with legend bottom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig, axs = plt.subplots(3,2, figsize=(5.3, 7))\n",
    "fig, axs = plt.subplots(3,2, figsize=(4.6, 6))\n",
    "titles = [\"height\",\"distance\", \"heading\", \"wrist_angle\", \"wrist_rotation\", \"gripper\"]\n",
    "for i in range(Config()[\"robot\"][\"action_space_size\"]):\n",
    "    ax = axs[i//2, i%2]\n",
    "    ax.set_ylim(0, 2.0)\n",
    "    for ypred, exp in zip(ypreds,exps):\n",
    "        #  the label to the name in the exp\n",
    "        ax.plot(t, ypred[:,i], label=exp[\"name\"], linewidth=1)\n",
    "    ax.plot(t, y[:,i], label=\"ground truth\", linewidth=2, color=\"black\")\n",
    "    if i==4:\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "        fig.legend(handles, labels, ncol=len(exps)+1,\n",
    "            bbox_to_anchor=(0.5, 0), loc=\"upper center\")\n",
    "    ax.set_title(titles[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "graphfilename = pathlib.Path(leadexp[\"data_dir\"], \"comparison23.pdf\")\n",
    "plt.savefig(graphfilename, bbox_inches='tight')\n",
    "graphfilename = pathlib.Path(leadexp[\"data_dir\"], \"comparison23.jpg\")\n",
    "plt.savefig(graphfilename, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the average accuracy\n",
    "Create a graph that is comparing the average accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2,3, figsize=(4, 3))\n",
    "\n",
    "for i in range(Config()[\"robot\"][\"action_space_size\"]):\n",
    "    ax = axs[i//3, i%3]\n",
    "    # ax.set_ylim(0, 0.4)\n",
    "    ax.set_ylim(0, 0.5)\n",
    "    # ax.plot(t, y[:,i], label=\"y\")\n",
    "    bars = []\n",
    "    names = []\n",
    "    for ypred, exp in zip(ypreds,exps):\n",
    "        error = math.sqrt(np.mean((y[:,i]- ypred[:,i]) ** 2))\n",
    "        br = ax.bar(exp[\"name\"], error)\n",
    "        bars.append(br)\n",
    "        names.append(exp[\"name\"])\n",
    "    # Remove x-axis labels if desired\n",
    "    ax.set_xticks([])\n",
    "    if i==0:\n",
    "        fig.legend(bars, names, bbox_to_anchor=(1.50, 0.9), ncol=1)\n",
    "    fig.tight_layout()\n",
    "    ax.set_title(titles[i])\n",
    "\n",
    "fig.tight_layout()\n",
    "graphfilename = pathlib.Path(leadexp[\"data_dir\"], \"msecomparison.pdf\")\n",
    "plt.savefig(graphfilename, bbox_inches='tight')\n",
    "graphfilename = pathlib.Path(leadexp[\"data_dir\"], \"msecomparison.jpg\")\n",
    "plt.savefig(graphfilename, bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Robot-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
