{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify models for visual proprioception\n",
    "\n",
    "Verifies a regression model for visual proprioception, as trained in the notebook Train_VisualProprioception\n",
    "\n",
    "The encoding and the regressor is specified in an experiment of type visual_proprioception. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from settings import Config\n",
    "\n",
    "import pathlib\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "from sensorprocessing import sp_conv_vae, sp_propriotuned_cnn, sp_vit\n",
    "from visual_proprioception.visproprio_helper import load_demonstrations_as_proprioception_training, get_visual_proprioception_sp\n",
    "from visual_proprioception.visproprio_models import VisProprio_SimpleMLPRegression\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = \"visual_proprioception\"\n",
    "\n",
    "# the latent space 128 ones\n",
    "run = \"vp_aruco_128\"\n",
    "# run = \"vp_convvae_128\"\n",
    "# run = \"vp_ptun_vgg19_128\"\n",
    "# run = \"vp_ptun_resnet50_128\"\n",
    "# run = \"vit_base\"\n",
    "# run = \"vit_large\"\n",
    "\n",
    "# the latent space 256 ones\n",
    "# run = \"vp_convvae_256\"\n",
    "# run = \"vp_ptun_vgg19_256\"\n",
    "# run = \"vp_ptun_resnet50_256\"\n",
    "exp = Config().get_experiment(experiment, run)\n",
    "\n",
    "pprint(exp)\n",
    "sp = get_visual_proprioception_sp(exp, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the regression model and load in the previously saved weights\n",
    "\n",
    "model = VisProprio_SimpleMLPRegression(exp)\n",
    "modelfile = pathlib.Path(exp[\"data_dir\"], \n",
    "                         exp[\"proprioception_mlp_model_file\"])\n",
    "model.load_state_dict(torch.load(modelfile))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy in time multigraph\n",
    "Visualize the accuracy of the proprioception on the testing task, by plotting the ground truth. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = exp[\"proprioception_testing_task\"]\n",
    "proprioception_input_file = pathlib.Path(\n",
    "    exp[\"data_dir\"], exp[\"proprioception_test_input_file\"])\n",
    "proprioception_target_file = pathlib.Path(\n",
    "    exp[\"data_dir\"], exp[\"proprioception_test_target_file\"])\n",
    "tr = load_demonstrations_as_proprioception_training(\n",
    "    sp, task, proprioception_input_file, proprioception_target_file)\n",
    "\n",
    "inputs = tr[\"inputs\"] # these are actually tensors\n",
    "targets = tr[\"targets\"]\n",
    "print(f\"There are {inputs.shape[0]} data points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_from = 0\n",
    "no_to = inputs.shape[0]\n",
    "ypred = []\n",
    "y = []\n",
    "t = []\n",
    "with torch.no_grad():\n",
    "    for i in range(no_from, no_to):\n",
    "        x = inputs[i]\n",
    "        predictions = model(torch.unsqueeze(x, dim=0))\n",
    "        # append the data \n",
    "        t.append(i)\n",
    "        y.append(targets[i].numpy())\n",
    "        ypred.append(predictions[0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = np.array(ypred)\n",
    "y = np.array(y)\n",
    "t = np.array(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a graph with the six degrees of freedom, predicted and real value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2,3, constrained_layout=True)\n",
    "titles = [\"height\",\"distance\", \"heading\", \"wrist_angle\", \"wrist_rotation\", \"gripper\"]\n",
    "for i in range(Config()[\"robot\"][\"action_space_size\"]):\n",
    "    ax = axs[i//3, i%3] \n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.plot(t, y[:,i], label=\"y\")\n",
    "    ax.plot(t, ypred[:,i], label=\"yhat\")\n",
    "    ax.legend()\n",
    "    ax.set_title(titles[i])\n",
    "\n",
    "graphfilename = pathlib.Path(exp[\"data_dir\"], \"proprio_error.pdf\")\n",
    "plt.savefig(graphfilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_MAX = {\"height\": 5.0, \"distance\": 10.0, \"heading\": 90.0, \n",
    "               \"wrist_angle\": 90.0, \"wrist_rotation\": 75.0 + 90.0, \n",
    "               \"gripper\": 100}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, fld in enumerate(POS_MAX):\n",
    "    print(i, fld)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Robot-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
