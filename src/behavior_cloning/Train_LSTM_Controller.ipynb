{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train an LSTM based controller \n",
    "\n",
    "Train and save an LSTM-based controller. It contains:\n",
    "* Code for loading and pre-processing the training data. \n",
    "* Training an LSTM with specific parameters and saving it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pointer config file: /home/lboloni/.config/BerryPicker/mainsettings.yaml\n",
      "Loading machine-specific config file: /home/lboloni/Insync/lotzi.boloni@gmail.com/Google Drive/LotziStudy/Code/PackageTracking/BerryPicker/settings/settings-tredy2.yaml\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from settings import Config\n",
    "\n",
    "import pathlib\n",
    "#from pprint import pformat\n",
    "\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "from sensorprocessing import sp_conv_vae\n",
    "from demo_to_trainingdata import create_RNN_training_sequence_xy, BCDemonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating training data\n",
    "\n",
    "In it's current form it creates sequential data from a single demonstration. \n",
    "\n",
    "TODO: this needs to create the training data from a mixture of various demonstrations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resume_model and jsonfile are:\n",
      "\tresume_model=/home/lboloni/Documents/Hackingwork/__Temporary/BerryPicker-models/Conv-VAE/models/VAE_Robot/0901_125042/checkpoint-epoch171.pth\n",
      "\tjsonfile=/home/lboloni/Documents/Hackingwork/__Temporary/BerryPicker-models/Conv-VAE/models/VAE_Robot/0901_125042/config.json\n",
      "Warning: logging configuration file is not found in logger/logger_config.json.\n",
      "{\n",
      "    \"name\": \"VAE_Robot\",\n",
      "    \"n_gpu\": 1,\n",
      "    \"arch\": {\n",
      "        \"type\": \"VanillaVAE\",\n",
      "        \"args\": {\n",
      "            \"in_channels\": 3,\n",
      "            \"latent_dims\": 128,\n",
      "            \"flow\": false\n",
      "        }\n",
      "    },\n",
      "    \"data_loader\": {\n",
      "        \"###type-prev\": \"RobotDataLoader\",\n",
      "        \"type\": \"CelebDataLoader\",\n",
      "        \"args\": {\n",
      "            \"data_dir\": \"/home/lboloni/Documents/Hackingwork/__Temporary/VisionBasedRobotManipulator-training-data/vae-training-data\",\n",
      "            \"batch_size\": 64,\n",
      "            \"shuffle\": true,\n",
      "            \"validation_split\": 0.2,\n",
      "            \"num_workers\": 2\n",
      "        }\n",
      "    },\n",
      "    \"optimizer\": {\n",
      "        \"type\": \"Adam\",\n",
      "        \"args\": {\n",
      "            \"lr\": 0.005,\n",
      "            \"weight_decay\": 0.0,\n",
      "            \"amsgrad\": true\n",
      "        }\n",
      "    },\n",
      "    \"loss\": \"elbo_loss\",\n",
      "    \"metrics\": [],\n",
      "    \"lr_scheduler\": {\n",
      "        \"type\": \"StepLR\",\n",
      "        \"args\": {\n",
      "            \"step_size\": 50,\n",
      "            \"gamma\": 0.1\n",
      "        }\n",
      "    },\n",
      "    \"trainer\": {\n",
      "        \"epochs\": 200,\n",
      "        \"save_dir\": \"/home/lboloni/Documents/Hackingwork/__Temporary/VisionBasedRobotManipulator-models/Conv-VAE/\",\n",
      "        \"save_period\": 1,\n",
      "        \"verbosity\": 2,\n",
      "        \"monitor\": \"min val_loss\",\n",
      "        \"early_stop\": 10,\n",
      "        \"tensorboard\": true\n",
      "    }\n",
      "}\n",
      "VanillaVAE(\n",
      "  (encoder): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "  )\n",
      "  (fc_mu): Linear(in_features=2048, out_features=128, bias=True)\n",
      "  (fc_var): Linear(in_features=2048, out_features=128, bias=True)\n",
      "  (decoder_input): Linear(in_features=128, out_features=2048, bias=True)\n",
      "  (decoder): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "  )\n",
      "  (final_layer): Sequential(\n",
      "    (0): ConvTranspose2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.01)\n",
      "    (3): Conv2d(32, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): Tanh()\n",
      "  )\n",
      ")\n",
      "Trainable parameters: 3937635\n",
      "Loading the checkpoint\n",
      "Checkpoint loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lboloni/Documents/Hackingwork/_Checkouts/BerryPicker/BerryPicker/src/behavior_cloning/../sensorprocessing/sp_conv_vae.py:72: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.checkpoint = torch.load(self.config.resume, map_location=torch.device('cpu'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cameras found: ['dev2']\n",
      "There are 753 steps in this demonstration\n",
      "This demonstration was recorded by the following cameras: ['dev2']\n",
      "{'actiontype': 'rc-position-target',\n",
      " 'camera': 'dev2',\n",
      " 'cameras': ['dev2'],\n",
      " 'maxsteps': 753,\n",
      " 'sensorprocessor': <sensorprocessing.sp_conv_vae.ConvVaeSensorProcessing object at 0x78aa2c2aa830>,\n",
      " 'source_dir': PosixPath('/home/lboloni/Documents/Hackingwork/__Temporary/BerryPicker-demos/demos/proprioception-uncluttered/2024_10_26__16_31_40'),\n",
      " 'trim_from': 1,\n",
      " 'trim_to': 753}\n",
      "(752, 128)\n",
      "(752, 6)\n",
      "Cameras found: ['dev2']\n",
      "There are 968 steps in this demonstration\n",
      "This demonstration was recorded by the following cameras: ['dev2']\n",
      "{'actiontype': 'rc-position-target',\n",
      " 'camera': 'dev2',\n",
      " 'cameras': ['dev2'],\n",
      " 'maxsteps': 968,\n",
      " 'sensorprocessor': <sensorprocessing.sp_conv_vae.ConvVaeSensorProcessing object at 0x78aa2c2aa830>,\n",
      " 'source_dir': PosixPath('/home/lboloni/Documents/Hackingwork/__Temporary/BerryPicker-demos/demos/proprioception-uncluttered/2024_10_26__16_23_22'),\n",
      " 'trim_from': 1,\n",
      " 'trim_to': 968}\n",
      "(967, 128)\n",
      "(967, 6)\n",
      "Cameras found: ['dev2']\n",
      "There are 744 steps in this demonstration\n",
      "This demonstration was recorded by the following cameras: ['dev2']\n",
      "{'actiontype': 'rc-position-target',\n",
      " 'camera': 'dev2',\n",
      " 'cameras': ['dev2'],\n",
      " 'maxsteps': 744,\n",
      " 'sensorprocessor': <sensorprocessing.sp_conv_vae.ConvVaeSensorProcessing object at 0x78aa2c2aa830>,\n",
      " 'source_dir': PosixPath('/home/lboloni/Documents/Hackingwork/__Temporary/BerryPicker-demos/demos/proprioception-uncluttered/2024_10_26__16_18_47'),\n",
      " 'trim_from': 1,\n",
      " 'trim_to': 744}\n",
      "(743, 128)\n",
      "(743, 6)\n",
      "Cameras found: ['dev2']\n",
      "There are 469 steps in this demonstration\n",
      "This demonstration was recorded by the following cameras: ['dev2']\n",
      "{'actiontype': 'rc-position-target',\n",
      " 'camera': 'dev2',\n",
      " 'cameras': ['dev2'],\n",
      " 'maxsteps': 469,\n",
      " 'sensorprocessor': <sensorprocessing.sp_conv_vae.ConvVaeSensorProcessing object at 0x78aa2c2aa830>,\n",
      " 'source_dir': PosixPath('/home/lboloni/Documents/Hackingwork/__Temporary/BerryPicker-demos/demos/proprioception-uncluttered/2024_10_26__16_34_31'),\n",
      " 'trim_from': 1,\n",
      " 'trim_to': 469}\n",
      "(468, 128)\n",
      "(468, 6)\n",
      "Cameras found: ['dev2']\n",
      "There are 787 steps in this demonstration\n",
      "This demonstration was recorded by the following cameras: ['dev2']\n",
      "{'actiontype': 'rc-position-target',\n",
      " 'camera': 'dev2',\n",
      " 'cameras': ['dev2'],\n",
      " 'maxsteps': 787,\n",
      " 'sensorprocessor': <sensorprocessing.sp_conv_vae.ConvVaeSensorProcessing object at 0x78aa2c2aa830>,\n",
      " 'source_dir': PosixPath('/home/lboloni/Documents/Hackingwork/__Temporary/BerryPicker-demos/demos/proprioception-uncluttered/2024_10_26__16_36_02'),\n",
      " 'trim_from': 1,\n",
      " 'trim_to': 787}\n",
      "(786, 128)\n",
      "(786, 6)\n",
      "Cameras found: ['dev2']\n",
      "There are 508 steps in this demonstration\n",
      "This demonstration was recorded by the following cameras: ['dev2']\n",
      "{'actiontype': 'rc-position-target',\n",
      " 'camera': 'dev2',\n",
      " 'cameras': ['dev2'],\n",
      " 'maxsteps': 508,\n",
      " 'sensorprocessor': <sensorprocessing.sp_conv_vae.ConvVaeSensorProcessing object at 0x78aa2c2aa830>,\n",
      " 'source_dir': PosixPath('/home/lboloni/Documents/Hackingwork/__Temporary/BerryPicker-demos/demos/proprioception-uncluttered/2024_10_26__16_33_14'),\n",
      " 'trim_from': 1,\n",
      " 'trim_to': 508}\n",
      "(507, 128)\n",
      "(507, 6)\n"
     ]
    }
   ],
   "source": [
    "task = \"proprioception-uncluttered\"\n",
    "sp = sp_conv_vae.ConvVaeSensorProcessing()\n",
    "\n",
    "demos_dir = pathlib.Path(Config()[\"demos\"][\"directory\"])\n",
    "task_dir = pathlib.Path(demos_dir, \"demos\", task)\n",
    "\n",
    "inputlist = []\n",
    "targetlist = []\n",
    "\n",
    "for demo_dir in task_dir.iterdir():\n",
    "    if not demo_dir.is_dir():\n",
    "        pass\n",
    "    bcd = BCDemonstration(demo_dir, sensorprocessor=sp)\n",
    "    print(bcd)\n",
    "    z, a = bcd.read_z_a()\n",
    "    print(z.shape)\n",
    "    print(a.shape)\n",
    "    inputs, targets = create_RNN_training_sequence_xy(z, a, sequence_length=10)\n",
    "    inputlist.append(inputs)\n",
    "    targetlist.append(targets)\n",
    "\n",
    "inputs = torch.cat(inputlist)\n",
    "targets = torch.cat(targetlist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM w/ 3 layers and skip connections\n",
    "\n",
    "This is an attempt to recreate the LSTM model from the Rouhollah 2020 paper. It does not have an MDM at the end. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# FIXME: this is sequence prediction\n",
    "\n",
    "class LSTMResidualController(nn.Module):\n",
    "    def __init__(self, latent_size, hidden_size):\n",
    "        super(LSTMResidualController, self).__init__()\n",
    "        self.lstm_1 = nn.LSTM(latent_size, hidden_size, num_layers=1, batch_first=True)\n",
    "\n",
    "        self.lstm_2 = nn.LSTM(hidden_size, hidden_size, num_layers=1, batch_first=True)\n",
    "\n",
    "        self.lstm_3 = nn.LSTM(hidden_size, hidden_size, num_layers=1, batch_first=True)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size, latent_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, sequence_length, latent_size]\n",
    "        out_1, _ = self.lstm_1(x)\n",
    "        residual = out_1\n",
    "        out_2, _ = self.lstm_2(out_1)\n",
    "        out_2 = out_2 + residual\n",
    "        residual = out_2\n",
    "        out_3, _ = self.lstm_3(out_2)\n",
    "        out_3 = out_3 + residual\n",
    "\n",
    "        # LSTM output shape: [batch_size, sequence_length, hidden_size]\n",
    "        out = self.fc(out_3[:, -1, :])  # Take last time step output and pass through the fully connected layer\n",
    "        return out  # Predicted next vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the architecture created by chatgpt\n",
    "class LSTMXYPredictor(nn.Module):\n",
    "    def __init__(self, latent_size, hidden_size, output_size, num_layers):\n",
    "        super(LSTMXYPredictor, self).__init__()\n",
    "        self.lstm = nn.LSTM(latent_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, sequence_length, latent_size]\n",
    "        out, _ = self.lstm(x)  # LSTM output shape: [batch_size, sequence_length, hidden_size]\n",
    "        out = self.fc(out[:, -1, :])  # Take last time step output and pass through the fully connected layer\n",
    "        return out  # Predicted next vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original\n",
    "latent_size = Config()[\"robot\"][\"latent_encoding_size\"]  \n",
    "hidden_size = 32  # degrees of freedom in the robot\n",
    "output_size = 6  # degrees of freedom in the robot\n",
    "num_layers = 2\n",
    "\n",
    "# Simple model: Instantiate model, loss function, and optimizer\n",
    "#model = LSTMResidualController(latent_size=latent_size, hidden_size=hidden_size)\n",
    "#criterion = nn.MSELoss()  # Mean Squared Error for regression\n",
    "#optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# Instantiate model, loss function, and optimizer\n",
    "model = LSTMXYPredictor(latent_size=latent_size, hidden_size=hidden_size, output_size = output_size, num_layers=num_layers)\n",
    "criterion = nn.MSELoss()  # Mean Squared Error for regression\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.07089406, -0.32879138,  0.3921941 , ...,  0.0507906 ,\n",
       "        -0.11549067, -0.4683873 ],\n",
       "       [ 0.07325773, -0.32755402,  0.39171767, ...,  0.05148034,\n",
       "        -0.11586552, -0.46543336],\n",
       "       [ 0.06882108, -0.32531896,  0.38906685, ...,  0.05078246,\n",
       "        -0.11425927, -0.45738766],\n",
       "       ...,\n",
       "       [ 0.03610518, -0.33893436,  0.3692953 , ...,  0.0880032 ,\n",
       "        -0.13571778, -0.497684  ],\n",
       "       [ 0.03718238, -0.3391816 ,  0.3708424 , ...,  0.08734815,\n",
       "        -0.13582218, -0.50285035],\n",
       "       [ 0.03575388, -0.33898264,  0.36980066, ...,  0.08724476,\n",
       "        -0.13428041, -0.49851295]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/100], Loss: 90.2824\n",
      "Epoch [4/100], Loss: 56.6518\n",
      "Epoch [6/100], Loss: 38.6657\n",
      "Epoch [8/100], Loss: 51.7366\n",
      "Epoch [10/100], Loss: 25.9904\n",
      "Epoch [12/100], Loss: 40.4824\n",
      "Epoch [14/100], Loss: 42.6599\n",
      "Epoch [16/100], Loss: 8.3470\n",
      "Epoch [18/100], Loss: 18.6223\n",
      "Epoch [20/100], Loss: 52.5237\n",
      "Epoch [22/100], Loss: 14.5628\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "num_epochs = 100\n",
    "\n",
    "z, a = bcd.read_z_a()\n",
    "num_sequences = inputs.shape[0]\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    # Loop over each sequence in the batch\n",
    "    for i in range(num_sequences):\n",
    "        # Prepare input and target\n",
    "        input_seq = inputs[i]\n",
    "        target = targets[i]\n",
    "\n",
    "        # Reshape for batch compatibility\n",
    "        input_seq = input_seq.unsqueeze(0)  # Shape: [1, sequence_length, latent_size]\n",
    "        target = target.unsqueeze(0)        # Shape: [1, latent_size]\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(input_seq)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 2 == 0: # was 0\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# FIXME: save the model\n",
    "filename_lstm = Config()[\"controller\"][\"lstm_model_file\"]\n",
    "torch.save(model.state_dict(), filename_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
