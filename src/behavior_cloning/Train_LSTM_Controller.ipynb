{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train an LSTM based controller \n",
    "\n",
    "Train and save an LSTM-based controller. It contains:\n",
    "* Code for loading and pre-processing the training data. \n",
    "* Training an LSTM with specific parameters and saving it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from settings import Config\n",
    "\n",
    "import pathlib\n",
    "#from pprint import pformat\n",
    "from tqdm import tqdm\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "from sensorprocessing import sp_conv_vae\n",
    "from demo_to_trainingdata import create_RNN_training_sequence_xy, BCDemonstration\n",
    "from bc_LSTM import LSTMXYPredictor, LSTMResidualController\n",
    "\n",
    "from tensorboardX import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating training and validation data\n",
    "Create training and validation data from all the demonstrations of a certain task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: logging configuration file is not found in logger/logger_config.json.\n",
      "Cameras found: ['dev2']\n",
      "There are 753 steps in this demonstration\n",
      "This demonstration was recorded by the following cameras: ['dev2']\n",
      "{'actiontype': 'rc-position-target',\n",
      " 'camera': 'dev2',\n",
      " 'cameras': ['dev2'],\n",
      " 'maxsteps': 753,\n",
      " 'sensorprocessor': <sensorprocessing.sp_conv_vae.ConvVaeSensorProcessing object at 0x71006e3d3370>,\n",
      " 'source_dir': PosixPath('/home/lboloni/Documents/Hackingwork/__Temporary/BerryPicker-demos/demos/proprioception-uncluttered/2024_10_26__16_31_40'),\n",
      " 'trim_from': 1,\n",
      " 'trim_to': 753}\n",
      "(752, 128)\n",
      "(752, 6)\n",
      "Cameras found: ['dev2']\n",
      "There are 968 steps in this demonstration\n",
      "This demonstration was recorded by the following cameras: ['dev2']\n",
      "{'actiontype': 'rc-position-target',\n",
      " 'camera': 'dev2',\n",
      " 'cameras': ['dev2'],\n",
      " 'maxsteps': 968,\n",
      " 'sensorprocessor': <sensorprocessing.sp_conv_vae.ConvVaeSensorProcessing object at 0x71006e3d3370>,\n",
      " 'source_dir': PosixPath('/home/lboloni/Documents/Hackingwork/__Temporary/BerryPicker-demos/demos/proprioception-uncluttered/2024_10_26__16_23_22'),\n",
      " 'trim_from': 1,\n",
      " 'trim_to': 968}\n",
      "(967, 128)\n",
      "(967, 6)\n",
      "Cameras found: ['dev2']\n",
      "There are 744 steps in this demonstration\n",
      "This demonstration was recorded by the following cameras: ['dev2']\n",
      "{'actiontype': 'rc-position-target',\n",
      " 'camera': 'dev2',\n",
      " 'cameras': ['dev2'],\n",
      " 'maxsteps': 744,\n",
      " 'sensorprocessor': <sensorprocessing.sp_conv_vae.ConvVaeSensorProcessing object at 0x71006e3d3370>,\n",
      " 'source_dir': PosixPath('/home/lboloni/Documents/Hackingwork/__Temporary/BerryPicker-demos/demos/proprioception-uncluttered/2024_10_26__16_18_47'),\n",
      " 'trim_from': 1,\n",
      " 'trim_to': 744}\n",
      "(743, 128)\n",
      "(743, 6)\n",
      "Cameras found: ['dev2']\n",
      "There are 469 steps in this demonstration\n",
      "This demonstration was recorded by the following cameras: ['dev2']\n",
      "{'actiontype': 'rc-position-target',\n",
      " 'camera': 'dev2',\n",
      " 'cameras': ['dev2'],\n",
      " 'maxsteps': 469,\n",
      " 'sensorprocessor': <sensorprocessing.sp_conv_vae.ConvVaeSensorProcessing object at 0x71006e3d3370>,\n",
      " 'source_dir': PosixPath('/home/lboloni/Documents/Hackingwork/__Temporary/BerryPicker-demos/demos/proprioception-uncluttered/2024_10_26__16_34_31'),\n",
      " 'trim_from': 1,\n",
      " 'trim_to': 469}\n",
      "(468, 128)\n",
      "(468, 6)\n",
      "Cameras found: ['dev2']\n",
      "There are 787 steps in this demonstration\n",
      "This demonstration was recorded by the following cameras: ['dev2']\n",
      "{'actiontype': 'rc-position-target',\n",
      " 'camera': 'dev2',\n",
      " 'cameras': ['dev2'],\n",
      " 'maxsteps': 787,\n",
      " 'sensorprocessor': <sensorprocessing.sp_conv_vae.ConvVaeSensorProcessing object at 0x71006e3d3370>,\n",
      " 'source_dir': PosixPath('/home/lboloni/Documents/Hackingwork/__Temporary/BerryPicker-demos/demos/proprioception-uncluttered/2024_10_26__16_36_02'),\n",
      " 'trim_from': 1,\n",
      " 'trim_to': 787}\n",
      "(786, 128)\n",
      "(786, 6)\n",
      "Cameras found: ['dev2']\n",
      "There are 508 steps in this demonstration\n",
      "This demonstration was recorded by the following cameras: ['dev2']\n",
      "{'actiontype': 'rc-position-target',\n",
      " 'camera': 'dev2',\n",
      " 'cameras': ['dev2'],\n",
      " 'maxsteps': 508,\n",
      " 'sensorprocessor': <sensorprocessing.sp_conv_vae.ConvVaeSensorProcessing object at 0x71006e3d3370>,\n",
      " 'source_dir': PosixPath('/home/lboloni/Documents/Hackingwork/__Temporary/BerryPicker-demos/demos/proprioception-uncluttered/2024_10_26__16_33_14'),\n",
      " 'trim_from': 1,\n",
      " 'trim_to': 508}\n",
      "(507, 128)\n",
      "(507, 6)\n"
     ]
    }
   ],
   "source": [
    "task = \"proprioception-uncluttered\"\n",
    "\n",
    "conv_vae_jsonfile = pathlib.Path(Config()[\"controller\"][\"vae_json\"])\n",
    "conv_vae_model_pthfile = pathlib.Path(Config()[\"controller\"][\"vae_model\"])\n",
    "\n",
    "\n",
    "sp = sp_conv_vae.ConvVaeSensorProcessing(conv_vae_jsonfile,\n",
    "                                         conv_vae_model_pthfile)\n",
    "\n",
    "demos_dir = pathlib.Path(Config()[\"demos\"][\"directory\"])\n",
    "task_dir = pathlib.Path(demos_dir, \"demos\", task)\n",
    "\n",
    "inputlist = []\n",
    "targetlist = []\n",
    "\n",
    "for demo_dir in task_dir.iterdir():\n",
    "    if not demo_dir.is_dir():\n",
    "        pass\n",
    "    bcd = BCDemonstration(demo_dir, sensorprocessor=sp)\n",
    "    print(bcd)\n",
    "    z, a = bcd.read_z_a()\n",
    "    print(z.shape)\n",
    "    print(a.shape)\n",
    "    # FIXME the repeated name for inputs and targets\n",
    "    inputs, targets = create_RNN_training_sequence_xy(z, a, sequence_length=10)\n",
    "    inputlist.append(inputs)\n",
    "    targetlist.append(targets)\n",
    "\n",
    "inputs = torch.cat(inputlist)\n",
    "targets = torch.cat(targetlist)\n",
    "\n",
    "# Separate the training and validation data. \n",
    "# We will be shuffling the demonstrations \n",
    "rows = torch.randperm(inputs.size(0)) \n",
    "shuffled_inputs = inputs[rows]\n",
    "shuffled_targets = targets[rows]\n",
    "\n",
    "training_size = int( inputs.size(0) * 0.67 )\n",
    "inputs_training = shuffled_inputs[1:training_size]\n",
    "targets_training = shuffled_targets[1:training_size]\n",
    "\n",
    "inputs_validation = shuffled_inputs[training_size:]\n",
    "targets_validation = shuffled_targets[training_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_behavior_cloning(model, criterion, inputs_validation, targets_validation):\n",
    "    num_sequences = inputs_validation.shape[0]\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for i in range(num_sequences):\n",
    "            # Forward pass\n",
    "            input_seq = inputs_validation[i]\n",
    "            target = targets_validation[i]\n",
    "            # Reshape for batch compatibility\n",
    "            input_seq = input_seq.unsqueeze(0)  # Shape: [1, sequence_length, latent_size]\n",
    "            target = target.unsqueeze(0)        # Shape: [1, latent_size]\n",
    "\n",
    "            outputs = model(input_seq)\n",
    "            loss = criterion(outputs, target)\n",
    "            # Accumulate loss\n",
    "            val_loss += loss.item()\n",
    "    avg_loss = val_loss / num_sequences\n",
    "    return avg_loss\n",
    "\n",
    "def train_behavior_cloning(model, optimizer, criterion, inputs_training, targets_training, inputs_validation, targets_validation, num_epochs, writer = None):\n",
    "    num_sequences = inputs_training.shape[0]\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        model.train()\n",
    "        \n",
    "        # Loop over each sequence in the batch\n",
    "        training_loss = 0\n",
    "        for i in range(num_sequences):\n",
    "            # Prepare input and target\n",
    "            input_seq = inputs_training[i]\n",
    "            target = targets_training[i]\n",
    "\n",
    "            # Reshape for batch compatibility\n",
    "            input_seq = input_seq.unsqueeze(0)  # Shape: [1, sequence_length, latent_size]\n",
    "            target = target.unsqueeze(0)        # Shape: [1, latent_size]\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(input_seq)\n",
    "            loss = criterion(output, target)\n",
    "            training_loss += loss.item()\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        avg_training_loss = training_loss / num_sequences\n",
    "        avg_validation_loss = validate_behavior_cloning(model, criterion, inputs_validation=inputs_validation, targets_validation=targets_validation)\n",
    "        if writer is not None:\n",
    "            writer.add_scalar(\"TrainingLoss\", avg_training_loss, epoch)\n",
    "            writer.add_scalar(\"ValidationLoss\", avg_validation_loss, epoch)\n",
    "            writer.flush()\n",
    "\n",
    "\n",
    "        if (epoch+1) % 2 == 0: # was 0\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_training_loss:.4f} Validation Loss: {avg_validation_loss:.4f} ')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/100 [00:14<12:09,  7.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/100], Training Loss: 681.1389 Validation Loss: 673.9735 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4/100 [00:30<12:33,  7.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/100], Training Loss: 657.6181 Validation Loss: 673.2660 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 6/100 [00:46<12:11,  7.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/100], Training Loss: 647.3553 Validation Loss: 655.2405 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 8/100 [01:01<11:40,  7.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/100], Training Loss: 605.5764 Validation Loss: 592.3224 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 10/100 [01:16<11:18,  7.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Training Loss: 430.0906 Validation Loss: 398.9063 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 12/100 [01:30<10:49,  7.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/100], Training Loss: 320.6419 Validation Loss: 311.4591 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 14/100 [01:45<10:39,  7.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/100], Training Loss: 278.7717 Validation Loss: 280.1741 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 16/100 [02:00<10:21,  7.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/100], Training Loss: 266.2260 Validation Loss: 263.6351 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 18/100 [02:15<10:02,  7.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/100], Training Loss: 241.3722 Validation Loss: 230.0230 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 20/100 [02:29<09:41,  7.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/100], Training Loss: 221.4708 Validation Loss: 232.3125 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 22/100 [02:44<09:27,  7.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/100], Training Loss: 218.3173 Validation Loss: 250.2005 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 24/100 [02:58<09:12,  7.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/100], Training Loss: 197.5010 Validation Loss: 174.7614 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 26/100 [03:12<08:49,  7.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26/100], Training Loss: 211.1167 Validation Loss: 237.6641 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 28/100 [03:27<08:38,  7.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28/100], Training Loss: 198.4672 Validation Loss: 170.8068 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 30/100 [03:42<08:33,  7.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/100], Training Loss: 195.5745 Validation Loss: 230.7051 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 32/100 [03:56<08:18,  7.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [32/100], Training Loss: 169.2372 Validation Loss: 159.3140 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 34/100 [04:11<08:03,  7.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [34/100], Training Loss: 177.7108 Validation Loss: 173.4800 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 36/100 [04:25<07:41,  7.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [36/100], Training Loss: 165.8977 Validation Loss: 176.2879 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 38/100 [04:39<07:22,  7.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [38/100], Training Loss: 163.1013 Validation Loss: 155.0908 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 40/100 [04:54<07:09,  7.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40/100], Training Loss: 171.2597 Validation Loss: 155.7435 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 42/100 [05:08<06:50,  7.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [42/100], Training Loss: 157.4768 Validation Loss: 158.5668 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 44/100 [05:22<06:31,  7.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [44/100], Training Loss: 164.1668 Validation Loss: 156.4378 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 46/100 [05:36<06:17,  6.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [46/100], Training Loss: 160.3865 Validation Loss: 153.0587 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 48/100 [05:50<06:15,  7.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [48/100], Training Loss: 160.3768 Validation Loss: 167.5916 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 50/100 [06:06<06:11,  7.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/100], Training Loss: 157.2570 Validation Loss: 152.8928 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 52/100 [06:20<05:53,  7.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [52/100], Training Loss: 158.5003 Validation Loss: 152.5841 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 54/100 [06:36<05:44,  7.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [54/100], Training Loss: 153.1759 Validation Loss: 151.1868 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 56/100 [06:51<05:30,  7.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [56/100], Training Loss: 151.6233 Validation Loss: 149.2172 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 58/100 [07:06<05:21,  7.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [58/100], Training Loss: 157.1310 Validation Loss: 154.3923 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 60/100 [07:22<05:06,  7.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [60/100], Training Loss: 146.8540 Validation Loss: 148.6823 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 62/100 [07:36<04:45,  7.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [62/100], Training Loss: 146.7536 Validation Loss: 145.9733 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 64/100 [07:51<04:27,  7.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [64/100], Training Loss: 146.4322 Validation Loss: 149.3700 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 66/100 [08:06<04:13,  7.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [66/100], Training Loss: 144.4244 Validation Loss: 139.3078 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 68/100 [08:21<04:01,  7.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [68/100], Training Loss: 142.5531 Validation Loss: 139.9715 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 70/100 [08:36<03:44,  7.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [70/100], Training Loss: 139.4719 Validation Loss: 140.2685 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 72/100 [08:51<03:28,  7.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [72/100], Training Loss: 142.0863 Validation Loss: 139.7440 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 74/100 [09:06<03:16,  7.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [74/100], Training Loss: 138.5612 Validation Loss: 138.2125 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 76/100 [09:21<02:59,  7.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [76/100], Training Loss: 146.0120 Validation Loss: 139.1573 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 78/100 [09:36<02:45,  7.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [78/100], Training Loss: 142.5335 Validation Loss: 139.5805 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 80/100 [09:51<02:32,  7.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [80/100], Training Loss: 139.2310 Validation Loss: 140.7064 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 82/100 [10:06<02:15,  7.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [82/100], Training Loss: 151.2741 Validation Loss: 138.1753 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 84/100 [10:22<02:00,  7.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [84/100], Training Loss: 138.3865 Validation Loss: 141.8983 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 86/100 [10:36<01:43,  7.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [86/100], Training Loss: 137.3000 Validation Loss: 153.0839 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 88/100 [10:51<01:29,  7.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [88/100], Training Loss: 140.3089 Validation Loss: 136.6296 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 90/100 [11:06<01:14,  7.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [90/100], Training Loss: 138.1232 Validation Loss: 135.3617 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 92/100 [11:21<00:59,  7.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [92/100], Training Loss: 133.7925 Validation Loss: 138.2719 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 94/100 [11:35<00:44,  7.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [94/100], Training Loss: 135.8373 Validation Loss: 136.8074 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 96/100 [11:50<00:29,  7.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [96/100], Training Loss: 135.3632 Validation Loss: 135.6534 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 98/100 [12:05<00:15,  7.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [98/100], Training Loss: 132.4732 Validation Loss: 132.8127 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [12:21<00:00,  7.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100], Training Loss: 135.8327 Validation Loss: 134.6216 \n",
      "Training complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'write' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 27\u001b[0m\n\u001b[1;32m     19\u001b[0m train_behavior_cloning(\n\u001b[1;32m     20\u001b[0m     model, optimizer, criterion,\n\u001b[1;32m     21\u001b[0m     inputs_training\u001b[38;5;241m=\u001b[39minputs_training, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m     targets_validation\u001b[38;5;241m=\u001b[39mtargets_validation,\n\u001b[1;32m     25\u001b[0m     num_epochs\u001b[38;5;241m=\u001b[39mnum_epochs, writer\u001b[38;5;241m=\u001b[39mwriter)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining complete.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 27\u001b[0m \u001b[43mwrite\u001b[49m\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'write' is not defined"
     ]
    }
   ],
   "source": [
    "# Original\n",
    "latent_size = Config()[\"robot\"][\"latent_encoding_size\"]  \n",
    "hidden_size = 32  # degrees of freedom in the robot\n",
    "output_size = 6  # degrees of freedom in the robot\n",
    "num_layers = 2\n",
    "\n",
    "# Instantiate model, loss function, and optimizer\n",
    "model = LSTMXYPredictor(latent_size=latent_size, hidden_size=hidden_size, output_size = output_size, num_layers=num_layers)\n",
    "criterion = nn.MSELoss()  # Mean Squared Error for regression\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 100\n",
    "\n",
    "# Create a SummaryWriter instance\n",
    "# where does the logdir go???\n",
    "writer = SummaryWriter(logdir=\"/home/lboloni/runs/example\")\n",
    "\n",
    "train_behavior_cloning(\n",
    "    model, optimizer, criterion,\n",
    "    inputs_training=inputs_training, \n",
    "    targets_training=targets_training, \n",
    "    inputs_validation=inputs_validation,\n",
    "    targets_validation=targets_validation,\n",
    "    num_epochs=num_epochs, writer=writer)\n",
    "print(\"Training complete.\")\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# FIXME: save the model\n",
    "filename_lstm = Config()[\"controller\"][\"lstm_model_file\"]\n",
    "torch.save(model.state_dict(), filename_lstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the behavior cloning controller and use it with a real time data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original\n",
    "latent_size = Config()[\"robot\"][\"latent_encoding_size\"]  \n",
    "hidden_size = 32  # degrees of freedom in the robot\n",
    "output_size = 6  # degrees of freedom in the robot\n",
    "num_layers = 2\n",
    "\n",
    "# Instantiate model, loss function, and optimizer\n",
    "model = LSTMXYPredictor(latent_size=latent_size, hidden_size=hidden_size, output_size = output_size, num_layers=num_layers)\n",
    "criterion = nn.MSELoss()  # Mean Squared Error for regression\n",
    "filename_lstm = Config()[\"controller\"][\"lstm_model_file\"]\n",
    "model.load_state_dict(torch.load(filename_lstm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get one demonstration\n",
    "task = \"proprioception-uncluttered\"\n",
    "sp = sp_conv_vae.ConvVaeSensorProcessing()\n",
    "\n",
    "demos_dir = pathlib.Path(Config()[\"demos\"][\"directory\"])\n",
    "task_dir = pathlib.Path(demos_dir, \"demos\", task)\n",
    "\n",
    "inputlist = []\n",
    "targetlist = []\n",
    "\n",
    "demo_dir = next(task_dir.iterdir())\n",
    "bcd = BCDemonstration(demo_dir, sensorprocessor=sp)\n",
    "z, a = bcd.read_z_a()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.shape[0]\n",
    "print(a[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(z.shape[0]-1):\n",
    "    input = torch.from_numpy(z[i])\n",
    "    input = input.unsqueeze(0)\n",
    "    input = input.unsqueeze(0)\n",
    "    print(input)\n",
    "    a_pred = model.forward_keep_state(input)\n",
    "    a_real = a[i+1]\n",
    "    print(f\"a_real: {a_real}\\na_pred: {a_pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
