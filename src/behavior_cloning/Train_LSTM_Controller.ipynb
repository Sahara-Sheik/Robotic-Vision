{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train an LSTM based controller \n",
    "\n",
    "Train and save an LSTM-based controller. It contains:\n",
    "* Code for loading and pre-processing the training data. \n",
    "* Training an LSTM with specific parameters and saving it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pointer config file: /home/lboloni/.config/BerryPicker/mainsettings.yaml\n",
      "Loading machine-specific config file: /home/lboloni/Insync/lotzi.boloni@gmail.com/Google Drive/LotziStudy/Code/PackageTracking/BerryPicker/settings/settings-tredy2.yaml\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from settings import Config\n",
    "\n",
    "import pathlib\n",
    "#from pprint import pformat\n",
    "from tqdm import tqdm\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "from sensorprocessing import sp_conv_vae\n",
    "from demo_to_trainingdata import create_RNN_training_sequence_xy, BCDemonstration\n",
    "from bc_LSTM import LSTMXYPredictor, LSTMResidualController\n",
    "\n",
    "from tensorboardX import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating training and validation data\n",
    "Create training and validation data from all the demonstrations of a certain task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m task \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproprioception-uncluttered\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m sp \u001b[38;5;241m=\u001b[39m \u001b[43msp_conv_vae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConvVaeSensorProcessing\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m demos_dir \u001b[38;5;241m=\u001b[39m pathlib\u001b[38;5;241m.\u001b[39mPath(Config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdemos\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdirectory\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      5\u001b[0m task_dir \u001b[38;5;241m=\u001b[39m pathlib\u001b[38;5;241m.\u001b[39mPath(demos_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdemos\u001b[39m\u001b[38;5;124m\"\u001b[39m, task)\n",
      "File \u001b[0;32m~/Documents/Hackingwork/_Checkouts/BerryPicker/BerryPicker/src/behavior_cloning/../sensorprocessing/sp_conv_vae.py:50\u001b[0m, in \u001b[0;36mConvVaeSensorProcessing.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     46\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;124;03m    TODO: Once it is cleaned up, transfer this code back to encoding-conv-vae and use it in Experiment-Conv-Vae as well to\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;124;03m    load the latest model, and use it to process\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjsonfile, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresume_model \u001b[38;5;241m=\u001b[39m \u001b[43mlatest_json_and_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m get_conv_vae_config(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjsonfile, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresume_model, inference_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;66;03m# build model architecture\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Hackingwork/_Checkouts/BerryPicker/BerryPicker/src/behavior_cloning/../encoding_conv_vae/conv_vae.py:165\u001b[0m, in \u001b[0;36mlatest_json_and_model\u001b[0;34m(values)\u001b[0m\n\u001b[1;32m    163\u001b[0m model \u001b[38;5;241m=\u001b[39m latest_model(model_path)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;66;03m# The model from which we are starting        \u001b[39;00m\n\u001b[0;32m--> 165\u001b[0m resume_model \u001b[38;5;241m=\u001b[39m \u001b[43mpathlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m jsonfile \u001b[38;5;241m=\u001b[39m pathlib\u001b[38;5;241m.\u001b[39mPath(model_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresume_model and jsonfile are:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mresume_model=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresume_model\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mjsonfile=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjsonfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.10/pathlib.py:960\u001b[0m, in \u001b[0;36mPath.__new__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    958\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m Path:\n\u001b[1;32m    959\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m WindowsPath \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnt\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m PosixPath\n\u001b[0;32m--> 960\u001b[0m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_parts\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flavour\u001b[38;5;241m.\u001b[39mis_supported:\n\u001b[1;32m    962\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot instantiate \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m on your system\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    963\u001b[0m                               \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,))\n",
      "File \u001b[0;32m/usr/lib/python3.10/pathlib.py:594\u001b[0m, in \u001b[0;36mPurePath._from_parts\u001b[0;34m(cls, args)\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    590\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_from_parts\u001b[39m(\u001b[38;5;28mcls\u001b[39m, args):\n\u001b[1;32m    591\u001b[0m     \u001b[38;5;66;03m# We need to call _parse_args on the instance, so as to get the\u001b[39;00m\n\u001b[1;32m    592\u001b[0m     \u001b[38;5;66;03m# right flavour.\u001b[39;00m\n\u001b[1;32m    593\u001b[0m     \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m--> 594\u001b[0m     drv, root, parts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    595\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_drv \u001b[38;5;241m=\u001b[39m drv\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_root \u001b[38;5;241m=\u001b[39m root\n",
      "File \u001b[0;32m/usr/lib/python3.10/pathlib.py:578\u001b[0m, in \u001b[0;36mPurePath._parse_args\u001b[0;34m(cls, args)\u001b[0m\n\u001b[1;32m    576\u001b[0m     parts \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39m_parts\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 578\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(a, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;66;03m# Force-cast str subclasses to str (issue #21127)\u001b[39;00m\n\u001b[1;32m    581\u001b[0m         parts\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mstr\u001b[39m(a))\n",
      "\u001b[0;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not NoneType"
     ]
    }
   ],
   "source": [
    "task = \"proprioception-uncluttered\"\n",
    "sp = sp_conv_vae.ConvVaeSensorProcessing()\n",
    "\n",
    "demos_dir = pathlib.Path(Config()[\"demos\"][\"directory\"])\n",
    "task_dir = pathlib.Path(demos_dir, \"demos\", task)\n",
    "\n",
    "inputlist = []\n",
    "targetlist = []\n",
    "\n",
    "for demo_dir in task_dir.iterdir():\n",
    "    if not demo_dir.is_dir():\n",
    "        pass\n",
    "    bcd = BCDemonstration(demo_dir, sensorprocessor=sp)\n",
    "    print(bcd)\n",
    "    z, a = bcd.read_z_a()\n",
    "    print(z.shape)\n",
    "    print(a.shape)\n",
    "    # FIXME the repeated name for inputs and targets\n",
    "    inputs, targets = create_RNN_training_sequence_xy(z, a, sequence_length=10)\n",
    "    inputlist.append(inputs)\n",
    "    targetlist.append(targets)\n",
    "\n",
    "inputs = torch.cat(inputlist)\n",
    "targets = torch.cat(targetlist)\n",
    "\n",
    "# Separate the training and validation data. \n",
    "# We will be shuffling the demonstrations \n",
    "rows = torch.randperm(inputs.size(0)) \n",
    "shuffled_inputs = inputs[rows]\n",
    "shuffled_targets = targets[rows]\n",
    "\n",
    "training_size = int( inputs.size(0) * 0.67 )\n",
    "inputs_training = shuffled_inputs[1:training_size]\n",
    "targets_training = shuffled_targets[1:training_size]\n",
    "\n",
    "inputs_validation = shuffled_inputs[training_size:]\n",
    "targets_validation = shuffled_targets[training_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_behavior_cloning(model, criterion, inputs_validation, targets_validation):\n",
    "    num_sequences = inputs_validation.shape[0]\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for i in range(num_sequences):\n",
    "            # Forward pass\n",
    "            input_seq = inputs_validation[i]\n",
    "            target = targets_validation[i]\n",
    "            # Reshape for batch compatibility\n",
    "            input_seq = input_seq.unsqueeze(0)  # Shape: [1, sequence_length, latent_size]\n",
    "            target = target.unsqueeze(0)        # Shape: [1, latent_size]\n",
    "\n",
    "            outputs = model(input_seq)\n",
    "            loss = criterion(outputs, target)\n",
    "            # Accumulate loss\n",
    "            val_loss += loss.item()\n",
    "    avg_loss = val_loss / num_sequences\n",
    "    return avg_loss\n",
    "\n",
    "def train_behavior_cloning(model, optimizer, criterion, inputs_training, targets_training, inputs_validation, targets_validation, num_epochs, writer = ModuleNotFoundError):\n",
    "    num_sequences = inputs_training.shape[0]\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        model.train()\n",
    "        \n",
    "        # Loop over each sequence in the batch\n",
    "        training_loss = 0\n",
    "        for i in range(num_sequences):\n",
    "            # Prepare input and target\n",
    "            input_seq = inputs_training[i]\n",
    "            target = targets_training[i]\n",
    "\n",
    "            # Reshape for batch compatibility\n",
    "            input_seq = input_seq.unsqueeze(0)  # Shape: [1, sequence_length, latent_size]\n",
    "            target = target.unsqueeze(0)        # Shape: [1, latent_size]\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(input_seq)\n",
    "            loss = criterion(output, target)\n",
    "            training_loss += loss.item()\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        avg_training_loss = training_loss / num_sequences\n",
    "        avg_validation_loss = validate_behavior_cloning(model, criterion, inputs_validation=inputs_validation, targets_validation=targets_validation)\n",
    "        if writer is not None:\n",
    "            writer.add_scalar(\"Training loss\", avg_training_loss)\n",
    "            writer.add_scalar(\"Validation loss\", avg_validation_loss)\n",
    "\n",
    "\n",
    "        if (epoch+1) % 2 == 0: # was 0\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_training_loss:.4f} Validation Loss: {avg_validation_loss:.4f} ')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original\n",
    "latent_size = Config()[\"robot\"][\"latent_encoding_size\"]  \n",
    "hidden_size = 32  # degrees of freedom in the robot\n",
    "output_size = 6  # degrees of freedom in the robot\n",
    "num_layers = 2\n",
    "\n",
    "# Instantiate model, loss function, and optimizer\n",
    "model = LSTMXYPredictor(latent_size=latent_size, hidden_size=hidden_size, output_size = output_size, num_layers=num_layers)\n",
    "criterion = nn.MSELoss()  # Mean Squared Error for regression\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 8\n",
    "\n",
    "# Create a SummaryWriter instance\n",
    "# where does the logdir go???\n",
    "writer = SummaryWriter(logdir=\"runs/example\")\n",
    "\n",
    "train_behavior_cloning(\n",
    "    model, optimizer, criterion,\n",
    "    inputs_training=inputs_training, \n",
    "    targets_training=targets_training, \n",
    "    inputs_validation=inputs_validation,\n",
    "    targets_validation=targets_validation,\n",
    "    num_epochs=num_epochs, writer=writer)\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# FIXME: save the model\n",
    "filename_lstm = Config()[\"controller\"][\"lstm_model_file\"]\n",
    "torch.save(model.state_dict(), filename_lstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the behavior cloning controller and use it with a real time data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original\n",
    "latent_size = Config()[\"robot\"][\"latent_encoding_size\"]  \n",
    "hidden_size = 32  # degrees of freedom in the robot\n",
    "output_size = 6  # degrees of freedom in the robot\n",
    "num_layers = 2\n",
    "\n",
    "# Instantiate model, loss function, and optimizer\n",
    "model = LSTMXYPredictor(latent_size=latent_size, hidden_size=hidden_size, output_size = output_size, num_layers=num_layers)\n",
    "criterion = nn.MSELoss()  # Mean Squared Error for regression\n",
    "filename_lstm = Config()[\"controller\"][\"lstm_model_file\"]\n",
    "model.load_state_dict(torch.load(filename_lstm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get one demonstration\n",
    "task = \"proprioception-uncluttered\"\n",
    "sp = sp_conv_vae.ConvVaeSensorProcessing()\n",
    "\n",
    "demos_dir = pathlib.Path(Config()[\"demos\"][\"directory\"])\n",
    "task_dir = pathlib.Path(demos_dir, \"demos\", task)\n",
    "\n",
    "inputlist = []\n",
    "targetlist = []\n",
    "\n",
    "demo_dir = next(task_dir.iterdir())\n",
    "bcd = BCDemonstration(demo_dir, sensorprocessor=sp)\n",
    "z, a = bcd.read_z_a()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.shape[0]\n",
    "print(a[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(z.shape[0]-1):\n",
    "    input = torch.from_numpy(z[i])\n",
    "    input = input.unsqueeze(0)\n",
    "    input = input.unsqueeze(0)\n",
    "    print(input)\n",
    "    a_pred = model.forward_keep_state(input)\n",
    "    a_real = a[i+1]\n",
    "    print(f\"a_real: {a_real}\\na_pred: {a_pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
