{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train an LSTM based controller \n",
    "\n",
    "Train and save an LSTM-based controller. It contains:\n",
    "* Code for loading and pre-processing the training data. \n",
    "* Training an LSTM with specific parameters and saving it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pointer config file: /home/lboloni/.config/BerryPicker/mainsettings.yaml\n",
      "Loading machine-specific config file: /home/lboloni/Insync/lotzi.boloni@gmail.com/Google Drive/LotziStudy/Code/PackageTracking/BerryPicker/settings/settings-tredy2.yaml\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from settings import Config\n",
    "\n",
    "import pathlib\n",
    "#from pprint import pformat\n",
    "\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "from sensorprocessing import sp_conv_vae\n",
    "from demo_to_trainingdata import create_RNN_training_sequence_xy, BCDemonstration\n",
    "from bc_LSTM import LSTMXYPredictor, LSTMResidualController"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating training and validation data\n",
    "Create training and validation data from all the demonstrations of a certain task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resume_model and jsonfile are:\n",
      "\tresume_model=/home/lboloni/Documents/Hackingwork/__Temporary/BerryPicker-models/Conv-VAE/models/VAE_Robot/0901_125042/checkpoint-epoch171.pth\n",
      "\tjsonfile=/home/lboloni/Documents/Hackingwork/__Temporary/BerryPicker-models/Conv-VAE/models/VAE_Robot/0901_125042/config.json\n",
      "Warning: logging configuration file is not found in logger/logger_config.json.\n",
      "{\n",
      "    \"name\": \"VAE_Robot\",\n",
      "    \"n_gpu\": 1,\n",
      "    \"arch\": {\n",
      "        \"type\": \"VanillaVAE\",\n",
      "        \"args\": {\n",
      "            \"in_channels\": 3,\n",
      "            \"latent_dims\": 128,\n",
      "            \"flow\": false\n",
      "        }\n",
      "    },\n",
      "    \"data_loader\": {\n",
      "        \"###type-prev\": \"RobotDataLoader\",\n",
      "        \"type\": \"CelebDataLoader\",\n",
      "        \"args\": {\n",
      "            \"data_dir\": \"/home/lboloni/Documents/Hackingwork/__Temporary/VisionBasedRobotManipulator-training-data/vae-training-data\",\n",
      "            \"batch_size\": 64,\n",
      "            \"shuffle\": true,\n",
      "            \"validation_split\": 0.2,\n",
      "            \"num_workers\": 2\n",
      "        }\n",
      "    },\n",
      "    \"optimizer\": {\n",
      "        \"type\": \"Adam\",\n",
      "        \"args\": {\n",
      "            \"lr\": 0.005,\n",
      "            \"weight_decay\": 0.0,\n",
      "            \"amsgrad\": true\n",
      "        }\n",
      "    },\n",
      "    \"loss\": \"elbo_loss\",\n",
      "    \"metrics\": [],\n",
      "    \"lr_scheduler\": {\n",
      "        \"type\": \"StepLR\",\n",
      "        \"args\": {\n",
      "            \"step_size\": 50,\n",
      "            \"gamma\": 0.1\n",
      "        }\n",
      "    },\n",
      "    \"trainer\": {\n",
      "        \"epochs\": 200,\n",
      "        \"save_dir\": \"/home/lboloni/Documents/Hackingwork/__Temporary/VisionBasedRobotManipulator-models/Conv-VAE/\",\n",
      "        \"save_period\": 1,\n",
      "        \"verbosity\": 2,\n",
      "        \"monitor\": \"min val_loss\",\n",
      "        \"early_stop\": 10,\n",
      "        \"tensorboard\": true\n",
      "    }\n",
      "}\n",
      "VanillaVAE(\n",
      "  (encoder): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "  )\n",
      "  (fc_mu): Linear(in_features=2048, out_features=128, bias=True)\n",
      "  (fc_var): Linear(in_features=2048, out_features=128, bias=True)\n",
      "  (decoder_input): Linear(in_features=128, out_features=2048, bias=True)\n",
      "  (decoder): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.01)\n",
      "    )\n",
      "  )\n",
      "  (final_layer): Sequential(\n",
      "    (0): ConvTranspose2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.01)\n",
      "    (3): Conv2d(32, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): Tanh()\n",
      "  )\n",
      ")\n",
      "Trainable parameters: 3937635\n",
      "Loading the checkpoint\n",
      "Checkpoint loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lboloni/Documents/Hackingwork/_Checkouts/BerryPicker/BerryPicker/src/behavior_cloning/../sensorprocessing/sp_conv_vae.py:72: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.checkpoint = torch.load(self.config.resume, map_location=torch.device('cpu'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cameras found: ['dev2']\n",
      "There are 753 steps in this demonstration\n",
      "This demonstration was recorded by the following cameras: ['dev2']\n",
      "{'actiontype': 'rc-position-target',\n",
      " 'camera': 'dev2',\n",
      " 'cameras': ['dev2'],\n",
      " 'maxsteps': 753,\n",
      " 'sensorprocessor': <sensorprocessing.sp_conv_vae.ConvVaeSensorProcessing object at 0x7594e0dc2d40>,\n",
      " 'source_dir': PosixPath('/home/lboloni/Documents/Hackingwork/__Temporary/BerryPicker-demos/demos/proprioception-uncluttered/2024_10_26__16_31_40'),\n",
      " 'trim_from': 1,\n",
      " 'trim_to': 753}\n",
      "(752, 128)\n",
      "(752, 6)\n",
      "Cameras found: ['dev2']\n",
      "There are 968 steps in this demonstration\n",
      "This demonstration was recorded by the following cameras: ['dev2']\n",
      "{'actiontype': 'rc-position-target',\n",
      " 'camera': 'dev2',\n",
      " 'cameras': ['dev2'],\n",
      " 'maxsteps': 968,\n",
      " 'sensorprocessor': <sensorprocessing.sp_conv_vae.ConvVaeSensorProcessing object at 0x7594e0dc2d40>,\n",
      " 'source_dir': PosixPath('/home/lboloni/Documents/Hackingwork/__Temporary/BerryPicker-demos/demos/proprioception-uncluttered/2024_10_26__16_23_22'),\n",
      " 'trim_from': 1,\n",
      " 'trim_to': 968}\n",
      "(967, 128)\n",
      "(967, 6)\n",
      "Cameras found: ['dev2']\n",
      "There are 744 steps in this demonstration\n",
      "This demonstration was recorded by the following cameras: ['dev2']\n",
      "{'actiontype': 'rc-position-target',\n",
      " 'camera': 'dev2',\n",
      " 'cameras': ['dev2'],\n",
      " 'maxsteps': 744,\n",
      " 'sensorprocessor': <sensorprocessing.sp_conv_vae.ConvVaeSensorProcessing object at 0x7594e0dc2d40>,\n",
      " 'source_dir': PosixPath('/home/lboloni/Documents/Hackingwork/__Temporary/BerryPicker-demos/demos/proprioception-uncluttered/2024_10_26__16_18_47'),\n",
      " 'trim_from': 1,\n",
      " 'trim_to': 744}\n",
      "(743, 128)\n",
      "(743, 6)\n",
      "Cameras found: ['dev2']\n",
      "There are 469 steps in this demonstration\n",
      "This demonstration was recorded by the following cameras: ['dev2']\n",
      "{'actiontype': 'rc-position-target',\n",
      " 'camera': 'dev2',\n",
      " 'cameras': ['dev2'],\n",
      " 'maxsteps': 469,\n",
      " 'sensorprocessor': <sensorprocessing.sp_conv_vae.ConvVaeSensorProcessing object at 0x7594e0dc2d40>,\n",
      " 'source_dir': PosixPath('/home/lboloni/Documents/Hackingwork/__Temporary/BerryPicker-demos/demos/proprioception-uncluttered/2024_10_26__16_34_31'),\n",
      " 'trim_from': 1,\n",
      " 'trim_to': 469}\n",
      "(468, 128)\n",
      "(468, 6)\n",
      "Cameras found: ['dev2']\n",
      "There are 787 steps in this demonstration\n",
      "This demonstration was recorded by the following cameras: ['dev2']\n",
      "{'actiontype': 'rc-position-target',\n",
      " 'camera': 'dev2',\n",
      " 'cameras': ['dev2'],\n",
      " 'maxsteps': 787,\n",
      " 'sensorprocessor': <sensorprocessing.sp_conv_vae.ConvVaeSensorProcessing object at 0x7594e0dc2d40>,\n",
      " 'source_dir': PosixPath('/home/lboloni/Documents/Hackingwork/__Temporary/BerryPicker-demos/demos/proprioception-uncluttered/2024_10_26__16_36_02'),\n",
      " 'trim_from': 1,\n",
      " 'trim_to': 787}\n",
      "(786, 128)\n",
      "(786, 6)\n",
      "Cameras found: ['dev2']\n",
      "There are 508 steps in this demonstration\n",
      "This demonstration was recorded by the following cameras: ['dev2']\n",
      "{'actiontype': 'rc-position-target',\n",
      " 'camera': 'dev2',\n",
      " 'cameras': ['dev2'],\n",
      " 'maxsteps': 508,\n",
      " 'sensorprocessor': <sensorprocessing.sp_conv_vae.ConvVaeSensorProcessing object at 0x7594e0dc2d40>,\n",
      " 'source_dir': PosixPath('/home/lboloni/Documents/Hackingwork/__Temporary/BerryPicker-demos/demos/proprioception-uncluttered/2024_10_26__16_33_14'),\n",
      " 'trim_from': 1,\n",
      " 'trim_to': 508}\n",
      "(507, 128)\n",
      "(507, 6)\n"
     ]
    }
   ],
   "source": [
    "task = \"proprioception-uncluttered\"\n",
    "sp = sp_conv_vae.ConvVaeSensorProcessing()\n",
    "\n",
    "demos_dir = pathlib.Path(Config()[\"demos\"][\"directory\"])\n",
    "task_dir = pathlib.Path(demos_dir, \"demos\", task)\n",
    "\n",
    "inputlist = []\n",
    "targetlist = []\n",
    "\n",
    "for demo_dir in task_dir.iterdir():\n",
    "    if not demo_dir.is_dir():\n",
    "        pass\n",
    "    bcd = BCDemonstration(demo_dir, sensorprocessor=sp)\n",
    "    print(bcd)\n",
    "    z, a = bcd.read_z_a()\n",
    "    print(z.shape)\n",
    "    print(a.shape)\n",
    "    inputs, targets = create_RNN_training_sequence_xy(z, a, sequence_length=10)\n",
    "    inputlist.append(inputs)\n",
    "    targetlist.append(targets)\n",
    "\n",
    "inputs = torch.cat(inputlist)\n",
    "targets = torch.cat(targetlist)\n",
    "\n",
    "# Separate the training and validation data. \n",
    "# We will be shuffling the demonstrations \n",
    "rows = torch.randperm(inputs.size(0)) \n",
    "shuffled_inputs = inputs[rows]\n",
    "shuffled_targets = targets[rows]\n",
    "\n",
    "training_size = int( inputs.size(0) * 0.67 )\n",
    "inputs_training = shuffled_inputs[1:training_size]\n",
    "targets_training = shuffled_targets[1:training_size]\n",
    "\n",
    "inputs_validation = shuffled_inputs[training_size:]\n",
    "targets_validation = shuffled_targets[training_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_behavior_cloning(model, criterion, inputs_validation, targets_validation):\n",
    "    num_sequences = inputs_validation.shape[0]\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for i in range(num_sequences):\n",
    "            # Forward pass\n",
    "            input_seq = inputs_validation[i]\n",
    "            target = targets_validation[i]\n",
    "            # Reshape for batch compatibility\n",
    "            input_seq = input_seq.unsqueeze(0)  # Shape: [1, sequence_length, latent_size]\n",
    "            target = target.unsqueeze(0)        # Shape: [1, latent_size]\n",
    "\n",
    "            outputs = model(input_seq)\n",
    "            loss = criterion(outputs, target)\n",
    "            # Accumulate loss\n",
    "            val_loss += loss.item()\n",
    "    avg_loss = val_loss / num_sequences\n",
    "    return avg_loss\n",
    "\n",
    "def train_behavior_cloning(model, optimizer, criterion, inputs_training, targets_training, inputs_validation, targets_validation, num_epochs):\n",
    "    num_sequences = inputs_training.shape[0]\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        \n",
    "        # Loop over each sequence in the batch\n",
    "        training_loss = 0\n",
    "        for i in range(num_sequences):\n",
    "            # Prepare input and target\n",
    "            input_seq = inputs_training[i]\n",
    "            target = targets_training[i]\n",
    "\n",
    "            # Reshape for batch compatibility\n",
    "            input_seq = input_seq.unsqueeze(0)  # Shape: [1, sequence_length, latent_size]\n",
    "            target = target.unsqueeze(0)        # Shape: [1, latent_size]\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(input_seq)\n",
    "            loss = criterion(output, target)\n",
    "            training_loss += loss.item()\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        avg_training_loss = training_loss / num_sequences\n",
    "        avg_validation_loss = validate_behavior_cloning(model, criterion, inputs_validation=inputs_validation, targets_validation=targets_validation)\n",
    "\n",
    "        if (epoch+1) % 2 == 0: # was 0\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_training_loss:.4f} Validation Loss: {avg_validation_loss:.4f} ')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/8], Training Loss: 681.6785 Validation Loss: 674.0059 \n",
      "Epoch [4/8], Training Loss: 657.6183 Validation Loss: 673.2659 \n",
      "Epoch [6/8], Training Loss: 657.6234 Validation Loss: 673.2649 \n",
      "Epoch [8/8], Training Loss: 655.6940 Validation Loss: 670.5735 \n",
      "Training complete.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Parent directory /home/lboloni/Documents/Hackingwork/__Temporary/BerryPicker-models/Controller does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# FIXME: save the model\u001b[39;00m\n\u001b[1;32m     25\u001b[0m filename_lstm \u001b[38;5;241m=\u001b[39m Config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontroller\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlstm_model_file\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 26\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename_lstm\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Hackingwork/_VirtualEnv/venvBerryPicker/venv/lib/python3.10/site-packages/torch/serialization.py:651\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    648\u001b[0m _check_save_filelike(f)\n\u001b[1;32m    650\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m--> 651\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    652\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)\n\u001b[1;32m    653\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Hackingwork/_VirtualEnv/venvBerryPicker/venv/lib/python3.10/site-packages/torch/serialization.py:525\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[0;34m(name_or_buffer)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    524\u001b[0m     container \u001b[38;5;241m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[0;32m--> 525\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Hackingwork/_VirtualEnv/venvBerryPicker/venv/lib/python3.10/site-packages/torch/serialization.py:496\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    494\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mPyTorchFileWriter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream))\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 496\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Parent directory /home/lboloni/Documents/Hackingwork/__Temporary/BerryPicker-models/Controller does not exist."
     ]
    }
   ],
   "source": [
    "# Original\n",
    "latent_size = Config()[\"robot\"][\"latent_encoding_size\"]  \n",
    "hidden_size = 32  # degrees of freedom in the robot\n",
    "output_size = 6  # degrees of freedom in the robot\n",
    "num_layers = 2\n",
    "\n",
    "# Instantiate model, loss function, and optimizer\n",
    "model = LSTMXYPredictor(latent_size=latent_size, hidden_size=hidden_size, output_size = output_size, num_layers=num_layers)\n",
    "criterion = nn.MSELoss()  # Mean Squared Error for regression\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 8\n",
    "\n",
    "train_behavior_cloning(\n",
    "    model, optimizer, criterion,\n",
    "    inputs_training=inputs_training, \n",
    "    targets_training=targets_training, \n",
    "    inputs_validation=inputs_validation,\n",
    "    targets_validation=targets_validation,\n",
    "    num_epochs=num_epochs)\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# FIXME: save the model\n",
    "filename_lstm = Config()[\"controller\"][\"lstm_model_file\"]\n",
    "torch.save(model.state_dict(), filename_lstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the behavior cloning controller and use it with a real time data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_274610/962100842.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(filename_lstm))\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/lboloni/Documents/Hackingwork/__Temporary/BerryPicker-models/Controller/lstm.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()  \u001b[38;5;66;03m# Mean Squared Error for regression\u001b[39;00m\n\u001b[1;32m      4\u001b[0m filename_lstm \u001b[38;5;241m=\u001b[39m Config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontroller\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlstm_model_file\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m----> 5\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename_lstm\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Documents/Hackingwork/_VirtualEnv/venvBerryPicker/venv/lib/python3.10/site-packages/torch/serialization.py:1065\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m   1063\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m-> 1065\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m   1066\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1067\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1068\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/Documents/Hackingwork/_VirtualEnv/venvBerryPicker/venv/lib/python3.10/site-packages/torch/serialization.py:468\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 468\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    470\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/Documents/Hackingwork/_VirtualEnv/venvBerryPicker/venv/lib/python3.10/site-packages/torch/serialization.py:449\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/lboloni/Documents/Hackingwork/__Temporary/BerryPicker-models/Controller/lstm.pth'"
     ]
    }
   ],
   "source": [
    "# Instantiate model, loss function, and optimizer\n",
    "model = LSTMXYPredictor(latent_size=latent_size, hidden_size=hidden_size, output_size = output_size, num_layers=num_layers)\n",
    "criterion = nn.MSELoss()  # Mean Squared Error for regression\n",
    "filename_lstm = Config()[\"controller\"][\"lstm_model_file\"]\n",
    "model.load_state_dict(torch.load(filename_lstm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2222975137.py, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[13], line 11\u001b[0;36m\u001b[0m\n\u001b[0;31m    demo_dir = task_dir.iterdir().next():\u001b[0m\n\u001b[0m                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Get one demonstration\n",
    "task = \"proprioception-uncluttered\"\n",
    "sp = sp_conv_vae.ConvVaeSensorProcessing()\n",
    "\n",
    "demos_dir = pathlib.Path(Config()[\"demos\"][\"directory\"])\n",
    "task_dir = pathlib.Path(demos_dir, \"demos\", task)\n",
    "\n",
    "inputlist = []\n",
    "targetlist = []\n",
    "\n",
    "demo_dir = task_dir.iterdir().next():\n",
    "bcd = BCDemonstration(demo_dir, sensorprocessor=sp)\n",
    "z, a = bcd.read_z_a()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[43mz\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m z[i]\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not callable"
     ]
    }
   ],
   "source": [
    "for i in range(z.size(0)-1):\n",
    "    input = z[i]\n",
    "    input.unsqueeze(0)\n",
    "    a_pred = model.forward_keep_state(input)\n",
    "    a_real = a[i+1]\n",
    "    print(\"a_real: {a_real}\\na_pred: {a_pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
