{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intensive-pavilion",
   "metadata": {},
   "source": [
    "# MLP controller\n",
    "\n",
    "Developing and debugging the multilayer perceptron implementation of the robot controller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "numerous-keeping",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pathlib\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D, Softmax\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from IPython import display\n",
    "import pickle\n",
    "import logging\n",
    "import platform\n",
    "# the finished version of this code\n",
    "from CVAE_VisualModule import CVAE_VisualModule\n",
    "from Datasets import Datasets\n",
    "import paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "suitable-bronze",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_Controller(tf.keras.Model):\n",
    "    \"\"\"A simple implementation of a robot controller, as a two layer regression network\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        \"\"\" Create a controller with the specified latent dimensions and control dimensions\n",
    "        latent_dim: the dimensionality of the input, the size of the z \n",
    "        control_dim: the dimensionality of the output, the size of the robot controls\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.latent_dim = config[\"latent_dim\"]\n",
    "        self.control_dim = config[\"control_dim\"]\n",
    "        self.network = tf.keras.Sequential(\n",
    "        [\n",
    "            tf.keras.layers.InputLayer(input_shape=(self.latent_dim, )),\n",
    "            tf.keras.layers.Dense(30),\n",
    "            tf.keras.layers.Dense(self.control_dim),\n",
    "          ]\n",
    "        )\n",
    "        self.network.summary()\n",
    "        \n",
    "\n",
    "    def predict(self, z):\n",
    "        \"\"\"Make a prediction\"\"\"\n",
    "        actions = self.network.predict(z)\n",
    "        return actions\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_trained_model(config, visual_module=None):\n",
    "        \"\"\"Returns a controller, either by loading the already trained model, or training a model on the\n",
    "        demonstrations in the demonstration_dir\"\"\"\n",
    "        \n",
    "        model_path = config[\"model_path\"]\n",
    "        demonstration_control_path = config[\"demonstration_control_path\"]\n",
    "        demonstration_images_dir = config[\"demonstration_images_dir\"]\n",
    "        latent_dim = config[\"latent_dim\"]\n",
    "        control_dim = config[\"control_dim\"]\n",
    "        load_only = config[\"load_only\"]\n",
    "        \n",
    "        model = MLP_Controller(config)\n",
    "        model.network.compile(loss=\n",
    "                           'mean_squared_error', optimizer=tf.keras.optimizers.Adam(0.001))\n",
    "        config_path = pathlib.Path(model_path.parent, model_path.name + \".config\")\n",
    "        if config_path.exists():\n",
    "            with open(config_path, \"rb\") as f:\n",
    "                # FIXME: \n",
    "                config = pickle.load(f)            \n",
    "        else:            \n",
    "            config[\"epochs_trained\"] = 0\n",
    "        if pathlib.Path(model_path.parent, model_path.name + \".index\").exists():\n",
    "            model.load_weights(str(model_path))\n",
    "            logging.info(f\"model loaded from {model_path}\")\n",
    "            if config[\"epochs_trained\"] >= config[\"epochs_target\"]:\n",
    "                logging.info(f\"Model fully trained to the required number of epochs {config['epochs_target']}\")\n",
    "                return model\n",
    "        if config[\"load_only\"]:\n",
    "            raise Exception(f\"Was instructed to load only, this model needs training {model_path}\")\n",
    "        logging.info(\"Proceed to train the model\")\n",
    "        # create the demonstration data\n",
    "        ds = Datasets.create_image_control_demonstration_dataset(demonstration_control_path, demonstration_images_dir)\n",
    "        x = []\n",
    "        y = []\n",
    "        #for a in ds.take(10):\n",
    "        for a in ds:\n",
    "            y.append(a[\"control\"])\n",
    "            imgbatch = np.array([a[\"image\"]])            \n",
    "            # we treat this as the mean, and unpack it from the array\n",
    "            h = visual_module.encode(imgbatch)\n",
    "            x.append(h[0])    \n",
    "            #print(a)\n",
    "        x = np.array(x)\n",
    "        y = np.array(y)\n",
    "        # \n",
    "        epoch_init = config[\"epochs_trained\"]\n",
    "        epochs_target = config[\"epochs_target\"]\n",
    "        for epoch in range(epoch_init, epochs_target+1):\n",
    "            ### validation_split???\n",
    "            hist = model.network.fit(x, y, verbose=0, validation_split=0.2, epochs=1)\n",
    "            # FIXME: Does this miscount it???\n",
    "            config[\"epochs_trained\"] = epoch+1\n",
    "            #hist.history.keys()\n",
    "            if epoch % 100 == 0:\n",
    "                logging.info(f\"Loss: {hist.history['loss'][-1]} validation loss {hist.history['val_loss'][-1]}\")\n",
    "                logging.info(f\"training epoch {epoch} / {epochs_target}\")\n",
    "                \n",
    "                with open(config_path, \"wb\") as f:\n",
    "                    pickle.dump(config, f)                    \n",
    "                model.save_weights(str(model_path))\n",
    "        return model        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "third-improvement",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:model loaded from /home/lboloni/Documents/HackingWork/__Temporary/2021-05-01-RobotImitationLearning/cvae\n",
      "INFO:root:Model fully trained to the required number of epochs 100\n",
      "INFO:root:Proceed to train the model\n",
      "INFO:root:Started creating image control demonstration dataset from /home/lboloni/Documents/HackingWork/2021-05-01-RobotImitationLearning/data/demonstration-32-task-3001-10090/10090.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 30)                7710      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 7)                 217       \n",
      "=================================================================\n",
      "Total params: 7,927\n",
      "Trainable params: 7,927\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Done creating image control demonstration dataset, total size 999\n",
      "INFO:root:Loss: 0.33849355578422546 validation loss 0.16478510200977325\n",
      "INFO:root:training epoch 0 / 100\n",
      "INFO:root:Loss: 0.00340396910905838 validation loss 0.029761185869574547\n",
      "INFO:root:training epoch 100 / 100\n"
     ]
    }
   ],
   "source": [
    "# For the training process we need a visual module\n",
    "config_visual_module = {\n",
    "    \"latent_dim\": 256, \"image_width\": 32, \"image_height\": 32, \"image_color_channels\": 3,\n",
    "    \"epochs_target\": 100, \"training_data_dir\": paths.unsupervised_dir, \n",
    "    \"model_path\": paths.visual_module_model_path, \"load_only\": True\n",
    "}\n",
    "visual_module = CVAE_VisualModule.get_trained_model(config_visual_module)\n",
    "\n",
    "control_path = pathlib.Path(paths.demonstration_dir, \"10090.txt\")\n",
    "pictures_dir = pathlib.Path(paths.demonstration_dir, \"pictures\")\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"latent_dim\": 256, \"control_dim\": 7, \"epochs_target\": 100, \n",
    "    \"demonstration_control_path\": control_path,\n",
    "    \"demonstration_images_dir\": pictures_dir,\n",
    "    \"model_path\": paths.controller_model_path, \n",
    "    \"load_only\": False\n",
    "}\n",
    "\n",
    "\n",
    "controller = MLP_Controller.get_trained_model(config, visual_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silent-authorization",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
