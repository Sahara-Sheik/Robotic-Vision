{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CVAE_VisualModule_Dev\n",
    "\n",
    "Developing and debugging the VAE implementation of the visual module of a robot imitation controller. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pathlib\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D, Softmax\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from IPython import display\n",
    "import pickle\n",
    "import logging\n",
    "import platform\n",
    "from Datasets import Datasets\n",
    "import paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The CVAE implementation\n",
    "Implemented as an object, both the implementation code. \n",
    "\n",
    "The training functions, checkpointing etc. are packaged here together as static functions.\n",
    "\n",
    "The CVAE.py file contains all this information, although the version here might be modified for experiments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE_VisualModule(tf.keras.Model):\n",
    "    \"\"\" A convolutional variational autoencoder \"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        \"\"\" Create the CVAE model for a specific image size, and size of latent dimensions\"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        latent_dim = config[\"latent_dim\"]\n",
    "        size_x = config[\"image_width\"]\n",
    "        size_y = config[\"image_height\"]\n",
    "        color_channels = config[\"image_color_channels\"]\n",
    "        # this is how small it gets through successive decompositions\n",
    "        small_x = int(size_x / 4)\n",
    "        small_y = int(size_y / 4)\n",
    "        \n",
    "        # the inference network\n",
    "        self.inference_net = tf.keras.Sequential(\n",
    "        [\n",
    "            tf.keras.layers.InputLayer(input_shape=(size_x, size_y, color_channels)),\n",
    "            tf.keras.layers.Conv2D(\n",
    "                filters=32, kernel_size=3, strides=(2, 2), activation='relu'),\n",
    "            tf.keras.layers.Conv2D(\n",
    "                filters=64, kernel_size=3, strides=(2, 2), activation='relu'),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            # No activation\n",
    "            tf.keras.layers.Dense(latent_dim + latent_dim),\n",
    "          ]\n",
    "        )\n",
    "\n",
    "        # the generative net\n",
    "        self.generative_net = tf.keras.Sequential(\n",
    "            [\n",
    "            tf.keras.layers.InputLayer(input_shape=(latent_dim,)),\n",
    "            tf.keras.layers.Dense(units=small_x * small_y * 32, activation=tf.nn.relu),\n",
    "            tf.keras.layers.Reshape(target_shape=(small_x, small_y, 32)),\n",
    "            tf.keras.layers.Conv2DTranspose(\n",
    "                filters=64,\n",
    "                kernel_size=3,\n",
    "                strides=(2, 2),\n",
    "                padding=\"SAME\",\n",
    "                activation='relu'),\n",
    "            tf.keras.layers.Conv2DTranspose(\n",
    "                filters=32,\n",
    "                kernel_size=3,\n",
    "                strides=(2, 2),\n",
    "                padding=\"SAME\",\n",
    "                activation='relu'),\n",
    "            # No activation\n",
    "            tf.keras.layers.Conv2DTranspose(\n",
    "                filters=3, kernel_size=3, strides=(1, 1), padding=\"SAME\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    @tf.function\n",
    "    def sample(self, eps=None):\n",
    "        \"\"\" Sampling a number of samples (by default 100 random generated ones) from the encoding, and decode them.\"\"\"\n",
    "        if eps is None:\n",
    "            eps = tf.random.normal(shape=(100, self.latent_dim))\n",
    "        return self.decode(eps, apply_sigmoid=True)\n",
    "\n",
    "    def encode_with_variance(self, x):\n",
    "        \"\"\" \n",
    "        Takes a batch of images x. Returns a batch of mean encodings, and a batch of log variance encodings.\n",
    "        \"\"\"\n",
    "        encoding = self.inference_net(x)\n",
    "        mean, logvar = tf.split(encoding, num_or_size_splits=2, axis=1)\n",
    "        return mean, logvar\n",
    "\n",
    "    def encode(self, x):\n",
    "        \"\"\" \n",
    "        Takes a batch of images x and returns a batch of encodings\n",
    "        \"\"\"\n",
    "        return self.encode_with_variance(x)[0]\n",
    "    \n",
    "    \n",
    "    def reparameterize(self, mean, logvar):\n",
    "        \"\"\"Performs the re-parametrization of the network by generating random values following the computed \n",
    "        log-variance and mean - the output of this one is a z value. \n",
    "        What re-parametrization means in this case is that this is going to be a random number. But we are pushing\n",
    "        in here a uniform distribution, so the parameters of this value will be the parameters that we used to \n",
    "        calculated the mean and logvar - that is the parameters of the inference network. \n",
    "        \n",
    "        FIXME: I don't understand what the 0.5 mean here?\n",
    "        \"\"\"\n",
    "        eps = tf.random.normal(shape=mean.shape)\n",
    "        return eps * tf.exp(logvar * .5) + mean\n",
    "\n",
    "    def decode(self, z, apply_sigmoid=False):\n",
    "        \"\"\"Decodes a certain z value by running the generative net, and possibly applying a sigmoid. \n",
    "        We might not apply the sigmoid, because we have loss expressions that directly take a logit. \n",
    "        We might apply the sigmoid, because it undoes the logit. \n",
    "        Then we simply interpret it not as a probability value, but as a color channel value. \"\"\"\n",
    "        logits = self.generative_net(z)\n",
    "        if apply_sigmoid:\n",
    "            probs = tf.sigmoid(logits)\n",
    "            return probs\n",
    "        return logits\n",
    "\n",
    "    @staticmethod   \n",
    "    def log_normal_pdf(sample, mean, logvar, raxis=1):\n",
    "        \"\"\"Implements the calculation of the log-normal probability density function.\n",
    "        Lotzi: if I understand correctly, this returns the sum of the probability for a certain sample, \n",
    "        when given the mean and the log variance for the probability. It does this for a vector.\"\"\"\n",
    "        log2pi = tf.math.log(2. * np.pi)\n",
    "        return tf.reduce_sum(\n",
    "            -.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi),\n",
    "            axis=raxis)\n",
    "\n",
    "    @tf.function\n",
    "    def compute_loss(self, x):\n",
    "        \"\"\" Calculates the loss of the VAE for an input batch x\"\"\"\n",
    "        # Runs through the model, and gets the output as a logit\n",
    "        mean, logvar = self.encode_with_variance(x)\n",
    "        z = self.reparameterize(mean, logvar)\n",
    "        x_logit = self.decode(z, apply_sigmoid = False)    \n",
    "        # calculates the cross entropy autoencoding loss, and then sums it up along x, y and color (is it???)\n",
    "        cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=x)\n",
    "        logpx_z = -tf.reduce_sum(cross_ent, axis=[1, 2, 3])\n",
    "        # the other component of the VAE loss is the KL divergence, \n",
    "        logpz = self.log_normal_pdf(z, 0., 0.)\n",
    "        logqz_x = self.log_normal_pdf(z, mean, logvar)\n",
    "        L = -tf.reduce_mean(logpx_z + logpz - logqz_x)\n",
    "        return L\n",
    "\n",
    "    @tf.function\n",
    "    def compute_apply_gradients(self, x, optimizer):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self.compute_loss(x)            \n",
    "            gradients = tape.gradient(loss, self.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "            # this won't work like this: it is only executed once at tracing\n",
    "            # print(float(loss))\n",
    "            #tf.print(loss)\n",
    "        return loss\n",
    "    \n",
    "    @staticmethod \n",
    "    def get_trained_model(config):\n",
    "        model_path = config[\"model_path\"]\n",
    "        epochs_target = config[\"epochs_target\"]\n",
    "        \"\"\"Returns a visual encoder, either by loading the existing model, \n",
    "        or training the model on the images in the unsupervised dir\"\"\"\n",
    "        model = CVAE_VisualModule(config)\n",
    "        # loading or creating the config file that \n",
    "        config_path = pathlib.Path(model_path.parent, model_path.name + \".config\")\n",
    "        if config_path.exists():\n",
    "            with open(config_path, \"rb\") as f:\n",
    "                # FIXME: this overwrites the config...\n",
    "                config = pickle.load(f)            \n",
    "        else:\n",
    "            # FIXME: it should be epoch_trained and start at 0\n",
    "            #\n",
    "            #config = {\"epoch\" : 1, \"epochs_max\": epochs_max}\n",
    "            config[\"epochs_trained\"] = 0\n",
    "        if pathlib.Path(model_path.parent, model_path.name + \".index\").exists():\n",
    "            model.load_weights(str(model_path))\n",
    "            logging.info(f\"model loaded from {model_path}\")\n",
    "            if config[\"epochs_trained\"] >= config[\"epochs_target\"]:\n",
    "                logging.info(f\"Model fully trained to the required number of epochs {config['epochs_target']}\")\n",
    "                return model\n",
    "        logging.debug(str(config))\n",
    "        # model either does not exist of it is not fully trained \n",
    "        if config[\"load_only\"]:\n",
    "            raise Exception(\"was instructed to load only, this model needs training.\")\n",
    "        CVAE_VisualModule.train_model(config, model, config_path, model_path)\n",
    "        logging.info(f\"Saving model to {model_path}\")\n",
    "        model.save_weights(str(config[\"model_path\"]))\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def train_model(config, model, config_path, model_path):\n",
    "        logging.info(\"Proceed to train the model\")\n",
    "        dataset = Datasets.create_unsupervised_dataset(config[\"training_data_dir\"])\n",
    "        optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "        epoch_init = config[\"epochs_trained\"]\n",
    "        epochs_target = config[\"epochs_target\"]\n",
    "        batch_count = 0\n",
    "        prev_batch_count = 0\n",
    "        for epoch in range(epoch_init, epochs_target+1):\n",
    "            config[\"epochs_trained\"] = epoch\n",
    "            for batch in dataset:\n",
    "                loss = model.compute_apply_gradients(batch, optimizer)\n",
    "                batch_count = batch_count + 1\n",
    "            # logging and saving intermediate values\n",
    "            if epoch % 100 == 0 or batch_count >= prev_batch_count + 1000:\n",
    "                logging.info(f\"training epoch {epoch} / {epochs_target} -- batch count {batch_count}\")\n",
    "                logging.info(f\"current loss: {loss.numpy()}\")\n",
    "                prev_batch_count = batch_count\n",
    "                with open(config_path, \"wb\") as f:\n",
    "                    pickle.dump(config, f)\n",
    "                model.save_weights(str(model_path))\n",
    "                # loss evaluation, on the first batch?                \n",
    "        logging.info(\"Model trained\")  \n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing and validation\n",
    "\n",
    "Experiments to test the performance and behavior of this VAE implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Proceed to train the model\n",
      "INFO:root:Started creating unsupervised dataset from /home/lboloni/Documents/HackingWork/2021-05-01-RobotImitationLearning/data/images-32-task-3001-10090\n",
      "INFO:root:About to create dataset from tensor slices\n",
      "INFO:root:About to shuffle\n",
      "INFO:root:About to batch\n",
      "INFO:root:Done creating unsupervised dataset from /home/lboloni/Documents/HackingWork/2021-05-01-RobotImitationLearning/data/images-32-task-3001-10090 with 111 images\n",
      "INFO:root:training epoch 0 / 100 -- batch count 6\n",
      "INFO:root:current loss: 2128.330078125\n",
      "INFO:root:training epoch 100 / 100 -- batch count 606\n",
      "INFO:root:current loss: 1947.858642578125\n",
      "INFO:root:Model trained\n",
      "INFO:root:Saving model to /home/lboloni/Documents/HackingWork/__Temporary/2021-05-01-RobotImitationLearning/cvae\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"latent_dim\": 50, \"image_width\": 32, \"image_height\": 32, \"image_color_channels\": 3,\n",
    "    \"epochs_target\": 100, \"training_data_dir\": paths.unsupervised_dir, \n",
    "    \"model_path\": paths.visual_module_model_path, \"load_only\": False\n",
    "}\n",
    "\n",
    "model = CVAE_VisualModule.get_trained_model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_image_no(dataset, number):\n",
    "    \"\"\"Returns the image number number from the dataset. Utility function\"\"\"\n",
    "    for batch in dataset:        \n",
    "        if number < len(batch):\n",
    "            img = batch[number]\n",
    "            return img\n",
    "        else:\n",
    "            number = number - len(batch)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Started creating unsupervised dataset from /home/lboloni/Documents/HackingWork/2021-05-01-RobotImitationLearning/data/images-32-task-3001-10090\n",
      "INFO:root:About to create dataset from tensor slices\n",
      "INFO:root:About to shuffle\n",
      "INFO:root:About to batch\n",
      "INFO:root:Done creating unsupervised dataset from /home/lboloni/Documents/HackingWork/2021-05-01-RobotImitationLearning/data/images-32-task-3001-10090 with 111 images\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Reconstituted')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# pick a certain image from the unsupervised dataset\n",
    "dataset = Datasets.create_unsupervised_dataset(paths.unsupervised_dir)\n",
    "img = pick_image_no(dataset, 80)\n",
    "x = np.array([img])\n",
    "z = model.encode(x)\n",
    "#print(z)\n",
    "# decode based on the center and no noise\n",
    "img2 = model.decode(z, apply_sigmoid=True)\n",
    "f, axs = plt.subplots(1,2, figsize=(10,5))\n",
    "axs[0].imshow(img, extent=[0, 32, 0, 32])\n",
    "axs[0].set_title(\"Original\")\n",
    "axs[1].imshow(img2[0], extent=[0, 32, 0, 32])\n",
    "axs[1].set_title(\"Reconstituted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batcheval = dataset.take(1)\n",
    "# why doesn't this work with take?\n",
    "for batcheval in dataset:\n",
    "    #print(batcheval)\n",
    "    value = model.compute_loss(batcheval)\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing on the full size dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Proceed to train the model\n",
      "INFO:root:Started creating unsupervised dataset from /home/lboloni/Documents/HackingWork/2021-05-01-Rouhollah-Code-and-Data/trajectories/al5d-32/task-3001/10090/camera-1\n",
      "INFO:root:At image: 5000\n",
      "INFO:root:At image: 10000\n",
      "INFO:root:At image: 15000\n",
      "INFO:root:At image: 20000\n",
      "INFO:root:At image: 25000\n",
      "INFO:root:At image: 30000\n",
      "INFO:root:At image: 35000\n",
      "INFO:root:At image: 40000\n",
      "INFO:root:At image: 45000\n",
      "INFO:root:At image: 50000\n",
      "INFO:root:At image: 55000\n",
      "INFO:root:About to create dataset from tensor slices\n",
      "INFO:root:About to shuffle\n",
      "INFO:root:About to batch\n",
      "INFO:root:Done creating unsupervised dataset from /home/lboloni/Documents/HackingWork/2021-05-01-Rouhollah-Code-and-Data/trajectories/al5d-32/task-3001/10090/camera-1 with 55163 images\n",
      "INFO:root:training epoch 0 / 100 -- batch count 3447\n",
      "INFO:root:current loss: 1903.7332763671875\n",
      "INFO:root:training epoch 1 / 100 -- batch count 6894\n",
      "INFO:root:current loss: 1919.394287109375\n",
      "INFO:root:training epoch 2 / 100 -- batch count 10341\n",
      "INFO:root:current loss: 1892.3515625\n",
      "INFO:root:training epoch 3 / 100 -- batch count 13788\n",
      "INFO:root:current loss: 1896.8499755859375\n",
      "INFO:root:training epoch 4 / 100 -- batch count 17235\n",
      "INFO:root:current loss: 1879.852294921875\n",
      "INFO:root:training epoch 5 / 100 -- batch count 20682\n",
      "INFO:root:current loss: 1883.5062255859375\n",
      "INFO:root:training epoch 6 / 100 -- batch count 24129\n",
      "INFO:root:current loss: 1887.812744140625\n",
      "INFO:root:training epoch 7 / 100 -- batch count 27576\n",
      "INFO:root:current loss: 1891.731689453125\n",
      "INFO:root:training epoch 8 / 100 -- batch count 31023\n",
      "INFO:root:current loss: 1864.3681640625\n",
      "INFO:root:training epoch 9 / 100 -- batch count 34470\n",
      "INFO:root:current loss: 1880.5562744140625\n",
      "INFO:root:training epoch 10 / 100 -- batch count 37917\n",
      "INFO:root:current loss: 1887.751953125\n",
      "INFO:root:training epoch 11 / 100 -- batch count 41364\n",
      "INFO:root:current loss: 1872.4425048828125\n",
      "INFO:root:training epoch 12 / 100 -- batch count 44811\n",
      "INFO:root:current loss: 1877.8309326171875\n",
      "INFO:root:training epoch 13 / 100 -- batch count 48258\n",
      "INFO:root:current loss: 1870.93212890625\n",
      "INFO:root:training epoch 14 / 100 -- batch count 51705\n",
      "INFO:root:current loss: 1893.470458984375\n",
      "INFO:root:training epoch 15 / 100 -- batch count 55152\n",
      "INFO:root:current loss: 1883.9068603515625\n",
      "INFO:root:training epoch 16 / 100 -- batch count 58599\n",
      "INFO:root:current loss: 1878.1866455078125\n",
      "INFO:root:training epoch 17 / 100 -- batch count 62046\n",
      "INFO:root:current loss: 1879.991455078125\n",
      "INFO:root:training epoch 18 / 100 -- batch count 65493\n",
      "INFO:root:current loss: 1878.08984375\n",
      "INFO:root:training epoch 19 / 100 -- batch count 68940\n",
      "INFO:root:current loss: 1868.86962890625\n",
      "INFO:root:training epoch 20 / 100 -- batch count 72387\n",
      "INFO:root:current loss: 1872.229248046875\n",
      "INFO:root:training epoch 21 / 100 -- batch count 75834\n",
      "INFO:root:current loss: 1875.590576171875\n",
      "INFO:root:training epoch 22 / 100 -- batch count 79281\n",
      "INFO:root:current loss: 1865.441162109375\n",
      "INFO:root:training epoch 23 / 100 -- batch count 82728\n",
      "INFO:root:current loss: 1876.50537109375\n",
      "INFO:root:training epoch 24 / 100 -- batch count 86175\n",
      "INFO:root:current loss: 1874.63916015625\n",
      "INFO:root:training epoch 25 / 100 -- batch count 89622\n",
      "INFO:root:current loss: 1872.944580078125\n",
      "INFO:root:training epoch 26 / 100 -- batch count 93069\n",
      "INFO:root:current loss: 1870.07666015625\n",
      "INFO:root:training epoch 27 / 100 -- batch count 96516\n",
      "INFO:root:current loss: 1874.543212890625\n",
      "INFO:root:training epoch 28 / 100 -- batch count 99963\n",
      "INFO:root:current loss: 1878.495361328125\n",
      "INFO:root:training epoch 29 / 100 -- batch count 103410\n",
      "INFO:root:current loss: 1867.6484375\n",
      "INFO:root:training epoch 30 / 100 -- batch count 106857\n",
      "INFO:root:current loss: 1890.589111328125\n",
      "INFO:root:training epoch 31 / 100 -- batch count 110304\n",
      "INFO:root:current loss: 1869.421630859375\n",
      "INFO:root:training epoch 32 / 100 -- batch count 113751\n",
      "INFO:root:current loss: 1876.507080078125\n",
      "INFO:root:training epoch 33 / 100 -- batch count 117198\n",
      "INFO:root:current loss: 1867.91748046875\n",
      "INFO:root:training epoch 34 / 100 -- batch count 120645\n",
      "INFO:root:current loss: 1855.5350341796875\n",
      "INFO:root:training epoch 35 / 100 -- batch count 124092\n",
      "INFO:root:current loss: 1870.99365234375\n",
      "INFO:root:training epoch 36 / 100 -- batch count 127539\n",
      "INFO:root:current loss: 1864.4014892578125\n",
      "INFO:root:training epoch 37 / 100 -- batch count 130986\n",
      "INFO:root:current loss: 1864.9263916015625\n",
      "INFO:root:training epoch 38 / 100 -- batch count 134433\n",
      "INFO:root:current loss: 1871.792724609375\n",
      "INFO:root:training epoch 39 / 100 -- batch count 137880\n",
      "INFO:root:current loss: 1865.092529296875\n",
      "INFO:root:training epoch 40 / 100 -- batch count 141327\n",
      "INFO:root:current loss: 1880.79541015625\n",
      "INFO:root:training epoch 41 / 100 -- batch count 144774\n",
      "INFO:root:current loss: 1854.534912109375\n",
      "INFO:root:training epoch 42 / 100 -- batch count 148221\n",
      "INFO:root:current loss: 1878.469482421875\n",
      "INFO:root:training epoch 43 / 100 -- batch count 151668\n",
      "INFO:root:current loss: 1870.3714599609375\n",
      "INFO:root:training epoch 44 / 100 -- batch count 155115\n",
      "INFO:root:current loss: 1866.5826416015625\n",
      "INFO:root:training epoch 45 / 100 -- batch count 158562\n",
      "INFO:root:current loss: 1867.918212890625\n",
      "INFO:root:training epoch 46 / 100 -- batch count 162009\n",
      "INFO:root:current loss: 1874.1954345703125\n",
      "INFO:root:training epoch 47 / 100 -- batch count 165456\n",
      "INFO:root:current loss: 1867.7342529296875\n",
      "INFO:root:training epoch 48 / 100 -- batch count 168903\n",
      "INFO:root:current loss: 1873.0098876953125\n",
      "INFO:root:training epoch 49 / 100 -- batch count 172350\n",
      "INFO:root:current loss: 1866.5625\n",
      "INFO:root:training epoch 50 / 100 -- batch count 175797\n",
      "INFO:root:current loss: 1860.797119140625\n",
      "INFO:root:training epoch 51 / 100 -- batch count 179244\n",
      "INFO:root:current loss: 1873.2911376953125\n",
      "INFO:root:training epoch 52 / 100 -- batch count 182691\n",
      "INFO:root:current loss: 1875.408203125\n",
      "INFO:root:training epoch 53 / 100 -- batch count 186138\n",
      "INFO:root:current loss: 1880.8076171875\n",
      "INFO:root:training epoch 54 / 100 -- batch count 189585\n",
      "INFO:root:current loss: 1856.290283203125\n",
      "INFO:root:training epoch 55 / 100 -- batch count 193032\n",
      "INFO:root:current loss: 1873.370849609375\n",
      "INFO:root:training epoch 56 / 100 -- batch count 196479\n",
      "INFO:root:current loss: 1864.373291015625\n",
      "INFO:root:training epoch 57 / 100 -- batch count 199926\n",
      "INFO:root:current loss: 1864.6502685546875\n",
      "INFO:root:training epoch 58 / 100 -- batch count 203373\n",
      "INFO:root:current loss: 1866.235595703125\n",
      "INFO:root:training epoch 59 / 100 -- batch count 206820\n",
      "INFO:root:current loss: 1858.26318359375\n",
      "INFO:root:training epoch 60 / 100 -- batch count 210267\n",
      "INFO:root:current loss: 1865.0810546875\n",
      "INFO:root:training epoch 61 / 100 -- batch count 213714\n",
      "INFO:root:current loss: 1881.592041015625\n",
      "INFO:root:training epoch 62 / 100 -- batch count 217161\n",
      "INFO:root:current loss: 1869.0325927734375\n",
      "INFO:root:training epoch 63 / 100 -- batch count 220608\n",
      "INFO:root:current loss: 1860.46337890625\n",
      "INFO:root:training epoch 64 / 100 -- batch count 224055\n",
      "INFO:root:current loss: 1859.205322265625\n",
      "INFO:root:training epoch 65 / 100 -- batch count 227502\n",
      "INFO:root:current loss: 1871.334228515625\n",
      "INFO:root:training epoch 66 / 100 -- batch count 230949\n",
      "INFO:root:current loss: 1861.719970703125\n",
      "INFO:root:training epoch 67 / 100 -- batch count 234396\n",
      "INFO:root:current loss: 1870.61279296875\n",
      "INFO:root:training epoch 68 / 100 -- batch count 237843\n",
      "INFO:root:current loss: 1869.1337890625\n",
      "INFO:root:training epoch 69 / 100 -- batch count 241290\n",
      "INFO:root:current loss: 1867.6328125\n",
      "INFO:root:training epoch 70 / 100 -- batch count 244737\n",
      "INFO:root:current loss: 1871.0850830078125\n",
      "INFO:root:training epoch 71 / 100 -- batch count 248184\n",
      "INFO:root:current loss: 1875.92431640625\n",
      "INFO:root:training epoch 72 / 100 -- batch count 251631\n",
      "INFO:root:current loss: 1871.112060546875\n",
      "INFO:root:training epoch 73 / 100 -- batch count 255078\n",
      "INFO:root:current loss: 1874.864013671875\n",
      "INFO:root:training epoch 74 / 100 -- batch count 258525\n",
      "INFO:root:current loss: 1860.68603515625\n",
      "INFO:root:training epoch 75 / 100 -- batch count 261972\n",
      "INFO:root:current loss: 1865.30810546875\n",
      "INFO:root:training epoch 76 / 100 -- batch count 265419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:current loss: 1858.406005859375\n",
      "INFO:root:training epoch 77 / 100 -- batch count 268866\n",
      "INFO:root:current loss: 1857.097900390625\n",
      "INFO:root:training epoch 78 / 100 -- batch count 272313\n",
      "INFO:root:current loss: 1860.477783203125\n",
      "INFO:root:training epoch 79 / 100 -- batch count 275760\n",
      "INFO:root:current loss: 1866.943603515625\n",
      "INFO:root:training epoch 80 / 100 -- batch count 279207\n",
      "INFO:root:current loss: 1876.9111328125\n",
      "INFO:root:training epoch 81 / 100 -- batch count 282654\n",
      "INFO:root:current loss: 1862.8778076171875\n",
      "INFO:root:training epoch 82 / 100 -- batch count 286101\n",
      "INFO:root:current loss: 1870.20849609375\n",
      "INFO:root:training epoch 83 / 100 -- batch count 289548\n",
      "INFO:root:current loss: 1862.7376708984375\n",
      "INFO:root:training epoch 84 / 100 -- batch count 292995\n",
      "INFO:root:current loss: 1875.3779296875\n",
      "INFO:root:training epoch 85 / 100 -- batch count 296442\n",
      "INFO:root:current loss: 1880.837890625\n",
      "INFO:root:training epoch 86 / 100 -- batch count 299889\n",
      "INFO:root:current loss: 1869.953125\n",
      "INFO:root:training epoch 87 / 100 -- batch count 303336\n",
      "INFO:root:current loss: 1861.3822021484375\n",
      "INFO:root:training epoch 88 / 100 -- batch count 306783\n",
      "INFO:root:current loss: 1858.6414794921875\n",
      "INFO:root:training epoch 89 / 100 -- batch count 310230\n",
      "INFO:root:current loss: 1868.97509765625\n",
      "INFO:root:training epoch 90 / 100 -- batch count 313677\n",
      "INFO:root:current loss: 1883.886474609375\n",
      "INFO:root:training epoch 91 / 100 -- batch count 317124\n",
      "INFO:root:current loss: 1854.964599609375\n",
      "INFO:root:training epoch 92 / 100 -- batch count 320571\n",
      "INFO:root:current loss: 1870.3323974609375\n",
      "INFO:root:training epoch 93 / 100 -- batch count 324018\n",
      "INFO:root:current loss: 1872.581298828125\n",
      "INFO:root:training epoch 94 / 100 -- batch count 327465\n",
      "INFO:root:current loss: 1865.453857421875\n",
      "INFO:root:training epoch 95 / 100 -- batch count 330912\n",
      "INFO:root:current loss: 1872.9144287109375\n",
      "INFO:root:training epoch 96 / 100 -- batch count 334359\n",
      "INFO:root:current loss: 1863.0740966796875\n",
      "INFO:root:training epoch 97 / 100 -- batch count 337806\n",
      "INFO:root:current loss: 1872.7789306640625\n",
      "INFO:root:training epoch 98 / 100 -- batch count 341253\n",
      "INFO:root:current loss: 1862.2156982421875\n",
      "INFO:root:training epoch 99 / 100 -- batch count 344700\n",
      "INFO:root:current loss: 1874.264892578125\n",
      "INFO:root:training epoch 100 / 100 -- batch count 348147\n",
      "INFO:root:current loss: 1861.2135009765625\n",
      "INFO:root:Model trained\n",
      "INFO:root:Saving model to /home/lboloni/Documents/HackingWork/__Temporary/2021-05-01-RobotImitationLearning/cvae\n"
     ]
    }
   ],
   "source": [
    "unsupervised_dir = pathlib.Path(paths.demonstrations_rouhi_dir, \"task-3001\", \"10090\", \"camera-1\")\n",
    "\n",
    "config = {\n",
    "    \"latent_dim\": 256, \"image_width\": 32, \"image_height\": 32, \"image_color_channels\": 3,\n",
    "    \"epochs_target\": 100, \"training_data_dir\": unsupervised_dir, \n",
    "    \"model_path\": paths.visual_module_model_path, \"load_only\": False\n",
    "}\n",
    "\n",
    "model = CVAE_VisualModule.get_trained_model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
